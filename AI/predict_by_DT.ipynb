{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from common.read_spectral_common import (\n",
    "    data_df_13_09_2022, \n",
    "    calculate_mutual_info_for_all, \n",
    "    generate_sample, create_X_train_Y_train, \n",
    "    mutual_info_regression, \n",
    "    get_max_bands, \n",
    "    get_bands_ix_from_mutual_info, \n",
    "    get_average_bands, \n",
    "    get_max_bands, \n",
    "    get_min_bands,\n",
    "    predict_using_neutral_network, \n",
    "    predict_using_random_forest, \n",
    "    predict_using_decision_tree,\n",
    "    get_full_path\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import cloudpickle\n",
    "import common\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_value = \"K\"\n",
    "train_field = \"J\"\n",
    "function_get = get_min_bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(95, 0.3688257039953742),\n",
       " (88, 0.29264949390372275),\n",
       " (28, 0.25055449189842705),\n",
       " (79, 0.24017540446825558),\n",
       " (41, 0.2023919235031495),\n",
       " (93, 0.20031068770075278),\n",
       " (107, 0.18689894820892272),\n",
       " (43, 0.1743737023754468),\n",
       " (108, 0.1670829649542478),\n",
       " (120, 0.1639961441547313),\n",
       " (87, 0.15693659342366528),\n",
       " (35, 0.15534485985360869),\n",
       " (90, 0.1549532095307744),\n",
       " (86, 0.1521470136381411),\n",
       " (58, 0.14568612217622356),\n",
       " (69, 0.13775497079043042),\n",
       " (67, 0.12682324501801956),\n",
       " (66, 0.1251597902330861),\n",
       " (33, 0.12429978570253919),\n",
       " (81, 0.12289143117938073),\n",
       " (31, 0.12099570427626238),\n",
       " (27, 0.12011406356338616),\n",
       " (48, 0.11706052679796164),\n",
       " (61, 0.11667336363842828),\n",
       " (26, 0.1134956172101882),\n",
       " (57, 0.11057250396576945),\n",
       " (78, 0.10498609407733284),\n",
       " (113, 0.1024974947137336),\n",
       " (42, 0.09472159304593353),\n",
       " (52, 0.09077707628380827),\n",
       " (83, 0.09016043895083925),\n",
       " (92, 0.0896116344320399),\n",
       " (71, 0.08746522884141372),\n",
       " (80, 0.08588997947204602),\n",
       " (103, 0.07982986527527114),\n",
       " (5, 0.07686219803984917),\n",
       " (111, 0.07530861945500522),\n",
       " (49, 0.07337877501598689),\n",
       " (105, 0.07020264145951138),\n",
       " (82, 0.07007821770440259),\n",
       " (109, 0.06974442406934855),\n",
       " (56, 0.06966586265225327),\n",
       " (91, 0.06872490950119747),\n",
       " (55, 0.0656230647310363),\n",
       " (84, 0.0654674508387334),\n",
       " (29, 0.06347792071172043),\n",
       " (75, 0.06285771497384252),\n",
       " (39, 0.061487548803491165),\n",
       " (99, 0.06061868316580421),\n",
       " (97, 0.059900756808551225),\n",
       " (102, 0.0579159988293485),\n",
       " (116, 0.05779009811109037),\n",
       " (18, 0.05756210694306296),\n",
       " (20, 0.056892446227170046),\n",
       " (0, 0.052613243499589846),\n",
       " (114, 0.04802319915076447),\n",
       " (100, 0.0461807837290964),\n",
       " (96, 0.04584136415378115),\n",
       " (59, 0.037969551290346404),\n",
       " (85, 0.03781680789682351),\n",
       " (94, 0.036226196794149956),\n",
       " (70, 0.03603343938020798),\n",
       " (19, 0.0349452372622201),\n",
       " (76, 0.03488269200642602),\n",
       " (104, 0.034392615977727026),\n",
       " (73, 0.031755930423782),\n",
       " (77, 0.031663255082487574),\n",
       " (22, 0.024833155095137505),\n",
       " (64, 0.02347266737801057),\n",
       " (40, 0.022295868000846486),\n",
       " (2, 0.01994895567309496),\n",
       " (25, 0.019723807939621807),\n",
       " (47, 0.01837924393125867),\n",
       " (115, 0.017933466147598942),\n",
       " (38, 0.015964726464487544),\n",
       " (60, 0.015001780413870236),\n",
       " (118, 0.010078974439936328),\n",
       " (14, 0.009259259259258634),\n",
       " (11, 0.009259259259258412),\n",
       " (15, 0.009259259259258412),\n",
       " (72, 0.007816268735500564),\n",
       " (30, 0.00781587978022813),\n",
       " (74, 0.007160221575622128),\n",
       " (51, 0.001093755363246629),\n",
       " (1, 0.0),\n",
       " (3, 0.0),\n",
       " (4, 0.0),\n",
       " (6, 0.0),\n",
       " (7, 0.0),\n",
       " (8, 0.0),\n",
       " (9, 0.0),\n",
       " (10, 0.0),\n",
       " (12, 0.0),\n",
       " (13, 0.0),\n",
       " (16, 0.0),\n",
       " (17, 0.0),\n",
       " (21, 0.0),\n",
       " (23, 0.0),\n",
       " (24, 0.0),\n",
       " (32, 0.0),\n",
       " (34, 0.0),\n",
       " (36, 0.0),\n",
       " (37, 0.0),\n",
       " (44, 0.0),\n",
       " (45, 0.0),\n",
       " (46, 0.0),\n",
       " (50, 0.0),\n",
       " (53, 0.0),\n",
       " (54, 0.0),\n",
       " (62, 0.0),\n",
       " (63, 0.0),\n",
       " (65, 0.0),\n",
       " (68, 0.0),\n",
       " (89, 0.0),\n",
       " (98, 0.0),\n",
       " (101, 0.0),\n",
       " (106, 0.0),\n",
       " (110, 0.0),\n",
       " (112, 0.0),\n",
       " (117, 0.0),\n",
       " (119, 0.0),\n",
       " (121, 0.0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_mutual_info_for_all(data_df_13_09_2022, target_value, train_field, function_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_ix = get_bands_ix_from_mutual_info(data_df_13_09_2022, 0.1, target_value, train_field, function_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bands_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_ix = filter(lambda data: data < 100, bands_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_ix = list(bands_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26,\n",
       " 27,\n",
       " 28,\n",
       " 31,\n",
       " 33,\n",
       " 35,\n",
       " 41,\n",
       " 43,\n",
       " 48,\n",
       " 57,\n",
       " 58,\n",
       " 61,\n",
       " 66,\n",
       " 67,\n",
       " 69,\n",
       " 78,\n",
       " 79,\n",
       " 81,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 90,\n",
       " 93,\n",
       " 95]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bands_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bands_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | loss train: 16023.5947265625\n",
      "Epoch: 100 | loss train: 15928.287109375\n",
      "Epoch: 200 | loss train: 2797.238525390625\n",
      "Epoch: 300 | loss train: 2783.033447265625\n",
      "Epoch: 400 | loss train: 2776.75146484375\n",
      "Epoch: 500 | loss train: 2768.85400390625\n",
      "Epoch: 600 | loss train: 2759.0908203125\n",
      "Epoch: 700 | loss train: 2742.276123046875\n",
      "Epoch: 800 | loss train: 2719.255126953125\n",
      "Epoch: 900 | loss train: 2683.732177734375\n",
      "Epoch: 1000 | loss train: 2631.398681640625\n",
      "Epoch: 1100 | loss train: 2553.850341796875\n",
      "Epoch: 1200 | loss train: 2456.4462890625\n",
      "Epoch: 1300 | loss train: 2350.131591796875\n",
      "Epoch: 1400 | loss train: 2274.645263671875\n",
      "Epoch: 1500 | loss train: 2242.91650390625\n",
      "Epoch: 1600 | loss train: 2222.805908203125\n",
      "Epoch: 1700 | loss train: 2202.324951171875\n",
      "Epoch: 1800 | loss train: 2179.75634765625\n",
      "Epoch: 1900 | loss train: 2156.19384765625\n",
      "Epoch: 2000 | loss train: 2131.8662109375\n",
      "Epoch: 2100 | loss train: 2102.179443359375\n",
      "Epoch: 2200 | loss train: 2067.617431640625\n",
      "Epoch: 2300 | loss train: 2033.8736572265625\n",
      "Epoch: 2400 | loss train: 1997.643310546875\n",
      "Epoch: 2500 | loss train: 1971.0574951171875\n",
      "Epoch: 2600 | loss train: 1947.3956298828125\n",
      "Epoch: 2700 | loss train: 1923.89111328125\n",
      "Epoch: 2800 | loss train: 1902.1290283203125\n",
      "Epoch: 2900 | loss train: 1883.0650634765625\n",
      "Epoch: 3000 | loss train: 1855.596923828125\n",
      "Epoch: 3100 | loss train: 1829.3470458984375\n",
      "Epoch: 3200 | loss train: 1807.8475341796875\n",
      "Epoch: 3300 | loss train: 1787.8289794921875\n",
      "Epoch: 3400 | loss train: 1769.76220703125\n",
      "Epoch: 3500 | loss train: 1752.48193359375\n",
      "Epoch: 3600 | loss train: 1731.7274169921875\n",
      "Epoch: 3700 | loss train: 1707.7496337890625\n",
      "Epoch: 3800 | loss train: 1680.451416015625\n",
      "Epoch: 3900 | loss train: 1651.7725830078125\n",
      "Epoch: 4000 | loss train: 1622.3511962890625\n",
      "Epoch: 4100 | loss train: 1595.8521728515625\n",
      "Epoch: 4200 | loss train: 1572.7293701171875\n",
      "Epoch: 4300 | loss train: 1541.828369140625\n",
      "Epoch: 4400 | loss train: 1519.21240234375\n",
      "Epoch: 4500 | loss train: 1495.1522216796875\n",
      "Epoch: 4600 | loss train: 1472.6748046875\n",
      "Epoch: 4700 | loss train: 1455.028564453125\n",
      "Epoch: 4800 | loss train: 1431.4351806640625\n",
      "Epoch: 4900 | loss train: 1409.60986328125\n",
      "Epoch: 5000 | loss train: 1395.889404296875\n",
      "Epoch: 5100 | loss train: 1369.3365478515625\n",
      "Epoch: 5200 | loss train: 1350.2716064453125\n",
      "Epoch: 5300 | loss train: 1340.5333251953125\n",
      "Epoch: 5400 | loss train: 1313.709228515625\n",
      "Epoch: 5500 | loss train: 1297.3109130859375\n",
      "Epoch: 5600 | loss train: 1283.39599609375\n",
      "Epoch: 5700 | loss train: 1283.101318359375\n",
      "Epoch: 5800 | loss train: 1256.22021484375\n",
      "Epoch: 5900 | loss train: 1251.1904296875\n",
      "Epoch: 6000 | loss train: 1226.876220703125\n",
      "Epoch: 6100 | loss train: 1224.609619140625\n",
      "Epoch: 6200 | loss train: 1194.5001220703125\n",
      "Epoch: 6300 | loss train: 1184.48681640625\n",
      "Epoch: 6400 | loss train: 1173.1922607421875\n",
      "Epoch: 6500 | loss train: 1150.608642578125\n",
      "Epoch: 6600 | loss train: 1166.46337890625\n",
      "Epoch: 6700 | loss train: 1126.3890380859375\n",
      "Epoch: 6800 | loss train: 1113.960205078125\n",
      "Epoch: 6900 | loss train: 1097.6783447265625\n",
      "Epoch: 7000 | loss train: 1079.7471923828125\n",
      "Epoch: 7100 | loss train: 1066.362060546875\n",
      "Epoch: 7200 | loss train: 1072.1605224609375\n",
      "Epoch: 7300 | loss train: 1038.7642822265625\n",
      "Epoch: 7400 | loss train: 1054.571044921875\n",
      "Epoch: 7500 | loss train: 1016.3828125\n",
      "Epoch: 7600 | loss train: 1004.4952392578125\n",
      "Epoch: 7700 | loss train: 1003.84423828125\n",
      "Epoch: 7800 | loss train: 979.296875\n",
      "Epoch: 7900 | loss train: 970.2274780273438\n",
      "Epoch: 8000 | loss train: 1003.8223266601562\n",
      "Epoch: 8100 | loss train: 946.98974609375\n",
      "Epoch: 8200 | loss train: 944.1917724609375\n",
      "Epoch: 8300 | loss train: 931.7764892578125\n",
      "Epoch: 8400 | loss train: 919.2884521484375\n",
      "Epoch: 8500 | loss train: 926.4935302734375\n",
      "Epoch: 8600 | loss train: 905.761474609375\n",
      "Epoch: 8700 | loss train: 918.4404907226562\n",
      "Epoch: 8800 | loss train: 896.0457763671875\n",
      "Epoch: 8900 | loss train: 878.3772583007812\n",
      "Epoch: 9000 | loss train: 889.403564453125\n",
      "Epoch: 9100 | loss train: 842.3084106445312\n",
      "Epoch: 9200 | loss train: 824.2796020507812\n",
      "Epoch: 9300 | loss train: 814.5328979492188\n",
      "Epoch: 9400 | loss train: 831.3153076171875\n",
      "Epoch: 9500 | loss train: 791.8086547851562\n",
      "Epoch: 9600 | loss train: 778.781982421875\n",
      "Epoch: 9700 | loss train: 772.4861450195312\n",
      "Epoch: 9800 | loss train: 781.1262817382812\n",
      "Epoch: 9900 | loss train: 750.1595458984375\n",
      "Epoch: 10000 | loss train: 759.728515625\n",
      "Epoch: 10100 | loss train: 731.9742431640625\n",
      "Epoch: 10200 | loss train: 705.6205444335938\n",
      "Epoch: 10300 | loss train: 694.099853515625\n",
      "Epoch: 10400 | loss train: 687.3756713867188\n",
      "Epoch: 10500 | loss train: 693.6602783203125\n",
      "Epoch: 10600 | loss train: 642.7584838867188\n",
      "Epoch: 10700 | loss train: 632.2088012695312\n",
      "Epoch: 10800 | loss train: 609.682373046875\n",
      "Epoch: 10900 | loss train: 614.5142822265625\n",
      "Epoch: 11000 | loss train: 580.2177124023438\n",
      "Epoch: 11100 | loss train: 565.0469970703125\n",
      "Epoch: 11200 | loss train: 541.3474731445312\n",
      "Epoch: 11300 | loss train: 553.9476928710938\n",
      "Epoch: 11400 | loss train: 518.0071411132812\n",
      "Epoch: 11500 | loss train: 590.4984741210938\n",
      "Epoch: 11600 | loss train: 470.9688720703125\n",
      "Epoch: 11700 | loss train: 465.1399230957031\n",
      "Epoch: 11800 | loss train: 437.61767578125\n",
      "Epoch: 11900 | loss train: 456.3580322265625\n",
      "Epoch: 12000 | loss train: 437.6378173828125\n",
      "Epoch: 12100 | loss train: 388.7158203125\n",
      "Epoch: 12200 | loss train: 377.45013427734375\n",
      "Epoch: 12300 | loss train: 357.1536865234375\n",
      "Epoch: 12400 | loss train: 351.1312561035156\n",
      "Epoch: 12500 | loss train: 396.8943176269531\n",
      "Epoch: 12600 | loss train: 322.52313232421875\n",
      "Epoch: 12700 | loss train: 341.59283447265625\n",
      "Epoch: 12800 | loss train: 322.8050231933594\n",
      "Epoch: 12900 | loss train: 313.1295166015625\n",
      "Epoch: 13000 | loss train: 279.4971008300781\n",
      "Epoch: 13100 | loss train: 323.65496826171875\n",
      "Epoch: 13200 | loss train: 300.7384033203125\n",
      "Epoch: 13300 | loss train: 258.7442626953125\n",
      "Epoch: 13400 | loss train: 320.4595031738281\n",
      "Epoch: 13500 | loss train: 329.3159484863281\n",
      "Epoch: 13600 | loss train: 326.08453369140625\n",
      "Epoch: 13700 | loss train: 254.69190979003906\n",
      "Epoch: 13800 | loss train: 231.5730743408203\n",
      "Epoch: 13900 | loss train: 232.3435516357422\n",
      "Epoch: 14000 | loss train: 240.31471252441406\n",
      "Epoch: 14100 | loss train: 222.45541381835938\n",
      "Epoch: 14200 | loss train: 211.8280792236328\n",
      "Epoch: 14300 | loss train: 360.99139404296875\n",
      "Epoch: 14400 | loss train: 218.55841064453125\n",
      "Epoch: 14500 | loss train: 201.00059509277344\n",
      "Epoch: 14600 | loss train: 251.56275939941406\n",
      "Epoch: 14700 | loss train: 172.10244750976562\n",
      "Epoch: 14800 | loss train: 209.86045837402344\n",
      "Epoch: 14900 | loss train: 358.13543701171875\n",
      "Epoch: 15000 | loss train: 161.77151489257812\n",
      "Epoch: 15100 | loss train: 180.9911651611328\n",
      "Epoch: 15200 | loss train: 166.6688232421875\n",
      "Epoch: 15300 | loss train: 171.37672424316406\n",
      "Epoch: 15400 | loss train: 151.1155242919922\n",
      "Epoch: 15500 | loss train: 143.11439514160156\n",
      "Epoch: 15600 | loss train: 138.1298370361328\n",
      "Epoch: 15700 | loss train: 174.68128967285156\n",
      "Epoch: 15800 | loss train: 138.66818237304688\n",
      "Epoch: 15900 | loss train: 133.82818603515625\n",
      "Epoch: 16000 | loss train: 125.01593780517578\n",
      "Epoch: 16100 | loss train: 123.08629608154297\n",
      "Epoch: 16200 | loss train: 128.94871520996094\n",
      "Epoch: 16300 | loss train: 469.6430969238281\n",
      "Epoch: 16400 | loss train: 124.41718292236328\n",
      "Epoch: 16500 | loss train: 109.29560089111328\n",
      "Epoch: 16600 | loss train: 155.50486755371094\n",
      "Epoch: 16700 | loss train: 112.96589660644531\n",
      "Epoch: 16800 | loss train: 193.61962890625\n",
      "Epoch: 16900 | loss train: 123.59508514404297\n",
      "Epoch: 17000 | loss train: 97.65667724609375\n",
      "Epoch: 17100 | loss train: 245.6148223876953\n",
      "Epoch: 17200 | loss train: 117.83787536621094\n",
      "Epoch: 17300 | loss train: 95.70954132080078\n",
      "Epoch: 17400 | loss train: 96.2145767211914\n",
      "Epoch: 17500 | loss train: 144.43768310546875\n",
      "Epoch: 17600 | loss train: 155.6611785888672\n",
      "Epoch: 17700 | loss train: 96.16682434082031\n",
      "Epoch: 17800 | loss train: 112.77107238769531\n",
      "Epoch: 17900 | loss train: 87.44569396972656\n",
      "Epoch: 18000 | loss train: 81.80982208251953\n",
      "Epoch: 18100 | loss train: 82.32022857666016\n",
      "Epoch: 18200 | loss train: 111.89894104003906\n",
      "Epoch: 18300 | loss train: 247.7451171875\n",
      "Epoch: 18400 | loss train: 105.97893524169922\n",
      "Epoch: 18500 | loss train: 68.86345672607422\n",
      "Epoch: 18600 | loss train: 122.90184783935547\n",
      "Epoch: 18700 | loss train: 68.97410583496094\n",
      "Epoch: 18800 | loss train: 66.19395446777344\n",
      "Epoch: 18900 | loss train: 70.57695770263672\n",
      "Epoch: 19000 | loss train: 66.77128601074219\n",
      "Epoch: 19100 | loss train: 172.29290771484375\n",
      "Epoch: 19200 | loss train: 60.829864501953125\n",
      "Epoch: 19300 | loss train: 83.9144515991211\n",
      "Epoch: 19400 | loss train: 82.70326232910156\n",
      "Epoch: 19500 | loss train: 593.503662109375\n",
      "Epoch: 19600 | loss train: 72.43978118896484\n",
      "Epoch: 19700 | loss train: 68.35871124267578\n",
      "Epoch: 19800 | loss train: 69.15900421142578\n",
      "Epoch: 19900 | loss train: 66.02682495117188\n",
      "Epoch: 20000 | loss train: 68.503173828125\n",
      "Epoch: 20100 | loss train: 65.27839660644531\n",
      "Epoch: 20200 | loss train: 48.86773681640625\n",
      "Epoch: 20300 | loss train: 85.14144134521484\n",
      "Epoch: 20400 | loss train: 101.03887176513672\n",
      "Epoch: 20500 | loss train: 51.56033706665039\n",
      "Epoch: 20600 | loss train: 47.661964416503906\n",
      "Epoch: 20700 | loss train: 45.032344818115234\n",
      "Epoch: 20800 | loss train: 49.6414794921875\n",
      "Epoch: 20900 | loss train: 57.31523513793945\n",
      "Epoch: 21000 | loss train: 49.504512786865234\n",
      "Epoch: 21100 | loss train: 157.98684692382812\n",
      "Epoch: 21200 | loss train: 59.92152404785156\n",
      "Epoch: 21300 | loss train: 55.56745529174805\n",
      "Epoch: 21400 | loss train: 54.53314971923828\n",
      "Epoch: 21500 | loss train: 52.922218322753906\n",
      "Epoch: 21600 | loss train: 52.87765884399414\n",
      "Epoch: 21700 | loss train: 73.23197937011719\n",
      "Epoch: 21800 | loss train: 92.19944763183594\n",
      "Epoch: 21900 | loss train: 52.91893768310547\n",
      "Epoch: 22000 | loss train: 88.03223419189453\n",
      "Epoch: 22100 | loss train: 66.0302963256836\n",
      "Epoch: 22200 | loss train: 60.5568733215332\n",
      "Epoch: 22300 | loss train: 56.070430755615234\n",
      "Epoch: 22400 | loss train: 49.944244384765625\n",
      "Epoch: 22500 | loss train: 45.978240966796875\n",
      "Epoch: 22600 | loss train: 47.51884460449219\n",
      "Epoch: 22700 | loss train: 66.17620849609375\n",
      "Epoch: 22800 | loss train: 40.67534255981445\n",
      "Epoch: 22900 | loss train: 49.277374267578125\n",
      "Epoch: 23000 | loss train: 36.97666549682617\n",
      "Epoch: 23100 | loss train: 663.0052490234375\n",
      "Epoch: 23200 | loss train: 46.67799377441406\n",
      "Epoch: 23300 | loss train: 38.82556915283203\n",
      "Epoch: 23400 | loss train: 34.41978073120117\n",
      "Epoch: 23500 | loss train: 32.250038146972656\n",
      "Epoch: 23600 | loss train: 69.4034194946289\n",
      "Epoch: 23700 | loss train: 816.6156005859375\n",
      "Epoch: 23800 | loss train: 85.89787292480469\n",
      "Epoch: 23900 | loss train: 70.29193878173828\n",
      "Epoch: 24000 | loss train: 55.89219284057617\n",
      "Epoch: 24100 | loss train: 50.30648422241211\n",
      "Epoch: 24200 | loss train: 44.58380126953125\n",
      "Epoch: 24300 | loss train: 40.82467269897461\n",
      "Epoch: 24400 | loss train: 38.15296936035156\n",
      "Epoch: 24500 | loss train: 45.86640548706055\n",
      "Epoch: 24600 | loss train: 38.359825134277344\n",
      "Epoch: 24700 | loss train: 58.017765045166016\n",
      "Epoch: 24800 | loss train: 98.56938934326172\n",
      "Epoch: 24900 | loss train: 40.943424224853516\n",
      "Epoch: 25000 | loss train: 35.908607482910156\n",
      "Epoch: 25100 | loss train: 34.03452682495117\n",
      "Epoch: 25200 | loss train: 264.581298828125\n",
      "Epoch: 25300 | loss train: 39.920291900634766\n",
      "Epoch: 25400 | loss train: 33.497901916503906\n",
      "Epoch: 25500 | loss train: 31.32467269897461\n",
      "Epoch: 25600 | loss train: 30.318857192993164\n",
      "Epoch: 25700 | loss train: 27.97109031677246\n",
      "Epoch: 25800 | loss train: 34.82405090332031\n",
      "Epoch: 25900 | loss train: 120.07350158691406\n",
      "Epoch: 26000 | loss train: 34.484375\n",
      "Epoch: 26100 | loss train: 30.113386154174805\n",
      "Epoch: 26200 | loss train: 28.516586303710938\n",
      "Epoch: 26300 | loss train: 27.254188537597656\n",
      "Epoch: 26400 | loss train: 26.712820053100586\n",
      "Epoch: 26500 | loss train: 25.974807739257812\n",
      "Epoch: 26600 | loss train: 27.59457778930664\n",
      "Epoch: 26700 | loss train: 31.909204483032227\n",
      "Epoch: 26800 | loss train: 389.816162109375\n",
      "Epoch: 26900 | loss train: 231.55670166015625\n",
      "Epoch: 27000 | loss train: 181.92376708984375\n",
      "Epoch: 27100 | loss train: 248.8265838623047\n",
      "Epoch: 27200 | loss train: 153.47364807128906\n",
      "Epoch: 27300 | loss train: 118.00519561767578\n",
      "Epoch: 27400 | loss train: 124.18218994140625\n",
      "Epoch: 27500 | loss train: 109.68479919433594\n",
      "Epoch: 27600 | loss train: 109.28976440429688\n",
      "Epoch: 27700 | loss train: 198.88011169433594\n",
      "Epoch: 27800 | loss train: 143.2431182861328\n",
      "Epoch: 27900 | loss train: 80.99712371826172\n",
      "Epoch: 28000 | loss train: 94.31172943115234\n",
      "Epoch: 28100 | loss train: 134.93907165527344\n",
      "Epoch: 28200 | loss train: 86.18959045410156\n",
      "Epoch: 28300 | loss train: 79.64334869384766\n",
      "Epoch: 28400 | loss train: 93.97074890136719\n",
      "Epoch: 28500 | loss train: 153.95803833007812\n",
      "Epoch: 28600 | loss train: 131.07611083984375\n",
      "Epoch: 28700 | loss train: 101.95381164550781\n",
      "Epoch: 28800 | loss train: 59.52324295043945\n",
      "Epoch: 28900 | loss train: 57.80086898803711\n",
      "Epoch: 29000 | loss train: 55.74076843261719\n",
      "Epoch: 29100 | loss train: 54.083919525146484\n",
      "Epoch: 29200 | loss train: 55.008934020996094\n",
      "Epoch: 29300 | loss train: 139.35511779785156\n",
      "Epoch: 29400 | loss train: 34.74736404418945\n",
      "Epoch: 29500 | loss train: 32.81880187988281\n",
      "Epoch: 29600 | loss train: 37.08399200439453\n",
      "Epoch: 29700 | loss train: 67.2634506225586\n",
      "Epoch: 29800 | loss train: 76.3864974975586\n",
      "Epoch: 29900 | loss train: 58.593650817871094\n",
      "Epoch: 30000 | loss train: 54.32909393310547\n",
      "Epoch: 30100 | loss train: 52.053260803222656\n",
      "Epoch: 30200 | loss train: 49.964595794677734\n",
      "Epoch: 30300 | loss train: 49.667335510253906\n",
      "Epoch: 30400 | loss train: 63.745086669921875\n",
      "Epoch: 30500 | loss train: 45.501731872558594\n",
      "Epoch: 30600 | loss train: 68.97360229492188\n",
      "Epoch: 30700 | loss train: 62.09617233276367\n",
      "Epoch: 30800 | loss train: 104.9755859375\n",
      "Epoch: 30900 | loss train: 46.99708938598633\n",
      "Epoch: 31000 | loss train: 114.59867858886719\n",
      "Epoch: 31100 | loss train: 61.28409194946289\n",
      "Epoch: 31200 | loss train: 102.24867248535156\n",
      "Epoch: 31300 | loss train: 19.506193161010742\n",
      "Epoch: 31400 | loss train: 154.40647888183594\n",
      "Epoch: 31500 | loss train: 38.8688850402832\n",
      "Epoch: 31600 | loss train: 70.61404418945312\n",
      "Epoch: 31700 | loss train: 40.22382736206055\n",
      "Epoch: 31800 | loss train: 79.68507385253906\n",
      "Epoch: 31900 | loss train: 53.4508056640625\n",
      "Epoch: 32000 | loss train: 63.15386962890625\n",
      "Epoch: 32100 | loss train: 49.95970153808594\n",
      "Epoch: 32200 | loss train: 45.535491943359375\n",
      "Epoch: 32300 | loss train: 66.87255859375\n",
      "Epoch: 32400 | loss train: 39.62702178955078\n",
      "Epoch: 32500 | loss train: 40.470760345458984\n",
      "Epoch: 32600 | loss train: 455.853759765625\n",
      "Epoch: 32700 | loss train: 110.49720001220703\n",
      "Epoch: 32800 | loss train: 106.126708984375\n",
      "Epoch: 32900 | loss train: 102.91973114013672\n",
      "Epoch: 33000 | loss train: 100.74381256103516\n",
      "Epoch: 33100 | loss train: 99.39432525634766\n",
      "Epoch: 33200 | loss train: 95.90100860595703\n",
      "Epoch: 33300 | loss train: 96.10953521728516\n",
      "Epoch: 33400 | loss train: 104.28298950195312\n",
      "Epoch: 33500 | loss train: 89.92961883544922\n",
      "Epoch: 33600 | loss train: 526.8084106445312\n",
      "Epoch: 33700 | loss train: 47.134063720703125\n",
      "Epoch: 33800 | loss train: 44.1038703918457\n",
      "Epoch: 33900 | loss train: 38.73604965209961\n",
      "Epoch: 34000 | loss train: 37.95420455932617\n",
      "Epoch: 34100 | loss train: 37.283443450927734\n",
      "Epoch: 34200 | loss train: 50.6936149597168\n",
      "Epoch: 34300 | loss train: 37.105167388916016\n",
      "Epoch: 34400 | loss train: 36.035194396972656\n",
      "Epoch: 34500 | loss train: 35.21030044555664\n",
      "Epoch: 34600 | loss train: 34.56997299194336\n",
      "Epoch: 34700 | loss train: 87.71421813964844\n",
      "Epoch: 34800 | loss train: 35.144927978515625\n",
      "Epoch: 34900 | loss train: 34.62151336669922\n",
      "Epoch: 35000 | loss train: 34.213768005371094\n",
      "Epoch: 35100 | loss train: 33.74028778076172\n",
      "Epoch: 35200 | loss train: 33.397117614746094\n",
      "Epoch: 35300 | loss train: 44.34111404418945\n",
      "Epoch: 35400 | loss train: 43.514137268066406\n",
      "Epoch: 35500 | loss train: 39.64900207519531\n",
      "Epoch: 35600 | loss train: 32.709651947021484\n",
      "Epoch: 35700 | loss train: 42.245765686035156\n",
      "Epoch: 35800 | loss train: 31.570707321166992\n",
      "Epoch: 35900 | loss train: 625.0711669921875\n",
      "Epoch: 36000 | loss train: 49.394256591796875\n",
      "Epoch: 36100 | loss train: 42.06708908081055\n",
      "Epoch: 36200 | loss train: 40.000335693359375\n",
      "Epoch: 36300 | loss train: 38.86698532104492\n",
      "Epoch: 36400 | loss train: 42.84231948852539\n",
      "Epoch: 36500 | loss train: 35.690338134765625\n",
      "Epoch: 36600 | loss train: 60.104183197021484\n",
      "Epoch: 36700 | loss train: 64.1937255859375\n",
      "Epoch: 36800 | loss train: 45.51540756225586\n",
      "Epoch: 36900 | loss train: 117.8097152709961\n",
      "Epoch: 37000 | loss train: 102.01725006103516\n",
      "Epoch: 37100 | loss train: 89.70234680175781\n",
      "Epoch: 37200 | loss train: 86.96296691894531\n",
      "Epoch: 37300 | loss train: 86.14029693603516\n",
      "Epoch: 37400 | loss train: 89.21072387695312\n",
      "Epoch: 37500 | loss train: 121.63488006591797\n",
      "Epoch: 37600 | loss train: 41.03358840942383\n",
      "Epoch: 37700 | loss train: 31.103906631469727\n",
      "Epoch: 37800 | loss train: 34.149879455566406\n",
      "Epoch: 37900 | loss train: 118.71028900146484\n",
      "Epoch: 38000 | loss train: 97.09772491455078\n",
      "Epoch: 38100 | loss train: 52.02267074584961\n",
      "Epoch: 38200 | loss train: 232.33128356933594\n",
      "Epoch: 38300 | loss train: 38.354740142822266\n",
      "Epoch: 38400 | loss train: 27.50499725341797\n",
      "Epoch: 38500 | loss train: 32.105499267578125\n",
      "Epoch: 38600 | loss train: 38.913265228271484\n",
      "Epoch: 38700 | loss train: 46.00507736206055\n",
      "Epoch: 38800 | loss train: 48.45071792602539\n",
      "Epoch: 38900 | loss train: 20.192859649658203\n",
      "Epoch: 39000 | loss train: 30.548715591430664\n",
      "Epoch: 39100 | loss train: 405.3548278808594\n",
      "Epoch: 39200 | loss train: 121.0704345703125\n",
      "Epoch: 39300 | loss train: 114.10675811767578\n",
      "Epoch: 39400 | loss train: 99.31283569335938\n",
      "Epoch: 39500 | loss train: 95.15138244628906\n",
      "Epoch: 39600 | loss train: 91.7569351196289\n",
      "Epoch: 39700 | loss train: 118.23070526123047\n",
      "Epoch: 39800 | loss train: 179.2162322998047\n",
      "Epoch: 39900 | loss train: 85.4332275390625\n",
      "loss_NN=tensor(10245.2295)\n",
      "pred_NN=tensor([[15298.2451],\n",
      "        [23770.7559],\n",
      "        [25103.6777],\n",
      "        [21635.5410]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sample = generate_sample(data_df_13_09_2022, bands_ix, target_value, train_field)\n",
    "X_train, Y_train = create_X_train_Y_train(sample, bands_ix)\n",
    "sample_target = generate_sample(data_df_13_09_2022, bands_ix, target_value, \"BC\")\n",
    "X_target, Y_target = create_X_train_Y_train(sample_target, bands_ix)\n",
    "super_param={\"lr\": 0.0001, \"weight_decay\": 1e-5, \"n_epochs\": 40000, \"stop_value\": 0.5}\n",
    "re_run = \"N\"\n",
    "loss_DT, pred_DT, DT_model = predict_using_decision_tree(X_train, Y_train, X_target, Y_target, bands_ix, super_param)\n",
    "print(f\"{loss_DT=}\")\n",
    "print(f\"{pred_DT=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudpickle.register_pickle_by_value(common)\n",
    "with open(get_full_path(f\"../../model_saved/DT_save/{'_'.join(train_field)}_predict_{'_'.join(target_value)}_{str(datetime.now().date())}_using_{function_get.__name__}.pkl\"), \"wb\") as file:\n",
    "    cloudpickle.dump(DT_model, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from common.read_spectral_common import (\n",
    "    data_df_13_09_2022, \n",
    "    calculate_mutual_info_for_all, \n",
    "    generate_sample, create_X_train_Y_train, \n",
    "    mutual_info_regression, \n",
    "    get_max_bands, \n",
    "    get_bands_ix_from_mutual_info, \n",
    "    get_average_bands, \n",
    "    get_max_bands, \n",
    "    get_min_bands,\n",
    "    predict_using_neutral_network, \n",
    "    predict_using_random_forest, \n",
    "    predict_using_decision_tree,\n",
    "    get_full_path\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import cloudpickle\n",
    "import common\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_value = \"N\"\n",
    "train_field = \"T\"\n",
    "function_get = get_average_bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(57, 0.23290697479540956),\n",
       " (52, 0.1759026223414959),\n",
       " (28, 0.1668367808648874),\n",
       " (114, 0.16272337752847887),\n",
       " (53, 0.15968828399093393),\n",
       " (58, 0.15325615374611568),\n",
       " (62, 0.1492229952682247),\n",
       " (119, 0.13614555720065846),\n",
       " (118, 0.11821251302710323),\n",
       " (11, 0.11205542427712523),\n",
       " (14, 0.11186543594256237),\n",
       " (56, 0.10213463494929176),\n",
       " (3, 0.09895775634864146),\n",
       " (10, 0.09845687121153412),\n",
       " (26, 0.09561746822400741),\n",
       " (15, 0.09060508860442917),\n",
       " (78, 0.09002688219611388),\n",
       " (75, 0.08577270151649508),\n",
       " (76, 0.07935222021856259),\n",
       " (1, 0.07932441796833256),\n",
       " (60, 0.0788743144097741),\n",
       " (87, 0.0780955650564632),\n",
       " (72, 0.07533019906010407),\n",
       " (12, 0.07363926240895102),\n",
       " (82, 0.06661206398962882),\n",
       " (73, 0.06655864653855126),\n",
       " (70, 0.06551507705057613),\n",
       " (31, 0.06122875584896725),\n",
       " (9, 0.05786841575319146),\n",
       " (71, 0.05281102425686868),\n",
       " (84, 0.05240081227837745),\n",
       " (74, 0.05208524699570738),\n",
       " (7, 0.05061162856475354),\n",
       " (121, 0.05007594417026118),\n",
       " (79, 0.045832369374150694),\n",
       " (30, 0.0427821318102386),\n",
       " (83, 0.039534935870834786),\n",
       " (77, 0.03696143199444135),\n",
       " (95, 0.03257371600626735),\n",
       " (16, 0.030883639960766374),\n",
       " (61, 0.0286154760798909),\n",
       " (25, 0.026050539433651654),\n",
       " (23, 0.024731016348357393),\n",
       " (80, 0.02382669998716036),\n",
       " (93, 0.018867825203723765),\n",
       " (100, 0.017831182758833553),\n",
       " (59, 0.012424347943682967),\n",
       " (55, 0.011575420636422251),\n",
       " (89, 0.011407307326538607),\n",
       " (81, 0.0063990131515789805),\n",
       " (105, 0.0030576817882224105),\n",
       " (0, 0.0),\n",
       " (2, 0.0),\n",
       " (4, 0.0),\n",
       " (5, 0.0),\n",
       " (6, 0.0),\n",
       " (8, 0.0),\n",
       " (13, 0.0),\n",
       " (17, 0.0),\n",
       " (18, 0.0),\n",
       " (19, 0.0),\n",
       " (20, 0.0),\n",
       " (21, 0.0),\n",
       " (22, 0.0),\n",
       " (24, 0.0),\n",
       " (27, 0.0),\n",
       " (29, 0.0),\n",
       " (32, 0.0),\n",
       " (33, 0.0),\n",
       " (34, 0.0),\n",
       " (35, 0.0),\n",
       " (36, 0.0),\n",
       " (37, 0.0),\n",
       " (38, 0.0),\n",
       " (39, 0.0),\n",
       " (40, 0.0),\n",
       " (41, 0.0),\n",
       " (42, 0.0),\n",
       " (43, 0.0),\n",
       " (44, 0.0),\n",
       " (45, 0.0),\n",
       " (46, 0.0),\n",
       " (47, 0.0),\n",
       " (48, 0.0),\n",
       " (49, 0.0),\n",
       " (50, 0.0),\n",
       " (51, 0.0),\n",
       " (54, 0.0),\n",
       " (63, 0.0),\n",
       " (64, 0.0),\n",
       " (65, 0.0),\n",
       " (66, 0.0),\n",
       " (67, 0.0),\n",
       " (68, 0.0),\n",
       " (69, 0.0),\n",
       " (85, 0.0),\n",
       " (86, 0.0),\n",
       " (88, 0.0),\n",
       " (90, 0.0),\n",
       " (91, 0.0),\n",
       " (92, 0.0),\n",
       " (94, 0.0),\n",
       " (96, 0.0),\n",
       " (97, 0.0),\n",
       " (98, 0.0),\n",
       " (99, 0.0),\n",
       " (101, 0.0),\n",
       " (102, 0.0),\n",
       " (103, 0.0),\n",
       " (104, 0.0),\n",
       " (106, 0.0),\n",
       " (107, 0.0),\n",
       " (108, 0.0),\n",
       " (109, 0.0),\n",
       " (110, 0.0),\n",
       " (111, 0.0),\n",
       " (112, 0.0),\n",
       " (113, 0.0),\n",
       " (115, 0.0),\n",
       " (116, 0.0),\n",
       " (117, 0.0),\n",
       " (120, 0.0)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_mutual_info_for_all(data_df_13_09_2022, target_value, train_field, function_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_ix = get_bands_ix_from_mutual_info(data_df_13_09_2022, 0.1, target_value, train_field, function_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bands_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_ix = filter(lambda data: data < 100, bands_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_ix = list(bands_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bands_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | loss train: 5287.9873046875\n",
      "Epoch: 100 | loss train: 5279.2412109375\n",
      "Epoch: 200 | loss train: 2085.003173828125\n",
      "Epoch: 300 | loss train: 2079.57470703125\n",
      "Epoch: 400 | loss train: 2078.718994140625\n",
      "Epoch: 500 | loss train: 2077.30224609375\n",
      "Epoch: 600 | loss train: 2075.4521484375\n",
      "Epoch: 700 | loss train: 2073.277587890625\n",
      "Epoch: 800 | loss train: 2070.776123046875\n",
      "Epoch: 900 | loss train: 2067.802001953125\n",
      "Epoch: 1000 | loss train: 2063.508056640625\n",
      "Epoch: 1100 | loss train: 2057.2705078125\n",
      "Epoch: 1200 | loss train: 2046.3232421875\n",
      "Epoch: 1300 | loss train: 2019.1202392578125\n",
      "Epoch: 1400 | loss train: 1956.4215087890625\n",
      "Epoch: 1500 | loss train: 1889.757568359375\n",
      "Epoch: 1600 | loss train: 1851.0902099609375\n",
      "Epoch: 1700 | loss train: 1815.272216796875\n",
      "Epoch: 1800 | loss train: 1781.783203125\n",
      "Epoch: 1900 | loss train: 1753.5350341796875\n",
      "Epoch: 2000 | loss train: 1729.5921630859375\n",
      "Epoch: 2100 | loss train: 1706.9107666015625\n",
      "Epoch: 2200 | loss train: 1681.8634033203125\n",
      "Epoch: 2300 | loss train: 1655.69091796875\n",
      "Epoch: 2400 | loss train: 1629.5150146484375\n",
      "Epoch: 2500 | loss train: 1599.8115234375\n",
      "Epoch: 2600 | loss train: 1567.9947509765625\n",
      "Epoch: 2700 | loss train: 1531.95263671875\n",
      "Epoch: 2800 | loss train: 1496.583984375\n",
      "Epoch: 2900 | loss train: 1465.7276611328125\n",
      "Epoch: 3000 | loss train: 1418.261962890625\n",
      "Epoch: 3100 | loss train: 1380.42919921875\n",
      "Epoch: 3200 | loss train: 1346.485107421875\n",
      "Epoch: 3300 | loss train: 1318.4779052734375\n",
      "Epoch: 3400 | loss train: 1288.9039306640625\n",
      "Epoch: 3500 | loss train: 1270.0643310546875\n",
      "Epoch: 3600 | loss train: 1258.14453125\n",
      "Epoch: 3700 | loss train: 1242.893798828125\n",
      "Epoch: 3800 | loss train: 1236.0567626953125\n",
      "Epoch: 3900 | loss train: 1220.36083984375\n",
      "Epoch: 4000 | loss train: 1214.9088134765625\n",
      "Epoch: 4100 | loss train: 1207.9385986328125\n",
      "Epoch: 4200 | loss train: 1197.117919921875\n",
      "Epoch: 4300 | loss train: 1195.984130859375\n",
      "Epoch: 4400 | loss train: 1185.0970458984375\n",
      "Epoch: 4500 | loss train: 1184.868896484375\n",
      "Epoch: 4600 | loss train: 1175.34716796875\n",
      "Epoch: 4700 | loss train: 1166.1734619140625\n",
      "Epoch: 4800 | loss train: 1158.136962890625\n",
      "Epoch: 4900 | loss train: 1154.6788330078125\n",
      "Epoch: 5000 | loss train: 1131.7939453125\n",
      "Epoch: 5100 | loss train: 1115.9176025390625\n",
      "Epoch: 5200 | loss train: 1103.4920654296875\n",
      "Epoch: 5300 | loss train: 1067.05224609375\n",
      "Epoch: 5400 | loss train: 1037.833740234375\n",
      "Epoch: 5500 | loss train: 1018.0769653320312\n",
      "Epoch: 5600 | loss train: 1025.77685546875\n",
      "Epoch: 5700 | loss train: 976.7462768554688\n",
      "Epoch: 5800 | loss train: 977.0538330078125\n",
      "Epoch: 5900 | loss train: 951.5596923828125\n",
      "Epoch: 6000 | loss train: 937.6005249023438\n",
      "Epoch: 6100 | loss train: 920.0535888671875\n",
      "Epoch: 6200 | loss train: 909.2973022460938\n",
      "Epoch: 6300 | loss train: 882.8705444335938\n",
      "Epoch: 6400 | loss train: 869.39404296875\n",
      "Epoch: 6500 | loss train: 855.8932495117188\n",
      "Epoch: 6600 | loss train: 953.7091064453125\n",
      "Epoch: 6700 | loss train: 829.3502197265625\n",
      "Epoch: 6800 | loss train: 836.5369262695312\n",
      "Epoch: 6900 | loss train: 824.3114013671875\n",
      "Epoch: 7000 | loss train: 821.8516845703125\n",
      "Epoch: 7100 | loss train: 785.3385009765625\n",
      "Epoch: 7200 | loss train: 782.7223510742188\n",
      "Epoch: 7300 | loss train: 768.3351440429688\n",
      "Epoch: 7400 | loss train: 750.0698852539062\n",
      "Epoch: 7500 | loss train: 744.4444580078125\n",
      "Epoch: 7600 | loss train: 728.2328491210938\n",
      "Epoch: 7700 | loss train: 761.814453125\n",
      "Epoch: 7800 | loss train: 738.0046997070312\n",
      "Epoch: 7900 | loss train: 713.5742797851562\n",
      "Epoch: 8000 | loss train: 692.2955322265625\n",
      "Epoch: 8100 | loss train: 712.941162109375\n",
      "Epoch: 8200 | loss train: 695.8590087890625\n",
      "Epoch: 8300 | loss train: 676.2125854492188\n",
      "Epoch: 8400 | loss train: 678.716064453125\n",
      "Epoch: 8500 | loss train: 659.495361328125\n",
      "Epoch: 8600 | loss train: 664.7783203125\n",
      "Epoch: 8700 | loss train: 655.0292358398438\n",
      "Epoch: 8800 | loss train: 655.4531860351562\n",
      "Epoch: 8900 | loss train: 673.4625854492188\n",
      "Epoch: 9000 | loss train: 640.8870849609375\n",
      "Epoch: 9100 | loss train: 693.648193359375\n",
      "Epoch: 9200 | loss train: 648.8660888671875\n",
      "Epoch: 9300 | loss train: 718.642578125\n",
      "Epoch: 9400 | loss train: 656.6276245117188\n",
      "Epoch: 9500 | loss train: 624.2083740234375\n",
      "Epoch: 9600 | loss train: 616.6180419921875\n",
      "Epoch: 9700 | loss train: 612.0036010742188\n",
      "Epoch: 9800 | loss train: 653.6959228515625\n",
      "Epoch: 9900 | loss train: 649.8521118164062\n",
      "Epoch: 10000 | loss train: 642.8656616210938\n",
      "Epoch: 10100 | loss train: 603.54541015625\n",
      "Epoch: 10200 | loss train: 596.61474609375\n",
      "Epoch: 10300 | loss train: 660.671142578125\n",
      "Epoch: 10400 | loss train: 588.028076171875\n",
      "Epoch: 10500 | loss train: 579.3544921875\n",
      "Epoch: 10600 | loss train: 582.9561157226562\n",
      "Epoch: 10700 | loss train: 613.3173217773438\n",
      "Epoch: 10800 | loss train: 596.7498779296875\n",
      "Epoch: 10900 | loss train: 574.2696533203125\n",
      "Epoch: 11000 | loss train: 603.0742797851562\n",
      "Epoch: 11100 | loss train: 646.2084350585938\n",
      "Epoch: 11200 | loss train: 583.8948974609375\n",
      "Epoch: 11300 | loss train: 588.0398559570312\n",
      "Epoch: 11400 | loss train: 575.4801025390625\n",
      "Epoch: 11500 | loss train: 556.716552734375\n",
      "Epoch: 11600 | loss train: 632.7630004882812\n",
      "Epoch: 11700 | loss train: 555.8836059570312\n",
      "Epoch: 11800 | loss train: 571.0257568359375\n",
      "Epoch: 11900 | loss train: 598.9063720703125\n",
      "Epoch: 12000 | loss train: 547.9318237304688\n",
      "Epoch: 12100 | loss train: 565.524658203125\n",
      "Epoch: 12200 | loss train: 599.8348388671875\n",
      "Epoch: 12300 | loss train: 571.4598388671875\n",
      "Epoch: 12400 | loss train: 584.1785888671875\n",
      "Epoch: 12500 | loss train: 554.185546875\n",
      "Epoch: 12600 | loss train: 546.80322265625\n",
      "Epoch: 12700 | loss train: 539.764404296875\n",
      "Epoch: 12800 | loss train: 561.4330444335938\n",
      "Epoch: 12900 | loss train: 652.7630004882812\n",
      "Epoch: 13000 | loss train: 547.986083984375\n",
      "Epoch: 13100 | loss train: 537.7041625976562\n",
      "Epoch: 13200 | loss train: 571.1709594726562\n",
      "Epoch: 13300 | loss train: 676.3014526367188\n",
      "Epoch: 13400 | loss train: 550.0577392578125\n",
      "Epoch: 13500 | loss train: 595.7053833007812\n",
      "Epoch: 13600 | loss train: 528.0812377929688\n",
      "Epoch: 13700 | loss train: 704.19287109375\n",
      "Epoch: 13800 | loss train: 551.7693481445312\n",
      "Epoch: 13900 | loss train: 558.5551147460938\n",
      "Epoch: 14000 | loss train: 541.166015625\n",
      "Epoch: 14100 | loss train: 553.4738159179688\n",
      "Epoch: 14200 | loss train: 537.6968383789062\n",
      "Epoch: 14300 | loss train: 540.5431518554688\n",
      "Epoch: 14400 | loss train: 608.7750244140625\n",
      "Epoch: 14500 | loss train: 560.0695190429688\n",
      "Epoch: 14600 | loss train: 546.6678466796875\n",
      "Epoch: 14700 | loss train: 577.1869506835938\n",
      "Epoch: 14800 | loss train: 622.7214965820312\n",
      "Epoch: 14900 | loss train: 572.0465087890625\n",
      "Epoch: 15000 | loss train: 569.2418212890625\n",
      "Epoch: 15100 | loss train: 973.6408081054688\n",
      "Epoch: 15200 | loss train: 537.5448608398438\n",
      "Epoch: 15300 | loss train: 570.6092529296875\n",
      "Epoch: 15400 | loss train: 521.4901123046875\n",
      "Epoch: 15500 | loss train: 554.2847290039062\n",
      "Epoch: 15600 | loss train: 541.955810546875\n",
      "Epoch: 15700 | loss train: 518.966064453125\n",
      "Epoch: 15800 | loss train: 521.1978759765625\n",
      "Epoch: 15900 | loss train: 531.2959594726562\n",
      "Epoch: 16000 | loss train: 523.34716796875\n",
      "Epoch: 16100 | loss train: 605.2989501953125\n",
      "Epoch: 16200 | loss train: 547.66845703125\n",
      "Epoch: 16300 | loss train: 554.4559326171875\n",
      "Epoch: 16400 | loss train: 523.0811157226562\n",
      "Epoch: 16500 | loss train: 518.9965209960938\n",
      "Epoch: 16600 | loss train: 561.3273315429688\n",
      "Epoch: 16700 | loss train: 564.31298828125\n",
      "Epoch: 16800 | loss train: 526.0651245117188\n",
      "Epoch: 16900 | loss train: 584.2606201171875\n",
      "Epoch: 17000 | loss train: 525.9075927734375\n",
      "Epoch: 17100 | loss train: 553.0088500976562\n",
      "Epoch: 17200 | loss train: 517.2174682617188\n",
      "Epoch: 17300 | loss train: 544.502685546875\n",
      "Epoch: 17400 | loss train: 608.9519653320312\n",
      "Epoch: 17500 | loss train: 518.76611328125\n",
      "Epoch: 17600 | loss train: 851.6153564453125\n",
      "Epoch: 17700 | loss train: 527.5394897460938\n",
      "Epoch: 17800 | loss train: 543.5330200195312\n",
      "Epoch: 17900 | loss train: 517.69873046875\n",
      "Epoch: 18000 | loss train: 513.3135986328125\n",
      "Epoch: 18100 | loss train: 555.4315795898438\n",
      "Epoch: 18200 | loss train: 558.0578002929688\n",
      "Epoch: 18300 | loss train: 547.0187377929688\n",
      "Epoch: 18400 | loss train: 551.8171997070312\n",
      "Epoch: 18500 | loss train: 521.0758056640625\n",
      "Epoch: 18600 | loss train: 907.5950317382812\n",
      "Epoch: 18700 | loss train: 508.2978515625\n",
      "Epoch: 18800 | loss train: 602.6592407226562\n",
      "Epoch: 18900 | loss train: 522.7549438476562\n",
      "Epoch: 19000 | loss train: 527.9572143554688\n",
      "Epoch: 19100 | loss train: 518.6668701171875\n",
      "Epoch: 19200 | loss train: 523.069580078125\n",
      "Epoch: 19300 | loss train: 988.05615234375\n",
      "Epoch: 19400 | loss train: 565.3317260742188\n",
      "Epoch: 19500 | loss train: 525.2080078125\n",
      "Epoch: 19600 | loss train: 525.74658203125\n",
      "Epoch: 19700 | loss train: 551.2127685546875\n",
      "Epoch: 19800 | loss train: 532.4965209960938\n",
      "Epoch: 19900 | loss train: 563.783203125\n",
      "Epoch: 20000 | loss train: 517.5294189453125\n",
      "Epoch: 20100 | loss train: 515.8049926757812\n",
      "Epoch: 20200 | loss train: 517.2300415039062\n",
      "Epoch: 20300 | loss train: 542.5240478515625\n",
      "Epoch: 20400 | loss train: 514.30908203125\n",
      "Epoch: 20500 | loss train: 507.00042724609375\n",
      "Epoch: 20600 | loss train: 546.7993774414062\n",
      "Epoch: 20700 | loss train: 507.4377136230469\n",
      "Epoch: 20800 | loss train: 578.2542724609375\n",
      "Epoch: 20900 | loss train: 508.1147766113281\n",
      "Epoch: 21000 | loss train: 512.890869140625\n",
      "Epoch: 21100 | loss train: 506.3576965332031\n",
      "Epoch: 21200 | loss train: 546.3031616210938\n",
      "Epoch: 21300 | loss train: 500.1029968261719\n",
      "Epoch: 21400 | loss train: 550.6498413085938\n",
      "Epoch: 21500 | loss train: 512.433349609375\n",
      "Epoch: 21600 | loss train: 578.8270874023438\n",
      "Epoch: 21700 | loss train: 584.1194458007812\n",
      "Epoch: 21800 | loss train: 948.8012084960938\n",
      "Epoch: 21900 | loss train: 495.11322021484375\n",
      "Epoch: 22000 | loss train: 515.4899291992188\n",
      "Epoch: 22100 | loss train: 624.0625\n",
      "Epoch: 22200 | loss train: 500.9325866699219\n",
      "Epoch: 22300 | loss train: 513.5435180664062\n",
      "Epoch: 22400 | loss train: 643.8858642578125\n",
      "Epoch: 22500 | loss train: 505.8048400878906\n",
      "Epoch: 22600 | loss train: 501.7843933105469\n",
      "Epoch: 22700 | loss train: 497.9101257324219\n",
      "Epoch: 22800 | loss train: 569.5430908203125\n",
      "Epoch: 22900 | loss train: 539.42236328125\n",
      "Epoch: 23000 | loss train: 642.1275024414062\n",
      "Epoch: 23100 | loss train: 554.298828125\n",
      "Epoch: 23200 | loss train: 589.13427734375\n",
      "Epoch: 23300 | loss train: 487.9315490722656\n",
      "Epoch: 23400 | loss train: 488.1097717285156\n",
      "Epoch: 23500 | loss train: 497.9899597167969\n",
      "Epoch: 23600 | loss train: 503.2738342285156\n",
      "Epoch: 23700 | loss train: 489.5035400390625\n",
      "Epoch: 23800 | loss train: 499.559814453125\n",
      "Epoch: 23900 | loss train: 487.3963317871094\n",
      "Epoch: 24000 | loss train: 486.9976806640625\n",
      "Epoch: 24100 | loss train: 989.0040283203125\n",
      "Epoch: 24200 | loss train: 498.6918029785156\n",
      "Epoch: 24300 | loss train: 485.3183898925781\n",
      "Epoch: 24400 | loss train: 546.8426513671875\n",
      "Epoch: 24500 | loss train: 484.093994140625\n",
      "Epoch: 24600 | loss train: 489.3630676269531\n",
      "Epoch: 24700 | loss train: 566.0518798828125\n",
      "Epoch: 24800 | loss train: 508.3660583496094\n",
      "Epoch: 24900 | loss train: 487.7532043457031\n",
      "Epoch: 25000 | loss train: 488.2042236328125\n",
      "Epoch: 25100 | loss train: 490.8601379394531\n",
      "Epoch: 25200 | loss train: 759.2425537109375\n",
      "Epoch: 25300 | loss train: 512.0777587890625\n",
      "Epoch: 25400 | loss train: 492.8341369628906\n",
      "Epoch: 25500 | loss train: 497.7529602050781\n",
      "Epoch: 25600 | loss train: 481.64532470703125\n",
      "Epoch: 25700 | loss train: 498.00653076171875\n",
      "Epoch: 25800 | loss train: 523.8457641601562\n",
      "Epoch: 25900 | loss train: 676.9533081054688\n",
      "Epoch: 26000 | loss train: 678.1666259765625\n",
      "Epoch: 26100 | loss train: 579.283203125\n",
      "Epoch: 26200 | loss train: 483.14776611328125\n",
      "Epoch: 26300 | loss train: 490.7613830566406\n",
      "Epoch: 26400 | loss train: 510.1174621582031\n",
      "Epoch: 26500 | loss train: 545.499267578125\n",
      "Epoch: 26600 | loss train: 488.9775695800781\n",
      "Epoch: 26700 | loss train: 480.5572204589844\n",
      "Epoch: 26800 | loss train: 486.2931213378906\n",
      "Epoch: 26900 | loss train: 595.784912109375\n",
      "Epoch: 27000 | loss train: 525.9752197265625\n",
      "Epoch: 27100 | loss train: 484.469482421875\n",
      "Epoch: 27200 | loss train: 512.9461059570312\n",
      "Epoch: 27300 | loss train: 479.47210693359375\n",
      "Epoch: 27400 | loss train: 499.4941711425781\n",
      "Epoch: 27500 | loss train: 578.0860595703125\n",
      "Epoch: 27600 | loss train: 473.64471435546875\n",
      "Epoch: 27700 | loss train: 475.2275085449219\n",
      "Epoch: 27800 | loss train: 500.17803955078125\n",
      "Epoch: 27900 | loss train: 687.0050048828125\n",
      "Epoch: 28000 | loss train: 531.7090454101562\n",
      "Epoch: 28100 | loss train: 493.4864501953125\n",
      "Epoch: 28200 | loss train: 484.9783935546875\n",
      "Epoch: 28300 | loss train: 486.5116271972656\n",
      "Epoch: 28400 | loss train: 475.19024658203125\n",
      "Epoch: 28500 | loss train: 467.5262145996094\n",
      "Epoch: 28600 | loss train: 468.25433349609375\n",
      "Epoch: 28700 | loss train: 652.5264282226562\n",
      "Epoch: 28800 | loss train: 1383.102294921875\n",
      "Epoch: 28900 | loss train: 469.511474609375\n",
      "Epoch: 29000 | loss train: 462.46356201171875\n",
      "Epoch: 29100 | loss train: 490.73992919921875\n",
      "Epoch: 29200 | loss train: 574.8363647460938\n",
      "Epoch: 29300 | loss train: 459.2074279785156\n",
      "Epoch: 29400 | loss train: 501.5639343261719\n",
      "Epoch: 29500 | loss train: 814.987548828125\n",
      "Epoch: 29600 | loss train: 468.3375549316406\n",
      "Epoch: 29700 | loss train: 544.5972290039062\n",
      "Epoch: 29800 | loss train: 463.2020263671875\n",
      "Epoch: 29900 | loss train: 459.4920654296875\n",
      "Epoch: 30000 | loss train: 634.2850341796875\n",
      "Epoch: 30100 | loss train: 472.1225280761719\n",
      "Epoch: 30200 | loss train: 475.2314147949219\n",
      "Epoch: 30300 | loss train: 642.107421875\n",
      "Epoch: 30400 | loss train: 465.3841857910156\n",
      "Epoch: 30500 | loss train: 483.8210144042969\n",
      "Epoch: 30600 | loss train: 476.4717102050781\n",
      "Epoch: 30700 | loss train: 472.6566467285156\n",
      "Epoch: 30800 | loss train: 461.94952392578125\n",
      "Epoch: 30900 | loss train: 477.7668151855469\n",
      "Epoch: 31000 | loss train: 485.0540466308594\n",
      "Epoch: 31100 | loss train: 504.5591735839844\n",
      "Epoch: 31200 | loss train: 477.69403076171875\n",
      "Epoch: 31300 | loss train: 485.97540283203125\n",
      "Epoch: 31400 | loss train: 457.86444091796875\n",
      "Epoch: 31500 | loss train: 454.6119689941406\n",
      "Epoch: 31600 | loss train: 456.69976806640625\n",
      "Epoch: 31700 | loss train: 480.8570861816406\n",
      "Epoch: 31800 | loss train: 564.4035034179688\n",
      "Epoch: 31900 | loss train: 577.8629150390625\n",
      "Epoch: 32000 | loss train: 1160.9625244140625\n",
      "Epoch: 32100 | loss train: 459.2020568847656\n",
      "Epoch: 32200 | loss train: 454.2150573730469\n",
      "Epoch: 32300 | loss train: 463.5537109375\n",
      "Epoch: 32400 | loss train: 487.4598083496094\n",
      "Epoch: 32500 | loss train: 460.3288879394531\n",
      "Epoch: 32600 | loss train: 852.9569702148438\n",
      "Epoch: 32700 | loss train: 557.7462158203125\n",
      "Epoch: 32800 | loss train: 456.2338562011719\n",
      "Epoch: 32900 | loss train: 450.2084655761719\n",
      "Epoch: 33000 | loss train: 524.271240234375\n",
      "Epoch: 33100 | loss train: 462.2763977050781\n",
      "Epoch: 33200 | loss train: 446.58709716796875\n",
      "Epoch: 33300 | loss train: 448.04345703125\n",
      "Epoch: 33400 | loss train: 473.1177673339844\n",
      "Epoch: 33500 | loss train: 441.712646484375\n",
      "Epoch: 33600 | loss train: 567.5132446289062\n",
      "Epoch: 33700 | loss train: 476.75030517578125\n",
      "Epoch: 33800 | loss train: 445.4757385253906\n",
      "Epoch: 33900 | loss train: 442.5329895019531\n",
      "Epoch: 34000 | loss train: 442.1889343261719\n",
      "Epoch: 34100 | loss train: 443.1666564941406\n",
      "Epoch: 34200 | loss train: 453.298583984375\n",
      "Epoch: 34300 | loss train: 442.26654052734375\n",
      "Epoch: 34400 | loss train: 724.322265625\n",
      "Epoch: 34500 | loss train: 503.5419616699219\n",
      "Epoch: 34600 | loss train: 622.67578125\n",
      "Epoch: 34700 | loss train: 456.433349609375\n",
      "Epoch: 34800 | loss train: 449.239013671875\n",
      "Epoch: 34900 | loss train: 451.0672607421875\n",
      "Epoch: 35000 | loss train: 494.05718994140625\n",
      "Epoch: 35100 | loss train: 441.4796447753906\n",
      "Epoch: 35200 | loss train: 454.2898864746094\n",
      "Epoch: 35300 | loss train: 465.4138488769531\n",
      "Epoch: 35400 | loss train: 480.02593994140625\n",
      "Epoch: 35500 | loss train: 436.7896423339844\n",
      "Epoch: 35600 | loss train: 442.1019287109375\n",
      "Epoch: 35700 | loss train: 608.335205078125\n",
      "Epoch: 35800 | loss train: 457.1177978515625\n",
      "Epoch: 35900 | loss train: 432.8464660644531\n",
      "Epoch: 36000 | loss train: 498.15631103515625\n",
      "Epoch: 36100 | loss train: 482.3719482421875\n",
      "Epoch: 36200 | loss train: 819.101318359375\n",
      "Epoch: 36300 | loss train: 495.2998046875\n",
      "Epoch: 36400 | loss train: 438.651123046875\n",
      "Epoch: 36500 | loss train: 471.6429138183594\n",
      "Epoch: 36600 | loss train: 428.2860412597656\n",
      "Epoch: 36700 | loss train: 454.63262939453125\n",
      "Epoch: 36800 | loss train: 433.335205078125\n",
      "Epoch: 36900 | loss train: 423.8491516113281\n",
      "Epoch: 37000 | loss train: 415.5985412597656\n",
      "Epoch: 37100 | loss train: 466.36358642578125\n",
      "Epoch: 37200 | loss train: 411.60809326171875\n",
      "Epoch: 37300 | loss train: 411.8495788574219\n",
      "Epoch: 37400 | loss train: 420.2573547363281\n",
      "Epoch: 37500 | loss train: 1008.9340209960938\n",
      "Epoch: 37600 | loss train: 419.2934265136719\n",
      "Epoch: 37700 | loss train: 416.337158203125\n",
      "Epoch: 37800 | loss train: 508.2108154296875\n",
      "Epoch: 37900 | loss train: 460.8919982910156\n",
      "Epoch: 38000 | loss train: 408.55596923828125\n",
      "Epoch: 38100 | loss train: 404.8199157714844\n",
      "Epoch: 38200 | loss train: 398.59912109375\n",
      "Epoch: 38300 | loss train: 488.26318359375\n",
      "Epoch: 38400 | loss train: 399.4734191894531\n",
      "Epoch: 38500 | loss train: 537.88623046875\n",
      "Epoch: 38600 | loss train: 515.990234375\n",
      "Epoch: 38700 | loss train: 395.7275695800781\n",
      "Epoch: 38800 | loss train: 445.08587646484375\n",
      "Epoch: 38900 | loss train: 413.6582336425781\n",
      "Epoch: 39000 | loss train: 411.301513671875\n",
      "Epoch: 39100 | loss train: 390.5720520019531\n",
      "Epoch: 39200 | loss train: 419.54876708984375\n",
      "Epoch: 39300 | loss train: 383.7279968261719\n",
      "Epoch: 39400 | loss train: 387.2574157714844\n",
      "Epoch: 39500 | loss train: 370.67559814453125\n",
      "Epoch: 39600 | loss train: 414.6886901855469\n",
      "Epoch: 39700 | loss train: 373.3317565917969\n",
      "Epoch: 39800 | loss train: 416.3022766113281\n",
      "Epoch: 39900 | loss train: 361.024169921875\n",
      "loss_NN=tensor(4752.9033)\n",
      "pred_NN=tensor([[7189.6694],\n",
      "        [7493.6494],\n",
      "        [8876.6572],\n",
      "        [6134.9263]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sample = generate_sample(data_df_13_09_2022, bands_ix, target_value, train_field)\n",
    "X_train, Y_train = create_X_train_Y_train(sample, bands_ix)\n",
    "sample_target = generate_sample(data_df_13_09_2022, bands_ix, target_value, \"BC\")\n",
    "X_target, Y_target = create_X_train_Y_train(sample_target, bands_ix)\n",
    "super_param={\"lr\": 0.0001, \"weight_decay\": 1e-5, \"n_epochs\": 40000, \"stop_value\": 0.5}\n",
    "re_run = \"Y\"\n",
    "loss_NN, pred_NN = predict_using_neutral_network(X_train, Y_train, X_target, Y_target, get_full_path(f\"../../model_saved/NN_save/using_mutual_information/{train_field}_preidct_{target_value}_{str(datetime.now().date())}_using_{function_get.__name__}.pt\"), super_param, re_run)\n",
    "print(f\"{loss_NN=}\")\n",
    "print(f\"{pred_NN=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

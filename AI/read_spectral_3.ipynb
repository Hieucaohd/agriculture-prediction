{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import các hàm cần thiết\n",
    "\n",
    "import spectral.io.envi as envi\t\t# hàm để đọc file .hdr và file .img\n",
    "import numpy as np \t\t\t\t\t# để thao tác với ma trận\n",
    "from spectral import open_image, imshow \t# để hiển thị hình ảnh\n",
    "import pandas as pd                         # để đọc file excel\n",
    "from typing import LiteralString\n",
    "from sklearn.metrics import mean_squared_error #để tính Mean squared error\n",
    "from sklearn import tree\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from typing import Literal\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAND_START_IX = 4\n",
    "BAND_END_IX = 101\n",
    "BAND_START_NAME = f\"band_{BAND_START_IX}\"\n",
    "BAND_END_NAME = f\"band_{BAND_END_IX}\"\n",
    "TOTAL_BAND = BAND_END_IX - BAND_START_IX + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'band_4'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BAND_START_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'band_101'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BAND_END_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOTAL_BAND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mở file .hdr và file .img\n",
    "img = envi.open(\"hyper_20220913_3cm.hdr\", \"hyper_20220913_3cm.img\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\tData Source:   '.\\hyper_20220913_3cm.img'\n",
       "\t# Rows:           6478\n",
       "\t# Samples:        5287\n",
       "\t# Bands:           122\n",
       "\tInterleave:        BSQ\n",
       "\tQuantization:   8 bits\n",
       "\tData format:     uint8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xem các thông tin cơ bản của file\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6478, 5287, 122)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xem kích cỡ của ma trận\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bands(row_num, col_num):\n",
    "    return img[row_num, col_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hiển thị hình ảnh chụp được\n",
    "# view = imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hàm này trả về các pixel nằm trong hình vuông có tọa độ (row_num, col_num) là tâm\n",
    "# và độ dài cạnh là (2 * scopes + 1)\n",
    "def pixel_in_scope(row_num, col_num, scopes):\n",
    "    pixel_in_scope = set()\n",
    "\n",
    "    for scope in range(1, scopes + 1):\n",
    "        \n",
    "        for i in range(row_num - scope, row_num + scope + 1):\n",
    "            pixel_in_scope.add((i, col_num + scope))\n",
    "            pixel_in_scope.add((i, col_num - scope))\n",
    "            \n",
    "        for i in range(col_num - scope, col_num + scope + 1):\n",
    "            pixel_in_scope.add((row_num + scope, i))\n",
    "            pixel_in_scope.add((row_num - scope, i))\n",
    "            \n",
    "    pixel_in_scope.add((row_num, col_num))\n",
    "    return pixel_in_scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lấy thông tin về map\n",
    "with open(\"hyper_20220913_3cm.hdr\", \"r\") as file:\n",
    "    list_lines = file.read().split(\"\\n\")\n",
    "    list_lines = filter(lambda line: \"map info\" in line, list_lines)\n",
    "    map_infor = list(list_lines)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'map info = {UTM, 1.000, 1.000, 530499.467, 2355871.685, 3.0000000000e-002, 3.0000000000e-002, 48, North, WGS-84, units=Meters}'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_infor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thông tin của map, tạm thời được đưa vào bằng tay\n",
    "# \"x1_pixel\": tọa độ pixel x của điểm 1\n",
    "# \"y1_pixel\": tọa độ pixel y của điểm 1\n",
    "# \"x1_axes\": tọa độ trái đất x của điểm 1\n",
    "# \"y1_axes\": tọa độ trái đất y của điểm 1\n",
    "# \"x_resolution\": độ phân giải x của map\n",
    "# \"y_resolution\": độ phân giải y của map\n",
    "map_infor = {\n",
    "    \"col_1\": 1.000,\n",
    "    \"row_1\": 1.000,\n",
    "    \"east_1\": 530499.467,\n",
    "    \"north_1\": 2355871.685,\n",
    "    \"col_resolution\": 3.0000000000e-002,\n",
    "    \"row_resolution\": 3.0000000000e-002\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'col_1': 1.0,\n",
       " 'row_1': 1.0,\n",
       " 'east_1': 530499.467,\n",
       " 'north_1': 2355871.685,\n",
       " 'col_resolution': 0.03,\n",
       " 'row_resolution': 0.03}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_infor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# công thức lấy tọa độ trái đất khi biết tạo độ trái đất của điểm có tạo độ pixel (1, 1)\n",
    "# (East - 530499.467 ) / 0.03 = số cột\n",
    "# (2355871.685 - north) / 0.03 = cố hàng\n",
    "# suy ra:\n",
    "# East = 530499.467 + so cột * 0.03\n",
    "# North = 2355871.685 - so hàng * 0.03 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# từ tọa độ pixel, lấy tọa độ trái đất\n",
    "def get_axes(row_num, col_num):\n",
    "    east = map_infor[\"east_1\"] + col_num * map_infor[\"col_resolution\"]  \n",
    "    north = map_infor[\"north_1\"] - row_num * map_infor[\"row_resolution\"]\n",
    "    return (north, east)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# từ tọa độ trái đất, lấy tọa độ pixel\n",
    "def get_pixel(north, east):\n",
    "    col_num = (east - map_infor[\"east_1\"]) / map_infor[\"col_resolution\"]\n",
    "    row_num = (map_infor[\"north_1\"] - north) / map_infor[\"row_resolution\"]\n",
    "    \n",
    "    col_num = int(round(col_num, 0))\n",
    "    row_num = int(round(row_num, 0))\n",
    "    return (row_num, col_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# đọc file excel chứa data của NPK và tọa độ Trái Đất\n",
    "data_df = pd.read_excel(\"DATA_Mua2_PhuTho_2022_3.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Treatment</th>\n",
       "      <th>Replicate</th>\n",
       "      <th>Sub-Replicate</th>\n",
       "      <th>East</th>\n",
       "      <th>North</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Chlorophyll</th>\n",
       "      <th>Rice_Height</th>\n",
       "      <th>Spectral_number</th>\n",
       "      <th>Digesion (g)</th>\n",
       "      <th>...</th>\n",
       "      <th>Chlorophyll-a.1</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "      <th>Wet weight (g)</th>\n",
       "      <th>Dried weight (g)</th>\n",
       "      <th>% Moiture</th>\n",
       "      <th>Digesion (g).1</th>\n",
       "      <th>P conc. (mg/kg).2</th>\n",
       "      <th>K conc. (mg/kg).2</th>\n",
       "      <th>N conc. (mg/kg).2</th>\n",
       "      <th>Chlorophyll-a.2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9/8/2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23/9/2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>T1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>530606.899</td>\n",
       "      <td>2355789.223</td>\n",
       "      <td>15.195</td>\n",
       "      <td>35.2</td>\n",
       "      <td>60.0</td>\n",
       "      <td>52</td>\n",
       "      <td>0.2497</td>\n",
       "      <td>...</td>\n",
       "      <td>35.8</td>\n",
       "      <td>T1.1</td>\n",
       "      <td>2355788.259</td>\n",
       "      <td>530607.202</td>\n",
       "      <td>15.180</td>\n",
       "      <td>0.2496</td>\n",
       "      <td>4408.453526</td>\n",
       "      <td>11414.262821</td>\n",
       "      <td>1366.862450</td>\n",
       "      <td>31.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>530602.233</td>\n",
       "      <td>2355788.619</td>\n",
       "      <td>15.198</td>\n",
       "      <td>38.1</td>\n",
       "      <td>58.0</td>\n",
       "      <td>53</td>\n",
       "      <td>0.2492</td>\n",
       "      <td>...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>T1.2</td>\n",
       "      <td>2355787.076</td>\n",
       "      <td>530602.173</td>\n",
       "      <td>15.187</td>\n",
       "      <td>0.2497</td>\n",
       "      <td>4888.165799</td>\n",
       "      <td>13192.831398</td>\n",
       "      <td>1696.964518</td>\n",
       "      <td>33.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>530597.380</td>\n",
       "      <td>2355787.695</td>\n",
       "      <td>15.211</td>\n",
       "      <td>30.7</td>\n",
       "      <td>64.0</td>\n",
       "      <td>54</td>\n",
       "      <td>0.2509</td>\n",
       "      <td>...</td>\n",
       "      <td>34.9</td>\n",
       "      <td>T1.3</td>\n",
       "      <td>2355786.113</td>\n",
       "      <td>530597.674</td>\n",
       "      <td>15.209</td>\n",
       "      <td>0.2494</td>\n",
       "      <td>4457.999198</td>\n",
       "      <td>14094.827586</td>\n",
       "      <td>1328.177670</td>\n",
       "      <td>32.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>T2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>530592.844</td>\n",
       "      <td>2355790.036</td>\n",
       "      <td>15.194</td>\n",
       "      <td>31.3</td>\n",
       "      <td>63.0</td>\n",
       "      <td>55</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>...</td>\n",
       "      <td>29.8</td>\n",
       "      <td>T2.1</td>\n",
       "      <td>2355789.548</td>\n",
       "      <td>530594.048</td>\n",
       "      <td>15.217</td>\n",
       "      <td>0.2495</td>\n",
       "      <td>4356.713427</td>\n",
       "      <td>13018.036072</td>\n",
       "      <td>4760.907380</td>\n",
       "      <td>30.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>NaN</td>\n",
       "      <td>BC2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>530553.290</td>\n",
       "      <td>2355759.834</td>\n",
       "      <td>15.213</td>\n",
       "      <td>35.4</td>\n",
       "      <td>70.0</td>\n",
       "      <td>107</td>\n",
       "      <td>0.2502</td>\n",
       "      <td>...</td>\n",
       "      <td>33.2</td>\n",
       "      <td>BC2</td>\n",
       "      <td>2355760.894</td>\n",
       "      <td>530555.404</td>\n",
       "      <td>15.207</td>\n",
       "      <td>0.2495</td>\n",
       "      <td>5666.833667</td>\n",
       "      <td>15003.006012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>NaN</td>\n",
       "      <td>BC3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>530592.596</td>\n",
       "      <td>2355733.669</td>\n",
       "      <td>15.178</td>\n",
       "      <td>42.1</td>\n",
       "      <td>70.0</td>\n",
       "      <td>108</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>BC3</td>\n",
       "      <td>2355732.521</td>\n",
       "      <td>530591.790</td>\n",
       "      <td>15.185</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>5459.300000</td>\n",
       "      <td>10918.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>NaN</td>\n",
       "      <td>BC4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>530622.914</td>\n",
       "      <td>2355777.478</td>\n",
       "      <td>15.030</td>\n",
       "      <td>43.9</td>\n",
       "      <td>68.0</td>\n",
       "      <td>109-110</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>...</td>\n",
       "      <td>41.8</td>\n",
       "      <td>BC4</td>\n",
       "      <td>2355776.865</td>\n",
       "      <td>530622.044</td>\n",
       "      <td>15.024</td>\n",
       "      <td>0.2504</td>\n",
       "      <td>6248.901757</td>\n",
       "      <td>13170.926518</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>NaN</td>\n",
       "      <td>BC5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>530618.803</td>\n",
       "      <td>2355796.165</td>\n",
       "      <td>15.220</td>\n",
       "      <td>41.4</td>\n",
       "      <td>75.0</td>\n",
       "      <td>111</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>BC5</td>\n",
       "      <td>2355798.179</td>\n",
       "      <td>530620.573</td>\n",
       "      <td>15.186</td>\n",
       "      <td>0.2503</td>\n",
       "      <td>6137.135437</td>\n",
       "      <td>12318.218138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>NaN</td>\n",
       "      <td>BC6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>530608.802</td>\n",
       "      <td>2355825.365</td>\n",
       "      <td>15.031</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>39.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2505</td>\n",
       "      <td>5499.800399</td>\n",
       "      <td>12195.608782</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Treatment Replicate  Sub-Replicate        East        North  Elevation  \\\n",
       "0   9/8/2022       NaN            NaN         NaN          NaN        NaN   \n",
       "1        NaN        T1            1.0  530606.899  2355789.223     15.195   \n",
       "2        NaN       NaN            2.0  530602.233  2355788.619     15.198   \n",
       "3        NaN       NaN            3.0  530597.380  2355787.695     15.211   \n",
       "4        NaN        T2            1.0  530592.844  2355790.036     15.194   \n",
       "..       ...       ...            ...         ...          ...        ...   \n",
       "74       NaN       BC2            NaN  530553.290  2355759.834     15.213   \n",
       "75       NaN       BC3            NaN  530592.596  2355733.669     15.178   \n",
       "76       NaN       BC4            NaN  530622.914  2355777.478     15.030   \n",
       "77       NaN       BC5            NaN  530618.803  2355796.165     15.220   \n",
       "78       NaN       BC6            NaN  530608.802  2355825.365     15.031   \n",
       "\n",
       "    Chlorophyll  Rice_Height Spectral_number  Digesion (g)  ...  \\\n",
       "0           NaN          NaN             NaN           NaN  ...   \n",
       "1          35.2         60.0              52        0.2497  ...   \n",
       "2          38.1         58.0              53        0.2492  ...   \n",
       "3          30.7         64.0              54        0.2509  ...   \n",
       "4          31.3         63.0              55        0.2500  ...   \n",
       "..          ...          ...             ...           ...  ...   \n",
       "74         35.4         70.0             107        0.2502  ...   \n",
       "75         42.1         70.0             108        0.2500  ...   \n",
       "76         43.9         68.0         109-110        0.2500  ...   \n",
       "77         41.4         75.0             111        0.2500  ...   \n",
       "78          NaN          NaN             NaN           NaN  ...   \n",
       "\n",
       "    Chlorophyll-a.1  Unnamed: 22  Wet weight (g)  Dried weight (g) % Moiture  \\\n",
       "0               NaN    23/9/2022             NaN               NaN       NaN   \n",
       "1              35.8         T1.1     2355788.259        530607.202    15.180   \n",
       "2              34.0         T1.2     2355787.076        530602.173    15.187   \n",
       "3              34.9         T1.3     2355786.113        530597.674    15.209   \n",
       "4              29.8         T2.1     2355789.548        530594.048    15.217   \n",
       "..              ...          ...             ...               ...       ...   \n",
       "74             33.2          BC2     2355760.894        530555.404    15.207   \n",
       "75             40.0          BC3     2355732.521        530591.790    15.185   \n",
       "76             41.8          BC4     2355776.865        530622.044    15.024   \n",
       "77             35.0          BC5     2355798.179        530620.573    15.186   \n",
       "78             39.6          NaN             NaN               NaN       NaN   \n",
       "\n",
       "    Digesion (g).1  P conc. (mg/kg).2  K conc. (mg/kg).2  N conc. (mg/kg).2  \\\n",
       "0              NaN                NaN                NaN                NaN   \n",
       "1           0.2496        4408.453526       11414.262821        1366.862450   \n",
       "2           0.2497        4888.165799       13192.831398        1696.964518   \n",
       "3           0.2494        4457.999198       14094.827586        1328.177670   \n",
       "4           0.2495        4356.713427       13018.036072        4760.907380   \n",
       "..             ...                ...                ...                ...   \n",
       "74          0.2495        5666.833667       15003.006012                NaN   \n",
       "75          0.2500        5459.300000       10918.000000                NaN   \n",
       "76          0.2504        6248.901757       13170.926518                NaN   \n",
       "77          0.2503        6137.135437       12318.218138                NaN   \n",
       "78          0.2505        5499.800399       12195.608782                NaN   \n",
       "\n",
       "    Chlorophyll-a.2  \n",
       "0               NaN  \n",
       "1              31.7  \n",
       "2              33.6  \n",
       "3              32.4  \n",
       "4              30.2  \n",
       "..              ...  \n",
       "74             32.4  \n",
       "75             41.4  \n",
       "76             35.9  \n",
       "77             29.6  \n",
       "78             34.3  \n",
       "\n",
       "[79 rows x 31 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Treatment', 'Replicate', 'Sub-Replicate', 'East', 'North', 'Elevation',\n",
       "       'Chlorophyll', 'Rice_Height', 'Spectral_number', 'Digesion (g)',\n",
       "       'P conc. (mg/kg)', 'K conc. (mg/kg)', 'N conc. (mg/kg)',\n",
       "       'Chlorophyll-a', 'Unnamed: 14', 'East.1', 'North.1', 'Elevation.1',\n",
       "       'P conc. (mg/kg).1', 'K conc. (mg/kg).1', 'N conc. (mg/kg).1',\n",
       "       'Chlorophyll-a.1', 'Unnamed: 22', 'Wet weight (g)', 'Dried weight (g)',\n",
       "       '% Moiture', 'Digesion (g).1', 'P conc. (mg/kg).2', 'K conc. (mg/kg).2',\n",
       "       'N conc. (mg/kg).2', 'Chlorophyll-a.2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 79 entries, 0 to 78\n",
      "Data columns (total 31 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Treatment          1 non-null      object \n",
      " 1   Replicate          30 non-null     object \n",
      " 2   Sub-Replicate      72 non-null     float64\n",
      " 3   East               78 non-null     float64\n",
      " 4   North              78 non-null     float64\n",
      " 5   Elevation          78 non-null     float64\n",
      " 6   Chlorophyll        77 non-null     float64\n",
      " 7   Rice_Height        77 non-null     float64\n",
      " 8   Spectral_number    77 non-null     object \n",
      " 9   Digesion (g)       77 non-null     float64\n",
      " 10  P conc. (mg/kg)    77 non-null     float64\n",
      " 11  K conc. (mg/kg)    77 non-null     float64\n",
      " 12  N conc. (mg/kg)    77 non-null     float64\n",
      " 13  Chlorophyll-a      77 non-null     float64\n",
      " 14  Unnamed: 14        77 non-null     object \n",
      " 15  East.1             76 non-null     float64\n",
      " 16  North.1            76 non-null     float64\n",
      " 17  Elevation.1        76 non-null     float64\n",
      " 18  P conc. (mg/kg).1  78 non-null     float64\n",
      " 19  K conc. (mg/kg).1  78 non-null     float64\n",
      " 20  N conc. (mg/kg).1  78 non-null     float64\n",
      " 21  Chlorophyll-a.1    78 non-null     float64\n",
      " 22  Unnamed: 22        78 non-null     object \n",
      " 23  Wet weight (g)     77 non-null     float64\n",
      " 24  Dried weight (g)   77 non-null     float64\n",
      " 25  % Moiture          77 non-null     float64\n",
      " 26  Digesion (g).1     78 non-null     float64\n",
      " 27  P conc. (mg/kg).2  78 non-null     float64\n",
      " 28  K conc. (mg/kg).2  78 non-null     float64\n",
      " 29  N conc. (mg/kg).2  36 non-null     float64\n",
      " 30  Chlorophyll-a.2    78 non-null     float64\n",
      "dtypes: float64(26), object(5)\n",
      "memory usage: 19.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lấy cột dữ liệu đo được của ngày 13/9/2022\n",
    "data_df_13_09_2022 = data_df.loc[1:, \"Unnamed: 14\":\"Chlorophyll-a.1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_13_09_2022.rename(columns={\n",
    "    \"Unnamed: 14\": \"type_of_field\"\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bỏ đi những dòng bị thiếu tên điểm \n",
    "data_df_13_09_2022 = data_df_13_09_2022[~pd.isna(data_df_13_09_2022[\"type_of_field\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type_of_field</th>\n",
       "      <th>East.1</th>\n",
       "      <th>North.1</th>\n",
       "      <th>Elevation.1</th>\n",
       "      <th>P conc. (mg/kg).1</th>\n",
       "      <th>K conc. (mg/kg).1</th>\n",
       "      <th>N conc. (mg/kg).1</th>\n",
       "      <th>Chlorophyll-a.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T1.1</td>\n",
       "      <td>530605.915</td>\n",
       "      <td>2355788.377</td>\n",
       "      <td>15.193</td>\n",
       "      <td>6098.182181</td>\n",
       "      <td>12505.992809</td>\n",
       "      <td>4311.970060</td>\n",
       "      <td>35.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T1.2</td>\n",
       "      <td>530601.735</td>\n",
       "      <td>2355788.448</td>\n",
       "      <td>15.178</td>\n",
       "      <td>5037.174349</td>\n",
       "      <td>12338.677355</td>\n",
       "      <td>4787.270920</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T1.3</td>\n",
       "      <td>530597.008</td>\n",
       "      <td>2355787.118</td>\n",
       "      <td>15.206</td>\n",
       "      <td>6471.319695</td>\n",
       "      <td>14923.786602</td>\n",
       "      <td>5286.813480</td>\n",
       "      <td>34.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T2.1</td>\n",
       "      <td>530593.342</td>\n",
       "      <td>2355789.827</td>\n",
       "      <td>15.215</td>\n",
       "      <td>6433.340016</td>\n",
       "      <td>17184.242181</td>\n",
       "      <td>3066.571650</td>\n",
       "      <td>29.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T2.2</td>\n",
       "      <td>530591.652</td>\n",
       "      <td>2355794.783</td>\n",
       "      <td>15.236</td>\n",
       "      <td>5679.671869</td>\n",
       "      <td>13274.309724</td>\n",
       "      <td>3433.145680</td>\n",
       "      <td>36.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>J12.3</td>\n",
       "      <td>530582.155</td>\n",
       "      <td>2355781.790</td>\n",
       "      <td>15.238</td>\n",
       "      <td>7009.292566</td>\n",
       "      <td>16528.776978</td>\n",
       "      <td>7556.516150</td>\n",
       "      <td>38.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>BC2</td>\n",
       "      <td>530553.718</td>\n",
       "      <td>2355758.342</td>\n",
       "      <td>15.224</td>\n",
       "      <td>5885.628743</td>\n",
       "      <td>14264.471058</td>\n",
       "      <td>2265.424250</td>\n",
       "      <td>33.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>BC3</td>\n",
       "      <td>530591.520</td>\n",
       "      <td>2355732.444</td>\n",
       "      <td>15.206</td>\n",
       "      <td>7505.289421</td>\n",
       "      <td>16167.664671</td>\n",
       "      <td>3268.614069</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>BC4</td>\n",
       "      <td>530622.015</td>\n",
       "      <td>2355776.121</td>\n",
       "      <td>15.086</td>\n",
       "      <td>6279.883674</td>\n",
       "      <td>10340.954673</td>\n",
       "      <td>2980.889693</td>\n",
       "      <td>41.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>BC5</td>\n",
       "      <td>530619.755</td>\n",
       "      <td>2355797.647</td>\n",
       "      <td>15.229</td>\n",
       "      <td>6624.348175</td>\n",
       "      <td>9675.391095</td>\n",
       "      <td>2460.532560</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   type_of_field      East.1      North.1  Elevation.1  P conc. (mg/kg).1  \\\n",
       "1           T1.1  530605.915  2355788.377       15.193        6098.182181   \n",
       "2           T1.2  530601.735  2355788.448       15.178        5037.174349   \n",
       "3           T1.3  530597.008  2355787.118       15.206        6471.319695   \n",
       "4           T2.1  530593.342  2355789.827       15.215        6433.340016   \n",
       "5           T2.2  530591.652  2355794.783       15.236        5679.671869   \n",
       "..           ...         ...          ...          ...                ...   \n",
       "72         J12.3  530582.155  2355781.790       15.238        7009.292566   \n",
       "74           BC2  530553.718  2355758.342       15.224        5885.628743   \n",
       "75           BC3  530591.520  2355732.444       15.206        7505.289421   \n",
       "76           BC4  530622.015  2355776.121       15.086        6279.883674   \n",
       "77           BC5  530619.755  2355797.647       15.229        6624.348175   \n",
       "\n",
       "    K conc. (mg/kg).1  N conc. (mg/kg).1  Chlorophyll-a.1  \n",
       "1        12505.992809        4311.970060             35.8  \n",
       "2        12338.677355        4787.270920             34.0  \n",
       "3        14923.786602        5286.813480             34.9  \n",
       "4        17184.242181        3066.571650             29.8  \n",
       "5        13274.309724        3433.145680             36.1  \n",
       "..                ...                ...              ...  \n",
       "72       16528.776978        7556.516150             38.6  \n",
       "74       14264.471058        2265.424250             33.2  \n",
       "75       16167.664671        3268.614069             40.0  \n",
       "76       10340.954673        2980.889693             41.8  \n",
       "77        9675.391095        2460.532560             35.0  \n",
       "\n",
       "[76 rows x 8 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df_13_09_2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 76 entries, 1 to 77\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   type_of_field      76 non-null     object \n",
      " 1   East.1             76 non-null     float64\n",
      " 2   North.1            76 non-null     float64\n",
      " 3   Elevation.1        76 non-null     float64\n",
      " 4   P conc. (mg/kg).1  76 non-null     float64\n",
      " 5   K conc. (mg/kg).1  76 non-null     float64\n",
      " 6   N conc. (mg/kg).1  76 non-null     float64\n",
      " 7   Chlorophyll-a.1    76 non-null     float64\n",
      "dtypes: float64(7), object(1)\n",
      "memory usage: 5.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data_df_13_09_2022.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo thêm cột 2 tọa độ pixel\n",
    "data_df_13_09_2022[\"row_num\"] = [None] * len(data_df_13_09_2022)\n",
    "data_df_13_09_2022[\"col_num\"] = [None] * len(data_df_13_09_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type_of_field</th>\n",
       "      <th>East.1</th>\n",
       "      <th>North.1</th>\n",
       "      <th>Elevation.1</th>\n",
       "      <th>P conc. (mg/kg).1</th>\n",
       "      <th>K conc. (mg/kg).1</th>\n",
       "      <th>N conc. (mg/kg).1</th>\n",
       "      <th>Chlorophyll-a.1</th>\n",
       "      <th>row_num</th>\n",
       "      <th>col_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T1.1</td>\n",
       "      <td>530605.915</td>\n",
       "      <td>2355788.377</td>\n",
       "      <td>15.193</td>\n",
       "      <td>6098.182181</td>\n",
       "      <td>12505.992809</td>\n",
       "      <td>4311.970060</td>\n",
       "      <td>35.8</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T1.2</td>\n",
       "      <td>530601.735</td>\n",
       "      <td>2355788.448</td>\n",
       "      <td>15.178</td>\n",
       "      <td>5037.174349</td>\n",
       "      <td>12338.677355</td>\n",
       "      <td>4787.270920</td>\n",
       "      <td>34.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T1.3</td>\n",
       "      <td>530597.008</td>\n",
       "      <td>2355787.118</td>\n",
       "      <td>15.206</td>\n",
       "      <td>6471.319695</td>\n",
       "      <td>14923.786602</td>\n",
       "      <td>5286.813480</td>\n",
       "      <td>34.9</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T2.1</td>\n",
       "      <td>530593.342</td>\n",
       "      <td>2355789.827</td>\n",
       "      <td>15.215</td>\n",
       "      <td>6433.340016</td>\n",
       "      <td>17184.242181</td>\n",
       "      <td>3066.571650</td>\n",
       "      <td>29.8</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T2.2</td>\n",
       "      <td>530591.652</td>\n",
       "      <td>2355794.783</td>\n",
       "      <td>15.236</td>\n",
       "      <td>5679.671869</td>\n",
       "      <td>13274.309724</td>\n",
       "      <td>3433.145680</td>\n",
       "      <td>36.1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>J12.3</td>\n",
       "      <td>530582.155</td>\n",
       "      <td>2355781.790</td>\n",
       "      <td>15.238</td>\n",
       "      <td>7009.292566</td>\n",
       "      <td>16528.776978</td>\n",
       "      <td>7556.516150</td>\n",
       "      <td>38.6</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>BC2</td>\n",
       "      <td>530553.718</td>\n",
       "      <td>2355758.342</td>\n",
       "      <td>15.224</td>\n",
       "      <td>5885.628743</td>\n",
       "      <td>14264.471058</td>\n",
       "      <td>2265.424250</td>\n",
       "      <td>33.2</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>BC3</td>\n",
       "      <td>530591.520</td>\n",
       "      <td>2355732.444</td>\n",
       "      <td>15.206</td>\n",
       "      <td>7505.289421</td>\n",
       "      <td>16167.664671</td>\n",
       "      <td>3268.614069</td>\n",
       "      <td>40.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>BC4</td>\n",
       "      <td>530622.015</td>\n",
       "      <td>2355776.121</td>\n",
       "      <td>15.086</td>\n",
       "      <td>6279.883674</td>\n",
       "      <td>10340.954673</td>\n",
       "      <td>2980.889693</td>\n",
       "      <td>41.8</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>BC5</td>\n",
       "      <td>530619.755</td>\n",
       "      <td>2355797.647</td>\n",
       "      <td>15.229</td>\n",
       "      <td>6624.348175</td>\n",
       "      <td>9675.391095</td>\n",
       "      <td>2460.532560</td>\n",
       "      <td>35.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   type_of_field      East.1      North.1  Elevation.1  P conc. (mg/kg).1  \\\n",
       "1           T1.1  530605.915  2355788.377       15.193        6098.182181   \n",
       "2           T1.2  530601.735  2355788.448       15.178        5037.174349   \n",
       "3           T1.3  530597.008  2355787.118       15.206        6471.319695   \n",
       "4           T2.1  530593.342  2355789.827       15.215        6433.340016   \n",
       "5           T2.2  530591.652  2355794.783       15.236        5679.671869   \n",
       "..           ...         ...          ...          ...                ...   \n",
       "72         J12.3  530582.155  2355781.790       15.238        7009.292566   \n",
       "74           BC2  530553.718  2355758.342       15.224        5885.628743   \n",
       "75           BC3  530591.520  2355732.444       15.206        7505.289421   \n",
       "76           BC4  530622.015  2355776.121       15.086        6279.883674   \n",
       "77           BC5  530619.755  2355797.647       15.229        6624.348175   \n",
       "\n",
       "    K conc. (mg/kg).1  N conc. (mg/kg).1  Chlorophyll-a.1 row_num col_num  \n",
       "1        12505.992809        4311.970060             35.8    None    None  \n",
       "2        12338.677355        4787.270920             34.0    None    None  \n",
       "3        14923.786602        5286.813480             34.9    None    None  \n",
       "4        17184.242181        3066.571650             29.8    None    None  \n",
       "5        13274.309724        3433.145680             36.1    None    None  \n",
       "..                ...                ...              ...     ...     ...  \n",
       "72       16528.776978        7556.516150             38.6    None    None  \n",
       "74       14264.471058        2265.424250             33.2    None    None  \n",
       "75       16167.664671        3268.614069             40.0    None    None  \n",
       "76       10340.954673        2980.889693             41.8    None    None  \n",
       "77        9675.391095        2460.532560             35.0    None    None  \n",
       "\n",
       "[76 rows x 10 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df_13_09_2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['type_of_field', 'East.1', 'North.1', 'Elevation.1',\n",
       "       'P conc. (mg/kg).1', 'K conc. (mg/kg).1', 'N conc. (mg/kg).1',\n",
       "       'Chlorophyll-a.1', 'row_num', 'col_num'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df_13_09_2022.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Định nghĩa hàm tính tọa độ pixel của điểm thực đo ngoài thực nghiệm\n",
    "def fill_row_num_and_col_num(index, row, df: pd.DataFrame):\n",
    "    east = row[\"East.1\"]\n",
    "    north = row[\"North.1\"]\n",
    "    row_num, col_num = get_pixel(north, east)\n",
    "    df.at[index, \"row_num\"] = row_num\n",
    "    df.at[index, \"col_num\"] = col_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lặp hàm tính tọa độ pixel trên cho từng dòng\n",
    "for index, row in data_df_13_09_2022.iterrows():\n",
    "    fill_row_num_and_col_num(index, row, data_df_13_09_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type_of_field</th>\n",
       "      <th>East.1</th>\n",
       "      <th>North.1</th>\n",
       "      <th>Elevation.1</th>\n",
       "      <th>P conc. (mg/kg).1</th>\n",
       "      <th>K conc. (mg/kg).1</th>\n",
       "      <th>N conc. (mg/kg).1</th>\n",
       "      <th>Chlorophyll-a.1</th>\n",
       "      <th>row_num</th>\n",
       "      <th>col_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T1.1</td>\n",
       "      <td>530605.915</td>\n",
       "      <td>2355788.377</td>\n",
       "      <td>15.193</td>\n",
       "      <td>6098.182181</td>\n",
       "      <td>12505.992809</td>\n",
       "      <td>4311.970060</td>\n",
       "      <td>35.8</td>\n",
       "      <td>2777</td>\n",
       "      <td>3548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T1.2</td>\n",
       "      <td>530601.735</td>\n",
       "      <td>2355788.448</td>\n",
       "      <td>15.178</td>\n",
       "      <td>5037.174349</td>\n",
       "      <td>12338.677355</td>\n",
       "      <td>4787.270920</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2775</td>\n",
       "      <td>3409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T1.3</td>\n",
       "      <td>530597.008</td>\n",
       "      <td>2355787.118</td>\n",
       "      <td>15.206</td>\n",
       "      <td>6471.319695</td>\n",
       "      <td>14923.786602</td>\n",
       "      <td>5286.813480</td>\n",
       "      <td>34.9</td>\n",
       "      <td>2819</td>\n",
       "      <td>3251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T2.1</td>\n",
       "      <td>530593.342</td>\n",
       "      <td>2355789.827</td>\n",
       "      <td>15.215</td>\n",
       "      <td>6433.340016</td>\n",
       "      <td>17184.242181</td>\n",
       "      <td>3066.571650</td>\n",
       "      <td>29.8</td>\n",
       "      <td>2729</td>\n",
       "      <td>3129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T2.2</td>\n",
       "      <td>530591.652</td>\n",
       "      <td>2355794.783</td>\n",
       "      <td>15.236</td>\n",
       "      <td>5679.671869</td>\n",
       "      <td>13274.309724</td>\n",
       "      <td>3433.145680</td>\n",
       "      <td>36.1</td>\n",
       "      <td>2563</td>\n",
       "      <td>3073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>J12.3</td>\n",
       "      <td>530582.155</td>\n",
       "      <td>2355781.790</td>\n",
       "      <td>15.238</td>\n",
       "      <td>7009.292566</td>\n",
       "      <td>16528.776978</td>\n",
       "      <td>7556.516150</td>\n",
       "      <td>38.6</td>\n",
       "      <td>2997</td>\n",
       "      <td>2756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>BC2</td>\n",
       "      <td>530553.718</td>\n",
       "      <td>2355758.342</td>\n",
       "      <td>15.224</td>\n",
       "      <td>5885.628743</td>\n",
       "      <td>14264.471058</td>\n",
       "      <td>2265.424250</td>\n",
       "      <td>33.2</td>\n",
       "      <td>3778</td>\n",
       "      <td>1808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>BC3</td>\n",
       "      <td>530591.520</td>\n",
       "      <td>2355732.444</td>\n",
       "      <td>15.206</td>\n",
       "      <td>7505.289421</td>\n",
       "      <td>16167.664671</td>\n",
       "      <td>3268.614069</td>\n",
       "      <td>40.0</td>\n",
       "      <td>4641</td>\n",
       "      <td>3068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>BC4</td>\n",
       "      <td>530622.015</td>\n",
       "      <td>2355776.121</td>\n",
       "      <td>15.086</td>\n",
       "      <td>6279.883674</td>\n",
       "      <td>10340.954673</td>\n",
       "      <td>2980.889693</td>\n",
       "      <td>41.8</td>\n",
       "      <td>3185</td>\n",
       "      <td>4085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>BC5</td>\n",
       "      <td>530619.755</td>\n",
       "      <td>2355797.647</td>\n",
       "      <td>15.229</td>\n",
       "      <td>6624.348175</td>\n",
       "      <td>9675.391095</td>\n",
       "      <td>2460.532560</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2468</td>\n",
       "      <td>4010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   type_of_field      East.1      North.1  Elevation.1  P conc. (mg/kg).1  \\\n",
       "1           T1.1  530605.915  2355788.377       15.193        6098.182181   \n",
       "2           T1.2  530601.735  2355788.448       15.178        5037.174349   \n",
       "3           T1.3  530597.008  2355787.118       15.206        6471.319695   \n",
       "4           T2.1  530593.342  2355789.827       15.215        6433.340016   \n",
       "5           T2.2  530591.652  2355794.783       15.236        5679.671869   \n",
       "..           ...         ...          ...          ...                ...   \n",
       "72         J12.3  530582.155  2355781.790       15.238        7009.292566   \n",
       "74           BC2  530553.718  2355758.342       15.224        5885.628743   \n",
       "75           BC3  530591.520  2355732.444       15.206        7505.289421   \n",
       "76           BC4  530622.015  2355776.121       15.086        6279.883674   \n",
       "77           BC5  530619.755  2355797.647       15.229        6624.348175   \n",
       "\n",
       "    K conc. (mg/kg).1  N conc. (mg/kg).1  Chlorophyll-a.1 row_num col_num  \n",
       "1        12505.992809        4311.970060             35.8    2777    3548  \n",
       "2        12338.677355        4787.270920             34.0    2775    3409  \n",
       "3        14923.786602        5286.813480             34.9    2819    3251  \n",
       "4        17184.242181        3066.571650             29.8    2729    3129  \n",
       "5        13274.309724        3433.145680             36.1    2563    3073  \n",
       "..                ...                ...              ...     ...     ...  \n",
       "72       16528.776978        7556.516150             38.6    2997    2756  \n",
       "74       14264.471058        2265.424250             33.2    3778    1808  \n",
       "75       16167.664671        3268.614069             40.0    4641    3068  \n",
       "76       10340.954673        2980.889693             41.8    3185    4085  \n",
       "77        9675.391095        2460.532560             35.0    2468    4010  \n",
       "\n",
       "[76 rows x 10 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df_13_09_2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 76 entries, 1 to 77\n",
      "Data columns (total 10 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   type_of_field      76 non-null     object \n",
      " 1   East.1             76 non-null     float64\n",
      " 2   North.1            76 non-null     float64\n",
      " 3   Elevation.1        76 non-null     float64\n",
      " 4   P conc. (mg/kg).1  76 non-null     float64\n",
      " 5   K conc. (mg/kg).1  76 non-null     float64\n",
      " 6   N conc. (mg/kg).1  76 non-null     float64\n",
      " 7   Chlorophyll-a.1    76 non-null     float64\n",
      " 8   row_num            76 non-null     object \n",
      " 9   col_num            76 non-null     object \n",
      "dtypes: float64(7), object(3)\n",
      "memory usage: 8.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data_df_13_09_2022.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lấy giá trị max của từng band trong các ô xung quanh\n",
    "def get_max_bands(row_num, col_num, scope):\n",
    "    number_points = 2 * scope + 1\n",
    "    band_in_scope = np.zeros((number_points ** 2, 122))\n",
    "    for i, (row_num, col_num) in enumerate(pixel_in_scope(row_num, col_num, scope)):\n",
    "        band_in_scope[i] = img[row_num, col_num, :].reshape(122)\n",
    "    return np.max(band_in_scope, axis=0) # Lấy giá trị max của từng cột (band)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(df: pd.DataFrame, type_of_output: Literal[\"N\", \"P\", \"K\"] = None, type_of_field: Literal[\"T\", \"J\", \"BC\"] = None):\n",
    "    if type_of_output == \"N\":\n",
    "        output_column = \"N conc. (mg/kg).1\"\n",
    "    elif type_of_output == \"K\":\n",
    "        output_column = \"K conc. (mg/kg).1\"\n",
    "    elif type_of_output == \"P\":\n",
    "        output_column = \"P conc. (mg/kg).1\"\n",
    "    \n",
    "    sample = df[[\"type_of_field\",output_column, \"row_num\", \"col_num\"]]\n",
    "\n",
    "    #Tạo bảng gồm giá trị của N, tọa độ pixel điểm thực nghiệm và 122 giá trị trống\n",
    "    for i in range(BAND_START_IX, BAND_END_IX + 1):\n",
    "        sample[f\"band_{i}\"] = [None] * len(sample)\n",
    "\n",
    "    # Thêm 122 giá trị max của 122 band vào tập data sample N\n",
    "    for index, row in sample.iterrows():\n",
    "        row_num = row[\"row_num\"]\n",
    "        col_num = row[\"col_num\"]\n",
    "        sample.loc[index, BAND_START_NAME:BAND_END_NAME] = get_max_bands(row_num, col_num, 3)[BAND_START_IX:BAND_END_IX + 1]\n",
    "    \n",
    "    if type_of_field:\n",
    "        if type(type_of_field) == list:\n",
    "            total_condition = sample[\"type_of_field\"].str.startswith(type_of_field[0])\n",
    "            for field in type_of_field[1:]:\n",
    "                condition = sample[\"type_of_field\"].str.startswith(field)\n",
    "                total_condition |= condition\n",
    "            sample = sample[total_condition]\n",
    "        else:\n",
    "            sample = sample[sample[\"type_of_field\"].str.startswith(type_of_field)] \n",
    "    sample.rename(columns={\n",
    "        output_column: \"target\"\n",
    "    }, inplace=True)\n",
    "    return sample\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type_of_field</th>\n",
       "      <th>target</th>\n",
       "      <th>row_num</th>\n",
       "      <th>col_num</th>\n",
       "      <th>band_4</th>\n",
       "      <th>band_5</th>\n",
       "      <th>band_6</th>\n",
       "      <th>band_7</th>\n",
       "      <th>band_8</th>\n",
       "      <th>band_9</th>\n",
       "      <th>...</th>\n",
       "      <th>band_92</th>\n",
       "      <th>band_93</th>\n",
       "      <th>band_94</th>\n",
       "      <th>band_95</th>\n",
       "      <th>band_96</th>\n",
       "      <th>band_97</th>\n",
       "      <th>band_98</th>\n",
       "      <th>band_99</th>\n",
       "      <th>band_100</th>\n",
       "      <th>band_101</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T1.1</td>\n",
       "      <td>4311.970060</td>\n",
       "      <td>2777</td>\n",
       "      <td>3548</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>90.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T1.2</td>\n",
       "      <td>4787.270920</td>\n",
       "      <td>2775</td>\n",
       "      <td>3409</td>\n",
       "      <td>43.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>111.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T1.3</td>\n",
       "      <td>5286.813480</td>\n",
       "      <td>2819</td>\n",
       "      <td>3251</td>\n",
       "      <td>20.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>95.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T2.1</td>\n",
       "      <td>3066.571650</td>\n",
       "      <td>2729</td>\n",
       "      <td>3129</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>86.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T2.2</td>\n",
       "      <td>3433.145680</td>\n",
       "      <td>2563</td>\n",
       "      <td>3073</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>98.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>T2.3</td>\n",
       "      <td>2551.822300</td>\n",
       "      <td>2416</td>\n",
       "      <td>3050</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>94.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>T3.1</td>\n",
       "      <td>6948.808340</td>\n",
       "      <td>2347</td>\n",
       "      <td>2897</td>\n",
       "      <td>47.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>82.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>T3.2</td>\n",
       "      <td>6590.181000</td>\n",
       "      <td>2342</td>\n",
       "      <td>2737</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>131.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>133.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>T3.3</td>\n",
       "      <td>7119.512810</td>\n",
       "      <td>2429</td>\n",
       "      <td>2604</td>\n",
       "      <td>17.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>96.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>T4.1</td>\n",
       "      <td>5642.719150</td>\n",
       "      <td>2310</td>\n",
       "      <td>2456</td>\n",
       "      <td>18.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>111.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>104.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>T4.2</td>\n",
       "      <td>5307.627710</td>\n",
       "      <td>2146</td>\n",
       "      <td>2471</td>\n",
       "      <td>25.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>108.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>T4.3</td>\n",
       "      <td>4826.388080</td>\n",
       "      <td>2040</td>\n",
       "      <td>2349</td>\n",
       "      <td>19.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>97.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>T5.1</td>\n",
       "      <td>1937.604340</td>\n",
       "      <td>2492</td>\n",
       "      <td>3473</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>95.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>T5.2</td>\n",
       "      <td>1875.544870</td>\n",
       "      <td>2466</td>\n",
       "      <td>3644</td>\n",
       "      <td>17.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>108.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>93.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>T5.3</td>\n",
       "      <td>1753.080200</td>\n",
       "      <td>2409</td>\n",
       "      <td>3770</td>\n",
       "      <td>25.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>94.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>T6.1</td>\n",
       "      <td>5843.023833</td>\n",
       "      <td>2071</td>\n",
       "      <td>3226</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>98.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>T6.2</td>\n",
       "      <td>5185.143990</td>\n",
       "      <td>2287</td>\n",
       "      <td>3287</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>86.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>T6.3</td>\n",
       "      <td>5888.054760</td>\n",
       "      <td>2416</td>\n",
       "      <td>3335</td>\n",
       "      <td>21.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>112.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>T7.1</td>\n",
       "      <td>1390.484510</td>\n",
       "      <td>2062</td>\n",
       "      <td>2830</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>86.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>T7.2</td>\n",
       "      <td>1971.916740</td>\n",
       "      <td>2026</td>\n",
       "      <td>2976</td>\n",
       "      <td>22.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>105.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>T7.3</td>\n",
       "      <td>1395.916090</td>\n",
       "      <td>2012</td>\n",
       "      <td>3111</td>\n",
       "      <td>23.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>99.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>T8.1</td>\n",
       "      <td>7411.332610</td>\n",
       "      <td>1698</td>\n",
       "      <td>2581</td>\n",
       "      <td>19.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>99.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>T8.2</td>\n",
       "      <td>7236.312780</td>\n",
       "      <td>1861</td>\n",
       "      <td>2632</td>\n",
       "      <td>19.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>78.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>T8.3</td>\n",
       "      <td>7302.342430</td>\n",
       "      <td>1982</td>\n",
       "      <td>2692</td>\n",
       "      <td>15.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>90.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>T9.1</td>\n",
       "      <td>3777.660720</td>\n",
       "      <td>2057</td>\n",
       "      <td>3949</td>\n",
       "      <td>14.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>108.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>T9.2</td>\n",
       "      <td>3608.516850</td>\n",
       "      <td>2079</td>\n",
       "      <td>3790</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>83.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>T9.3</td>\n",
       "      <td>2992.688560</td>\n",
       "      <td>2154</td>\n",
       "      <td>3650</td>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>102.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>T10.1</td>\n",
       "      <td>6018.848260</td>\n",
       "      <td>2053</td>\n",
       "      <td>3527</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>97.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>T10.2</td>\n",
       "      <td>5693.651620</td>\n",
       "      <td>1899</td>\n",
       "      <td>3475</td>\n",
       "      <td>23.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>108.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>T10.3</td>\n",
       "      <td>6802.865380</td>\n",
       "      <td>1801</td>\n",
       "      <td>3417</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>107.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>107.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>T11.1</td>\n",
       "      <td>4671.300890</td>\n",
       "      <td>1708</td>\n",
       "      <td>3240</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>108.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>T11.2</td>\n",
       "      <td>4124.001550</td>\n",
       "      <td>1738</td>\n",
       "      <td>3103</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>95.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>T11.3</td>\n",
       "      <td>4739.000910</td>\n",
       "      <td>1749</td>\n",
       "      <td>2988</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>92.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>T12.1</td>\n",
       "      <td>8007.944390</td>\n",
       "      <td>1663</td>\n",
       "      <td>2846</td>\n",
       "      <td>27.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>112.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>T12.2</td>\n",
       "      <td>8008.692100</td>\n",
       "      <td>1502</td>\n",
       "      <td>2820</td>\n",
       "      <td>18.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>110.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>T12.3</td>\n",
       "      <td>8284.080400</td>\n",
       "      <td>1372</td>\n",
       "      <td>2765</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>129.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>115.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   type_of_field       target row_num col_num band_4 band_5 band_6 band_7  \\\n",
       "1           T1.1  4311.970060    2777    3548   14.0   14.0   11.0   10.0   \n",
       "2           T1.2  4787.270920    2775    3409   43.0   18.0   10.0    9.0   \n",
       "3           T1.3  5286.813480    2819    3251   20.0   15.0   12.0    9.0   \n",
       "4           T2.1  3066.571650    2729    3129   18.0   14.0    9.0   10.0   \n",
       "5           T2.2  3433.145680    2563    3073   16.0   12.0   13.0   10.0   \n",
       "6           T2.3  2551.822300    2416    3050   14.0   15.0   11.0   10.0   \n",
       "7           T3.1  6948.808340    2347    2897   47.0   19.0    9.0   10.0   \n",
       "8           T3.2  6590.181000    2342    2737   15.0   11.0   12.0   12.0   \n",
       "9           T3.3  7119.512810    2429    2604   17.0   15.0   12.0   43.0   \n",
       "10          T4.1  5642.719150    2310    2456   18.0   19.0   12.0    9.0   \n",
       "11          T4.2  5307.627710    2146    2471   25.0   19.0   15.0   11.0   \n",
       "12          T4.3  4826.388080    2040    2349   19.0   15.0    9.0    9.0   \n",
       "13          T5.1  1937.604340    2492    3473   17.0   16.0    9.0    8.0   \n",
       "14          T5.2  1875.544870    2466    3644   17.0   14.0    9.0   11.0   \n",
       "15          T5.3  1753.080200    2409    3770   25.0   10.0   15.0   12.0   \n",
       "16          T6.1  5843.023833    2071    3226   13.0   12.0   11.0    8.0   \n",
       "17          T6.2  5185.143990    2287    3287   14.0   14.0   11.0    9.0   \n",
       "18          T6.3  5888.054760    2416    3335   21.0   15.0    7.0   11.0   \n",
       "19          T7.1  1390.484510    2062    2830   12.0   14.0   11.0    9.0   \n",
       "20          T7.2  1971.916740    2026    2976   22.0   14.0   12.0   11.0   \n",
       "21          T7.3  1395.916090    2012    3111   23.0   16.0   12.0    9.0   \n",
       "22          T8.1  7411.332610    1698    2581   19.0   13.0    8.0   10.0   \n",
       "23          T8.2  7236.312780    1861    2632   19.0   13.0   11.0   13.0   \n",
       "24          T8.3  7302.342430    1982    2692   15.0   14.0   10.0   10.0   \n",
       "25          T9.1  3777.660720    2057    3949   14.0   34.0   10.0    8.0   \n",
       "26          T9.2  3608.516850    2079    3790   16.0   12.0   11.0   10.0   \n",
       "27          T9.3  2992.688560    2154    3650   15.0   11.0   11.0   11.0   \n",
       "28         T10.1  6018.848260    2053    3527   16.0   12.0   12.0   12.0   \n",
       "29         T10.2  5693.651620    1899    3475   23.0   13.0   12.0    9.0   \n",
       "30         T10.3  6802.865380    1801    3417   14.0   14.0   17.0   14.0   \n",
       "31         T11.1  4671.300890    1708    3240   15.0   12.0   10.0   10.0   \n",
       "32         T11.2  4124.001550    1738    3103   15.0   13.0   13.0    9.0   \n",
       "33         T11.3  4739.000910    1749    2988   12.0   13.0   12.0   10.0   \n",
       "34         T12.1  8007.944390    1663    2846   27.0   14.0   12.0   12.0   \n",
       "35         T12.2  8008.692100    1502    2820   18.0   13.0    9.0   11.0   \n",
       "36         T12.3  8284.080400    1372    2765   16.0   13.0   12.0   12.0   \n",
       "\n",
       "   band_8 band_9  ... band_92 band_93 band_94 band_95 band_96 band_97 band_98  \\\n",
       "1     8.0    8.0  ...    90.0    88.0    90.0    87.0    91.0    88.0    89.0   \n",
       "2    12.0   12.0  ...   111.0   115.0   108.0   107.0   103.0   108.0   107.0   \n",
       "3    10.0    9.0  ...    95.0    84.0    83.0    88.0    86.0    90.0    89.0   \n",
       "4     8.0    8.0  ...    86.0    82.0    78.0    83.0    78.0    87.0    76.0   \n",
       "5     8.0    9.0  ...    98.0    94.0    93.0    88.0    91.0    89.0    91.0   \n",
       "6    27.0    8.0  ...    94.0    91.0    87.0    92.0    87.0    82.0    85.0   \n",
       "7     8.0   11.0  ...    82.0    78.0    83.0    77.0    72.0    74.0    75.0   \n",
       "8    13.0   10.0  ...   131.0   126.0   131.0   122.0   125.0   124.0   128.0   \n",
       "9     9.0   11.0  ...    96.0    87.0    87.0    89.0    85.0    83.0    84.0   \n",
       "10   11.0    8.0  ...   111.0   109.0   107.0   109.0   103.0   105.0   101.0   \n",
       "11    9.0   10.0  ...   108.0   107.0   104.0   102.0   107.0    98.0   107.0   \n",
       "12    9.0    9.0  ...    97.0   102.0    94.0    92.0    87.0    90.0    94.0   \n",
       "13    9.0    9.0  ...    95.0    94.0   101.0    92.0    93.0    87.0    90.0   \n",
       "14    8.0    8.0  ...   108.0    98.0   102.0    99.0    97.0    97.0    94.0   \n",
       "15   10.0   11.0  ...    94.0   101.0    89.0    95.0    93.0    94.0    93.0   \n",
       "16    9.0    7.0  ...    98.0    92.0    91.0    93.0    87.0    86.0    89.0   \n",
       "17   10.0    8.0  ...    86.0    87.0    82.0    83.0    85.0    78.0    78.0   \n",
       "18    6.0    9.0  ...   112.0   102.0    97.0   102.0   101.0    98.0    99.0   \n",
       "19   12.0    8.0  ...    86.0    89.0    82.0    82.0    85.0    85.0    83.0   \n",
       "20    9.0   10.0  ...   105.0   105.0   104.0   100.0   101.0    97.0    96.0   \n",
       "21    8.0   10.0  ...    99.0    96.0    94.0    94.0    89.0    87.0    88.0   \n",
       "22    9.0   11.0  ...    99.0    92.0    90.0    98.0    93.0    92.0   102.0   \n",
       "23    9.0    9.0  ...    78.0    79.0    76.0    73.0    72.0    74.0    81.0   \n",
       "24   12.0   12.0  ...    90.0    88.0    86.0    85.0    87.0    86.0    86.0   \n",
       "25    9.0    8.0  ...   108.0    96.0    97.0    94.0    95.0    92.0    95.0   \n",
       "26    9.0   10.0  ...    83.0    75.0    80.0    83.0    69.0    76.0    74.0   \n",
       "27    7.0    7.0  ...   102.0    96.0    97.0    93.0    91.0    94.0    87.0   \n",
       "28    7.0   12.0  ...    97.0   103.0    99.0    97.0   101.0    91.0    91.0   \n",
       "29   10.0    8.0  ...   108.0   105.0   103.0   103.0   100.0    96.0   100.0   \n",
       "30   10.0    8.0  ...   107.0   107.0   107.0   108.0   102.0   101.0   109.0   \n",
       "31   11.0    8.0  ...   108.0   100.0    92.0    97.0    93.0    92.0    93.0   \n",
       "32    8.0   10.0  ...    95.0    90.0    91.0    91.0    88.0    91.0    86.0   \n",
       "33    8.0   11.0  ...    92.0    89.0    85.0    86.0    85.0    89.0    87.0   \n",
       "34    9.0    9.0  ...   112.0   112.0   100.0   105.0   101.0    98.0    98.0   \n",
       "35    8.0    9.0  ...   110.0   111.0   107.0   108.0   104.0   102.0   103.0   \n",
       "36    8.0    9.0  ...   129.0   120.0   111.0   113.0   114.0   112.0   117.0   \n",
       "\n",
       "   band_99 band_100 band_101  \n",
       "1     87.0     86.0     87.0  \n",
       "2    101.0    104.0    100.0  \n",
       "3     83.0     84.0     84.0  \n",
       "4     79.0     79.0     81.0  \n",
       "5     95.0     95.0     93.0  \n",
       "6     83.0     82.0     82.0  \n",
       "7     75.0     77.0     75.0  \n",
       "8    128.0    122.0    133.0  \n",
       "9     85.0     84.0     85.0  \n",
       "10   104.0    103.0    104.0  \n",
       "11   103.0    107.0    105.0  \n",
       "12    90.0     89.0     90.0  \n",
       "13    86.0     93.0    102.0  \n",
       "14    93.0     95.0     93.0  \n",
       "15    92.0     94.0     95.0  \n",
       "16    85.0     91.0     92.0  \n",
       "17    79.0     84.0     81.0  \n",
       "18    97.0     99.0     98.0  \n",
       "19    82.0     79.0     79.0  \n",
       "20    96.0    100.0    105.0  \n",
       "21    91.0     88.0     87.0  \n",
       "22    96.0     94.0     94.0  \n",
       "23    72.0     76.0     79.0  \n",
       "24    88.0     79.0     88.0  \n",
       "25    90.0     95.0     90.0  \n",
       "26    77.0     75.0     75.0  \n",
       "27    92.0     95.0     86.0  \n",
       "28    93.0     94.0     92.0  \n",
       "29    98.0     96.0    100.0  \n",
       "30   107.0    100.0    107.0  \n",
       "31    88.0     97.0     97.0  \n",
       "32    85.0     84.0     87.0  \n",
       "33    87.0     87.0     87.0  \n",
       "34    97.0     98.0    101.0  \n",
       "35   103.0    103.0    105.0  \n",
       "36   113.0    112.0    115.0  \n",
       "\n",
       "[36 rows x 102 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sample(data_df_13_09_2022, \"N\", \"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dùng deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_X_train_Y_train(df: pd.DataFrame):\n",
    "    X = df.loc[:, BAND_START_NAME:BAND_END_NAME].to_numpy()\n",
    "    Y = df.loc[:, \"target\"].to_numpy()\n",
    "\n",
    "    X = X.astype(np.float32)\n",
    "    Y = Y.astype(np.float32)\n",
    "\n",
    "    total_sample = len(X)\n",
    "    max_train = int(total_sample * 1)\n",
    "    # max_val = int(total_sample * 1)\n",
    "\n",
    "    X_train = torch.tensor(X[0:max_train])\n",
    "    # X_val = torch.tensor(X[max_train:max_val])\n",
    "    # X_test = torch.tensor(X[max_val:])\n",
    "\n",
    "    Y_train = torch.tensor(Y[0:max_train])\n",
    "    # Y_val = torch.tensor(Y[max_train:max_val])\n",
    "    # Y_test = torch.tensor(Y[max_val:])\n",
    "\n",
    "    Y_train = Y_train.reshape((Y_train.shape[0], 1))\n",
    "    # Y_val = Y_val.reshape((Y_val.shape[0], 1))\n",
    "    # Y_test = Y_test.reshape((Y_test.shape[0], 1))\n",
    "\n",
    "    return X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeutralNetwork(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(TOTAL_BAND, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N_value = self.linear_relu_stack(x)\n",
    "        return N_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loss_fn, optimizer, X_train, Y_train, X_val, Y_val, n_epochs, min_loss=0.5):\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "\n",
    "        y_pred = model(X_train)\n",
    "        loss = loss_fn(y_pred, Y_train)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            model.eval()\n",
    "            with torch.inference_mode():\n",
    "                loss = np.sqrt(loss)\n",
    "                # y_pred_val = model(X_val)\n",
    "                # loss_val = np.sqrt(loss_fn(y_pred_val, Y_val))\n",
    "                # print(f\"Epoch: {epoch} | loss train: {loss} | loss val: {loss_val}\")\n",
    "                print(f\"Epoch: {epoch} | loss train: {loss}\")\n",
    "                if loss < min_loss:\n",
    "                    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "RERUN_NN_MODEL = \"N\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_using_neutral_network(X_train, Y_train, X_target, Y_target, name_file_output, super_param={\"lr\": 0.0001, \"weight_decay\": 1e-5, \"n_epochs\": 40000, \"stop_value\": 0.5}, re_run=\"N\"):\n",
    "    loss_fn = nn.MSELoss()\n",
    "    if not os.path.exists(name_file_output) or re_run == \"Y\" or RERUN_NN_MODEL:\n",
    "        model = NeutralNetwork()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=super_param[\"lr\"], weight_decay=super_param[\"weight_decay\"])\n",
    "        n_epochs = 40000\n",
    "        train_model(model, loss_fn, optimizer, X_train, Y_train, [], [], n_epochs, 0.5)\n",
    "        torch.jit.script(model).save(name_file_output)\n",
    "    else:\n",
    "        model = torch.jit.load(name_file_output)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        loss_fn = nn.MSELoss()\n",
    "        Y_target_pred = model(X_target)\n",
    "        loss = loss_fn(Y_target, Y_target_pred)\n",
    "        return np.sqrt(loss), Y_target_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_using_random_forest(X_train, Y_train, X_target, Y_target, super_param={}):\n",
    "    clf = RandomForestRegressor()\n",
    "    Y_train = Y_train.reshape(Y_train.shape[0])\n",
    "    clf = clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_target)\n",
    "    loss = np.sqrt(mean_squared_error(Y_target, Y_pred))\n",
    "    return loss, Y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_using_decision_tree(X_train, Y_train, X_target, Y_target, super_param={}):\n",
    "    clf = tree.DecisionTreeRegressor()\n",
    "    Y_train = Y_train.reshape(Y_train.shape[0])\n",
    "    clf = clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_target)\n",
    "    loss = np.sqrt(mean_squared_error(Y_target, Y_pred))\n",
    "    return loss, Y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_value = \"N\"\n",
    "train_field = \"T\"\n",
    "sample = generate_sample(data_df_13_09_2022, target_value, train_field)\n",
    "X_train, Y_train = create_X_train_Y_train(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36, 98])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_new = Y_train.reshape((36))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_info_T_N = mutual_info_regression(X_train, Y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02971586, 0.        , 0.        , 0.07250951, 0.        ,\n",
       "       0.        , 0.05920854, 0.        , 0.15723484, 0.03336544,\n",
       "       0.04008483, 0.11746885, 0.11681022, 0.13078452, 0.        ,\n",
       "       0.        , 0.        , 0.15411008, 0.        , 0.12317959,\n",
       "       0.01529997, 0.03728232, 0.05549729, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.01367259, 0.02080663,\n",
       "       0.        , 0.07305699, 0.        , 0.06648018, 0.        ,\n",
       "       0.00990904, 0.10552564, 0.04065423, 0.11996907, 0.        ,\n",
       "       0.        , 0.        , 0.03305784, 0.05716375, 0.        ,\n",
       "       0.06807066, 0.        , 0.02533721, 0.        , 0.        ,\n",
       "       0.        , 0.01866552, 0.05182001, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.06889889, 0.        , 0.01004971,\n",
       "       0.02052822, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.09055599, 0.        , 0.        , 0.07176921, 0.04507869,\n",
       "       0.12310512, 0.05841518, 0.        , 0.        , 0.        ,\n",
       "       0.15305806, 0.        , 0.        , 0.04905472, 0.        ,\n",
       "       0.        , 0.        , 0.02734207, 0.10459292, 0.10998329,\n",
       "       0.        , 0.07103798, 0.0551902 , 0.        , 0.08747562,\n",
       "       0.        , 0.07664018, 0.13590119, 0.        , 0.        ,\n",
       "       0.06761614, 0.        , 0.        ])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutual_info_T_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02971586, 0.        , 0.        , 0.07250951, 0.        ,\n",
       "       0.        , 0.05920854, 0.        , 0.15723484, 0.03336544,\n",
       "       0.04008483, 0.11746885, 0.11681022, 0.13078452, 0.        ,\n",
       "       0.        , 0.        , 0.15411008, 0.        , 0.12317959,\n",
       "       0.01529997, 0.03728232, 0.05549729, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.01367259, 0.02080663,\n",
       "       0.        , 0.07305699, 0.        , 0.06648018, 0.        ,\n",
       "       0.00990904, 0.10552564, 0.04065423, 0.11996907, 0.        ,\n",
       "       0.        , 0.        , 0.03305784, 0.05716375, 0.        ,\n",
       "       0.06807066, 0.        , 0.02533721, 0.        , 0.        ,\n",
       "       0.        , 0.01866552, 0.05182001, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.06889889, 0.        , 0.01004971,\n",
       "       0.02052822, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.09055599, 0.        , 0.        , 0.07176921, 0.04507869,\n",
       "       0.12310512, 0.05841518, 0.        , 0.        , 0.        ,\n",
       "       0.15305806, 0.        , 0.        , 0.04905472, 0.        ,\n",
       "       0.        , 0.        , 0.02734207, 0.10459292, 0.10998329,\n",
       "       0.        , 0.07103798, 0.0551902 , 0.        , 0.08747562,\n",
       "       0.        , 0.07664018, 0.13590119, 0.        , 0.        ,\n",
       "       0.06761614, 0.        , 0.        ])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutual_info_T_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = mutual_info_T_N[mutual_info_T_N != 0]\n",
    "a.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47,)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mutual_info_T_N = mutual_info_T_N.reshape((1, mutual_info_T_N.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 98)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutual_info_T_N.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_bands = np.arange(BAND_START_IX, BAND_END_IX + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,  15,  16,\n",
       "        17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,  29,\n",
       "        30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,\n",
       "        43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "        56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,\n",
       "        69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,\n",
       "        82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,\n",
       "        95,  96,  97,  98,  99, 100, 101])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data_df_13_09_2022[[\"type_of_field\",\"N conc. (mg/kg).1\", \"row_num\", \"col_num\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[f\"band_1\"] = [None] * len(sample)\n",
    "sample[f\"band_5\"] = [None] * len(sample)\n",
    "sample[f\"band_2\"] = [None] * len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type_of_field</th>\n",
       "      <th>N conc. (mg/kg).1</th>\n",
       "      <th>row_num</th>\n",
       "      <th>col_num</th>\n",
       "      <th>band_1</th>\n",
       "      <th>band_5</th>\n",
       "      <th>band_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T1.1</td>\n",
       "      <td>4311.970060</td>\n",
       "      <td>2777</td>\n",
       "      <td>3548</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T1.2</td>\n",
       "      <td>4787.270920</td>\n",
       "      <td>2775</td>\n",
       "      <td>3409</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T1.3</td>\n",
       "      <td>5286.813480</td>\n",
       "      <td>2819</td>\n",
       "      <td>3251</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T2.1</td>\n",
       "      <td>3066.571650</td>\n",
       "      <td>2729</td>\n",
       "      <td>3129</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T2.2</td>\n",
       "      <td>3433.145680</td>\n",
       "      <td>2563</td>\n",
       "      <td>3073</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>J12.3</td>\n",
       "      <td>7556.516150</td>\n",
       "      <td>2997</td>\n",
       "      <td>2756</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>BC2</td>\n",
       "      <td>2265.424250</td>\n",
       "      <td>3778</td>\n",
       "      <td>1808</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>BC3</td>\n",
       "      <td>3268.614069</td>\n",
       "      <td>4641</td>\n",
       "      <td>3068</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>BC4</td>\n",
       "      <td>2980.889693</td>\n",
       "      <td>3185</td>\n",
       "      <td>4085</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>BC5</td>\n",
       "      <td>2460.532560</td>\n",
       "      <td>2468</td>\n",
       "      <td>4010</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   type_of_field  N conc. (mg/kg).1 row_num col_num band_1 band_5 band_2\n",
       "1           T1.1        4311.970060    2777    3548   None   None   None\n",
       "2           T1.2        4787.270920    2775    3409   None   None   None\n",
       "3           T1.3        5286.813480    2819    3251   None   None   None\n",
       "4           T2.1        3066.571650    2729    3129   None   None   None\n",
       "5           T2.2        3433.145680    2563    3073   None   None   None\n",
       "..           ...                ...     ...     ...    ...    ...    ...\n",
       "72         J12.3        7556.516150    2997    2756   None   None   None\n",
       "74           BC2        2265.424250    3778    1808   None   None   None\n",
       "75           BC3        3268.614069    4641    3068   None   None   None\n",
       "76           BC4        2980.889693    3185    4085   None   None   None\n",
       "77           BC5        2460.532560    2468    4010   None   None   None\n",
       "\n",
       "[76 rows x 7 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.loc[0, \"band_1\":\"band_2\"] = [1, 5, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type_of_field</th>\n",
       "      <th>N conc. (mg/kg).1</th>\n",
       "      <th>row_num</th>\n",
       "      <th>col_num</th>\n",
       "      <th>band_1</th>\n",
       "      <th>band_5</th>\n",
       "      <th>band_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T1.1</td>\n",
       "      <td>4311.970060</td>\n",
       "      <td>2777</td>\n",
       "      <td>3548</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T1.2</td>\n",
       "      <td>4787.270920</td>\n",
       "      <td>2775</td>\n",
       "      <td>3409</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T1.3</td>\n",
       "      <td>5286.813480</td>\n",
       "      <td>2819</td>\n",
       "      <td>3251</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T2.1</td>\n",
       "      <td>3066.571650</td>\n",
       "      <td>2729</td>\n",
       "      <td>3129</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T2.2</td>\n",
       "      <td>3433.145680</td>\n",
       "      <td>2563</td>\n",
       "      <td>3073</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>BC2</td>\n",
       "      <td>2265.424250</td>\n",
       "      <td>3778</td>\n",
       "      <td>1808</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>BC3</td>\n",
       "      <td>3268.614069</td>\n",
       "      <td>4641</td>\n",
       "      <td>3068</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>BC4</td>\n",
       "      <td>2980.889693</td>\n",
       "      <td>3185</td>\n",
       "      <td>4085</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>BC5</td>\n",
       "      <td>2460.532560</td>\n",
       "      <td>2468</td>\n",
       "      <td>4010</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   type_of_field  N conc. (mg/kg).1 row_num col_num band_1 band_5 band_2\n",
       "1           T1.1        4311.970060    2777    3548   None   None   None\n",
       "2           T1.2        4787.270920    2775    3409   None   None   None\n",
       "3           T1.3        5286.813480    2819    3251   None   None   None\n",
       "4           T2.1        3066.571650    2729    3129   None   None   None\n",
       "5           T2.2        3433.145680    2563    3073   None   None   None\n",
       "..           ...                ...     ...     ...    ...    ...    ...\n",
       "74           BC2        2265.424250    3778    1808   None   None   None\n",
       "75           BC3        3268.614069    4641    3068   None   None   None\n",
       "76           BC4        2980.889693    3185    4085   None   None   None\n",
       "77           BC5        2460.532560    2468    4010   None   None   None\n",
       "0            NaN                NaN     NaN     NaN      1      5      4\n",
       "\n",
       "[77 rows x 7 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 6, 7])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_bands[[1, 2, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_bands = index_bands.reshape((1, index_bands.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 98)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_bands.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_info_T_N = np.concatenate([mutual_info_T_N, index_bands], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 98)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutual_info_T_N.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02971586, 0.        ],\n",
       "       [4.        , 5.        ]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutual_info_T_N[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dự đoán Nito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | loss train: 5287.9921875\n",
      "Epoch: 100 | loss train: 1961.630859375\n",
      "Epoch: 200 | loss train: 1750.2176513671875\n",
      "Epoch: 300 | loss train: 1117.271484375\n",
      "Epoch: 400 | loss train: 801.7138061523438\n",
      "Epoch: 500 | loss train: 569.9813842773438\n",
      "Epoch: 600 | loss train: 432.4056396484375\n",
      "Epoch: 700 | loss train: 564.3724365234375\n",
      "Epoch: 800 | loss train: 358.99688720703125\n",
      "Epoch: 900 | loss train: 267.75439453125\n",
      "Epoch: 1000 | loss train: 251.62118530273438\n",
      "Epoch: 1100 | loss train: 273.3931579589844\n",
      "Epoch: 1200 | loss train: 164.75439453125\n",
      "Epoch: 1300 | loss train: 244.81680297851562\n",
      "Epoch: 1400 | loss train: 197.86843872070312\n",
      "Epoch: 1500 | loss train: 157.14755249023438\n",
      "Epoch: 1600 | loss train: 190.35531616210938\n",
      "Epoch: 1700 | loss train: 155.8056640625\n",
      "Epoch: 1800 | loss train: 130.05213928222656\n",
      "Epoch: 1900 | loss train: 91.7403793334961\n",
      "Epoch: 2000 | loss train: 58.569068908691406\n",
      "Epoch: 2100 | loss train: 65.68785095214844\n",
      "Epoch: 2200 | loss train: 107.15345764160156\n",
      "Epoch: 2300 | loss train: 394.40008544921875\n",
      "Epoch: 2400 | loss train: 49.95549392700195\n",
      "Epoch: 2500 | loss train: 228.9466552734375\n",
      "Epoch: 2600 | loss train: 49.20987319946289\n",
      "Epoch: 2700 | loss train: 93.82324981689453\n",
      "Epoch: 2800 | loss train: 50.15989685058594\n",
      "Epoch: 2900 | loss train: 74.70388793945312\n",
      "Epoch: 3000 | loss train: 24.294103622436523\n",
      "Epoch: 3100 | loss train: 49.462135314941406\n",
      "Epoch: 3200 | loss train: 39.438663482666016\n",
      "Epoch: 3300 | loss train: 316.2239685058594\n",
      "Epoch: 3400 | loss train: 93.86528015136719\n",
      "Epoch: 3500 | loss train: 20.847074508666992\n",
      "Epoch: 3600 | loss train: 20.889564514160156\n",
      "Epoch: 3700 | loss train: 229.99264526367188\n",
      "Epoch: 3800 | loss train: 17.450952529907227\n",
      "Epoch: 3900 | loss train: 342.5058898925781\n",
      "Epoch: 4000 | loss train: 15.734646797180176\n",
      "Epoch: 4100 | loss train: 80.89717864990234\n",
      "Epoch: 4200 | loss train: 13.793880462646484\n",
      "Epoch: 4300 | loss train: 72.44290924072266\n",
      "Epoch: 4400 | loss train: 13.166330337524414\n",
      "Epoch: 4500 | loss train: 440.0984802246094\n",
      "Epoch: 4600 | loss train: 12.284586906433105\n",
      "Epoch: 4700 | loss train: 543.339599609375\n",
      "Epoch: 4800 | loss train: 13.577882766723633\n",
      "Epoch: 4900 | loss train: 10.302133560180664\n",
      "Epoch: 5000 | loss train: 262.3668518066406\n",
      "Epoch: 5100 | loss train: 11.798951148986816\n",
      "Epoch: 5200 | loss train: 35.000885009765625\n",
      "Epoch: 5300 | loss train: 8.631892204284668\n",
      "Epoch: 5400 | loss train: 28.940017700195312\n",
      "Epoch: 5500 | loss train: 8.276853561401367\n",
      "Epoch: 5600 | loss train: 90.83262634277344\n",
      "Epoch: 5700 | loss train: 12.104391098022461\n",
      "Epoch: 5800 | loss train: 30.147350311279297\n",
      "Epoch: 5900 | loss train: 32.398372650146484\n",
      "Epoch: 6000 | loss train: 180.7249298095703\n",
      "Epoch: 6100 | loss train: 50.326637268066406\n",
      "Epoch: 6200 | loss train: 12.655749320983887\n",
      "Epoch: 6300 | loss train: 490.1245422363281\n",
      "Epoch: 6400 | loss train: 12.570233345031738\n",
      "Epoch: 6500 | loss train: 65.90052032470703\n",
      "Epoch: 6600 | loss train: 6.392754077911377\n",
      "Epoch: 6700 | loss train: 32.57354736328125\n",
      "Epoch: 6800 | loss train: 8.91042709350586\n",
      "Epoch: 6900 | loss train: 46.72686767578125\n",
      "Epoch: 7000 | loss train: 24.132266998291016\n",
      "Epoch: 7100 | loss train: 12.701519012451172\n",
      "Epoch: 7200 | loss train: 9.882579803466797\n",
      "Epoch: 7300 | loss train: 110.6055679321289\n",
      "Epoch: 7400 | loss train: 11.65505599975586\n",
      "Epoch: 7500 | loss train: 7.619534492492676\n",
      "Epoch: 7600 | loss train: 40.04148483276367\n",
      "Epoch: 7700 | loss train: 10.587964057922363\n",
      "Epoch: 7800 | loss train: 8.336038589477539\n",
      "Epoch: 7900 | loss train: 7.008946895599365\n",
      "Epoch: 8000 | loss train: 345.8276672363281\n",
      "Epoch: 8100 | loss train: 29.66520881652832\n",
      "Epoch: 8200 | loss train: 14.148319244384766\n",
      "Epoch: 8300 | loss train: 8.73579216003418\n",
      "Epoch: 8400 | loss train: 7.146137714385986\n",
      "Epoch: 8500 | loss train: 6.11987829208374\n",
      "Epoch: 8600 | loss train: 310.3141174316406\n",
      "Epoch: 8700 | loss train: 6.3362345695495605\n",
      "Epoch: 8800 | loss train: 457.9303283691406\n",
      "Epoch: 8900 | loss train: 22.778961181640625\n",
      "Epoch: 9000 | loss train: 12.251928329467773\n",
      "Epoch: 9100 | loss train: 9.875484466552734\n",
      "Epoch: 9200 | loss train: 8.569592475891113\n",
      "Epoch: 9300 | loss train: 7.0625786781311035\n",
      "Epoch: 9400 | loss train: 7.016436576843262\n",
      "Epoch: 9500 | loss train: 57.26201629638672\n",
      "Epoch: 9600 | loss train: 125.2763442993164\n",
      "Epoch: 9700 | loss train: 9.694779396057129\n",
      "Epoch: 9800 | loss train: 5.639763355255127\n",
      "Epoch: 9900 | loss train: 145.55697631835938\n",
      "Epoch: 10000 | loss train: 116.84827423095703\n",
      "Epoch: 10100 | loss train: 150.04039001464844\n",
      "Epoch: 10200 | loss train: 111.5052490234375\n",
      "Epoch: 10300 | loss train: 99.53069305419922\n",
      "Epoch: 10400 | loss train: 92.44210815429688\n",
      "Epoch: 10500 | loss train: 79.48699951171875\n",
      "Epoch: 10600 | loss train: 75.13123321533203\n",
      "Epoch: 10700 | loss train: 161.44142150878906\n",
      "Epoch: 10800 | loss train: 65.3842544555664\n",
      "Epoch: 10900 | loss train: 74.27188873291016\n",
      "Epoch: 11000 | loss train: 648.6708374023438\n",
      "Epoch: 11100 | loss train: 56.65154266357422\n",
      "Epoch: 11200 | loss train: 553.5673217773438\n",
      "Epoch: 11300 | loss train: 51.64054870605469\n",
      "Epoch: 11400 | loss train: 47.90578842163086\n",
      "Epoch: 11500 | loss train: 155.68682861328125\n",
      "Epoch: 11600 | loss train: 44.0290412902832\n",
      "Epoch: 11700 | loss train: 101.82772827148438\n",
      "Epoch: 11800 | loss train: 39.594730377197266\n",
      "Epoch: 11900 | loss train: 53.56966781616211\n",
      "Epoch: 12000 | loss train: 41.55573654174805\n",
      "Epoch: 12100 | loss train: 36.63362121582031\n",
      "Epoch: 12200 | loss train: 47.49061965942383\n",
      "Epoch: 12300 | loss train: 31.222734451293945\n",
      "Epoch: 12400 | loss train: 37.40934371948242\n",
      "Epoch: 12500 | loss train: 245.34359741210938\n",
      "Epoch: 12600 | loss train: 26.791908264160156\n",
      "Epoch: 12700 | loss train: 100.04356384277344\n",
      "Epoch: 12800 | loss train: 24.24220085144043\n",
      "Epoch: 12900 | loss train: 773.7658081054688\n",
      "Epoch: 13000 | loss train: 22.9927921295166\n",
      "Epoch: 13100 | loss train: 20.23605728149414\n",
      "Epoch: 13200 | loss train: 50.55058288574219\n",
      "Epoch: 13300 | loss train: 18.17086410522461\n",
      "Epoch: 13400 | loss train: 83.25760650634766\n",
      "Epoch: 13500 | loss train: 16.441192626953125\n",
      "Epoch: 13600 | loss train: 29.737934112548828\n",
      "Epoch: 13700 | loss train: 17.37558364868164\n",
      "Epoch: 13800 | loss train: 13.368730545043945\n",
      "Epoch: 13900 | loss train: 23.64957046508789\n",
      "Epoch: 14000 | loss train: 12.042123794555664\n",
      "Epoch: 14100 | loss train: 85.57024383544922\n",
      "Epoch: 14200 | loss train: 11.000125885009766\n",
      "Epoch: 14300 | loss train: 174.21429443359375\n",
      "Epoch: 14400 | loss train: 11.721334457397461\n",
      "Epoch: 14500 | loss train: 31.448083877563477\n",
      "Epoch: 14600 | loss train: 11.780560493469238\n",
      "Epoch: 14700 | loss train: 8.264330863952637\n",
      "Epoch: 14800 | loss train: 23.02843475341797\n",
      "Epoch: 14900 | loss train: 7.345648765563965\n",
      "Epoch: 15000 | loss train: 123.92180633544922\n",
      "Epoch: 15100 | loss train: 6.750735282897949\n",
      "Epoch: 15200 | loss train: 249.67617797851562\n",
      "Epoch: 15300 | loss train: 7.411974906921387\n",
      "Epoch: 15400 | loss train: 9.976152420043945\n",
      "Epoch: 15500 | loss train: 8.111799240112305\n",
      "Epoch: 15600 | loss train: 4.8982625007629395\n",
      "Epoch: 15700 | loss train: 32.2296028137207\n",
      "Epoch: 15800 | loss train: 4.875857830047607\n",
      "Epoch: 15900 | loss train: 208.01573181152344\n",
      "Epoch: 16000 | loss train: 5.737217426300049\n",
      "Epoch: 16100 | loss train: 3.8022007942199707\n",
      "Epoch: 16200 | loss train: 15.264779090881348\n",
      "Epoch: 16300 | loss train: 4.32395601272583\n",
      "Epoch: 16400 | loss train: 289.7444763183594\n",
      "Epoch: 16500 | loss train: 17.396387100219727\n",
      "Epoch: 16600 | loss train: 11.064255714416504\n",
      "Epoch: 16700 | loss train: 3.595522880554199\n",
      "Epoch: 16800 | loss train: 27.999088287353516\n",
      "Epoch: 16900 | loss train: 5.743886470794678\n",
      "Epoch: 17000 | loss train: 4.353199005126953\n",
      "Epoch: 17100 | loss train: 3.214644432067871\n",
      "Epoch: 17200 | loss train: 129.45697021484375\n",
      "Epoch: 17300 | loss train: 32.07429504394531\n",
      "Epoch: 17400 | loss train: 28.908702850341797\n",
      "Epoch: 17500 | loss train: 45.73234939575195\n",
      "Epoch: 17600 | loss train: 102.43363189697266\n",
      "Epoch: 17700 | loss train: 205.5637664794922\n",
      "Epoch: 17800 | loss train: 87.33821105957031\n",
      "Epoch: 17900 | loss train: 21.68130874633789\n",
      "Epoch: 18000 | loss train: 31.748991012573242\n",
      "Epoch: 18100 | loss train: 34.83956527709961\n",
      "Epoch: 18200 | loss train: 19.439786911010742\n",
      "Epoch: 18300 | loss train: 28.748090744018555\n",
      "Epoch: 18400 | loss train: 18.356948852539062\n",
      "Epoch: 18500 | loss train: 52.97479248046875\n",
      "Epoch: 18600 | loss train: 17.086627960205078\n",
      "Epoch: 18700 | loss train: 18.098390579223633\n",
      "Epoch: 18800 | loss train: 52.98878860473633\n",
      "Epoch: 18900 | loss train: 17.06092071533203\n",
      "Epoch: 19000 | loss train: 23.63614845275879\n",
      "Epoch: 19100 | loss train: 247.0686492919922\n",
      "Epoch: 19200 | loss train: 14.01792049407959\n",
      "Epoch: 19300 | loss train: 35.03490447998047\n",
      "Epoch: 19400 | loss train: 14.740741729736328\n",
      "Epoch: 19500 | loss train: 23.1639404296875\n",
      "Epoch: 19600 | loss train: 38.166107177734375\n",
      "Epoch: 19700 | loss train: 13.097065925598145\n",
      "Epoch: 19800 | loss train: 32.2846794128418\n",
      "Epoch: 19900 | loss train: 26.550113677978516\n",
      "Epoch: 20000 | loss train: 11.772981643676758\n",
      "Epoch: 20100 | loss train: 56.811153411865234\n",
      "Epoch: 20200 | loss train: 14.648370742797852\n",
      "Epoch: 20300 | loss train: 130.77320861816406\n",
      "Epoch: 20400 | loss train: 382.3424377441406\n",
      "Epoch: 20500 | loss train: 10.9587984085083\n",
      "Epoch: 20600 | loss train: 9.715153694152832\n",
      "Epoch: 20700 | loss train: 64.12480926513672\n",
      "Epoch: 20800 | loss train: 152.17579650878906\n",
      "Epoch: 20900 | loss train: 9.2910737991333\n",
      "Epoch: 21000 | loss train: 15.516507148742676\n",
      "Epoch: 21100 | loss train: 8.73453426361084\n",
      "Epoch: 21200 | loss train: 11.393319129943848\n",
      "Epoch: 21300 | loss train: 8.215877532958984\n",
      "Epoch: 21400 | loss train: 297.53302001953125\n",
      "Epoch: 21500 | loss train: 8.754632949829102\n",
      "Epoch: 21600 | loss train: 64.2564468383789\n",
      "Epoch: 21700 | loss train: 8.023176193237305\n",
      "Epoch: 21800 | loss train: 76.86260986328125\n",
      "Epoch: 21900 | loss train: 38.848087310791016\n",
      "Epoch: 22000 | loss train: 9.761090278625488\n",
      "Epoch: 22100 | loss train: 7.9121994972229\n",
      "Epoch: 22200 | loss train: 7.026832103729248\n",
      "Epoch: 22300 | loss train: 6.743589401245117\n",
      "Epoch: 22400 | loss train: 10.457459449768066\n",
      "Epoch: 22500 | loss train: 415.281005859375\n",
      "Epoch: 22600 | loss train: 12.646857261657715\n",
      "Epoch: 22700 | loss train: 15.636478424072266\n",
      "Epoch: 22800 | loss train: 149.3031768798828\n",
      "Epoch: 22900 | loss train: 6.9537129402160645\n",
      "Epoch: 23000 | loss train: 293.1164855957031\n",
      "Epoch: 23100 | loss train: 6.3243489265441895\n",
      "Epoch: 23200 | loss train: 16.597700119018555\n",
      "Epoch: 23300 | loss train: 5.9677557945251465\n",
      "Epoch: 23400 | loss train: 26.99470329284668\n",
      "Epoch: 23500 | loss train: 75.91228485107422\n",
      "Epoch: 23600 | loss train: 6.403711795806885\n",
      "Epoch: 23700 | loss train: 133.0381622314453\n",
      "Epoch: 23800 | loss train: 5.862027168273926\n",
      "Epoch: 23900 | loss train: 106.2821044921875\n",
      "Epoch: 24000 | loss train: 8.801581382751465\n",
      "Epoch: 24100 | loss train: 12.130599975585938\n",
      "Epoch: 24200 | loss train: 351.8311767578125\n",
      "Epoch: 24300 | loss train: 6.125597953796387\n",
      "Epoch: 24400 | loss train: 362.5431823730469\n",
      "Epoch: 24500 | loss train: 6.279546737670898\n",
      "Epoch: 24600 | loss train: 34.7982063293457\n",
      "Epoch: 24700 | loss train: 5.076834201812744\n",
      "Epoch: 24800 | loss train: 25.112455368041992\n",
      "Epoch: 24900 | loss train: 5.6886887550354\n",
      "Epoch: 25000 | loss train: 22.5200138092041\n",
      "Epoch: 25100 | loss train: 5.213421821594238\n",
      "Epoch: 25200 | loss train: 54.504878997802734\n",
      "Epoch: 25300 | loss train: 6.88189172744751\n",
      "Epoch: 25400 | loss train: 45.87680435180664\n",
      "Epoch: 25500 | loss train: 4.7191290855407715\n",
      "Epoch: 25600 | loss train: 163.09432983398438\n",
      "Epoch: 25700 | loss train: 5.042716979980469\n",
      "Epoch: 25800 | loss train: 76.44721984863281\n",
      "Epoch: 25900 | loss train: 4.687817573547363\n",
      "Epoch: 26000 | loss train: 55.09284591674805\n",
      "Epoch: 26100 | loss train: 8.048480987548828\n",
      "Epoch: 26200 | loss train: 278.6776428222656\n",
      "Epoch: 26300 | loss train: 5.307160377502441\n",
      "Epoch: 26400 | loss train: 4.546561241149902\n",
      "Epoch: 26500 | loss train: 542.3716430664062\n",
      "Epoch: 26600 | loss train: 6.272723197937012\n",
      "Epoch: 26700 | loss train: 3.8347885608673096\n",
      "Epoch: 26800 | loss train: 51.508995056152344\n",
      "Epoch: 26900 | loss train: 91.56800079345703\n",
      "Epoch: 27000 | loss train: 4.902057647705078\n",
      "Epoch: 27100 | loss train: 4.584370136260986\n",
      "Epoch: 27200 | loss train: 8.127168655395508\n",
      "Epoch: 27300 | loss train: 5.568399906158447\n",
      "Epoch: 27400 | loss train: 43.568115234375\n",
      "Epoch: 27500 | loss train: 3.8707847595214844\n",
      "Epoch: 27600 | loss train: 119.0444564819336\n",
      "Epoch: 27700 | loss train: 4.218044281005859\n",
      "Epoch: 27800 | loss train: 17.136022567749023\n",
      "Epoch: 27900 | loss train: 10.603937149047852\n",
      "Epoch: 28000 | loss train: 3.8875064849853516\n",
      "Epoch: 28100 | loss train: 46.58192825317383\n",
      "Epoch: 28200 | loss train: 9.927642822265625\n",
      "Epoch: 28300 | loss train: 532.9325561523438\n",
      "Epoch: 28400 | loss train: 5.6847639083862305\n",
      "Epoch: 28500 | loss train: 3.575169086456299\n",
      "Epoch: 28600 | loss train: 84.36626434326172\n",
      "Epoch: 28700 | loss train: 4.16819953918457\n",
      "Epoch: 28800 | loss train: 3.9257588386535645\n",
      "Epoch: 28900 | loss train: 60.77762985229492\n",
      "Epoch: 29000 | loss train: 4.289098262786865\n",
      "Epoch: 29100 | loss train: 3.24056339263916\n",
      "Epoch: 29200 | loss train: 10.165977478027344\n",
      "Epoch: 29300 | loss train: 330.09063720703125\n",
      "Epoch: 29400 | loss train: 4.898747444152832\n",
      "Epoch: 29500 | loss train: 3.3267998695373535\n",
      "Epoch: 29600 | loss train: 603.0909423828125\n",
      "Epoch: 29700 | loss train: 5.405172824859619\n",
      "Epoch: 29800 | loss train: 3.1929826736450195\n",
      "Epoch: 29900 | loss train: 82.13230895996094\n",
      "Epoch: 30000 | loss train: 3.8013737201690674\n",
      "Epoch: 30100 | loss train: 3.8044862747192383\n",
      "Epoch: 30200 | loss train: 25.061494827270508\n",
      "Epoch: 30300 | loss train: 3.2999112606048584\n",
      "Epoch: 30400 | loss train: 354.4100036621094\n",
      "Epoch: 30500 | loss train: 5.982914924621582\n",
      "Epoch: 30600 | loss train: 5.975818157196045\n",
      "Epoch: 30700 | loss train: 28.00575828552246\n",
      "Epoch: 30800 | loss train: 4.24765682220459\n",
      "Epoch: 30900 | loss train: 3.3379018306732178\n",
      "Epoch: 31000 | loss train: 60.85139465332031\n",
      "Epoch: 31100 | loss train: 5.305589199066162\n",
      "Epoch: 31200 | loss train: 297.8493347167969\n",
      "Epoch: 31300 | loss train: 3.8718366622924805\n",
      "Epoch: 31400 | loss train: 10.696187973022461\n",
      "Epoch: 31500 | loss train: 10.801507949829102\n",
      "Epoch: 31600 | loss train: 4.580808162689209\n",
      "Epoch: 31700 | loss train: 30.794408798217773\n",
      "Epoch: 31800 | loss train: 3.177741765975952\n",
      "Epoch: 31900 | loss train: 16.336956024169922\n",
      "Epoch: 32000 | loss train: 3.228922128677368\n",
      "Epoch: 32100 | loss train: 87.1592025756836\n",
      "Epoch: 32200 | loss train: 4.337707042694092\n",
      "Epoch: 32300 | loss train: 5.1915717124938965\n",
      "Epoch: 32400 | loss train: 5.761229515075684\n",
      "Epoch: 32500 | loss train: 8.445701599121094\n",
      "Epoch: 32600 | loss train: 20.409440994262695\n",
      "Epoch: 32700 | loss train: 3.3642477989196777\n",
      "Epoch: 32800 | loss train: 98.9907455444336\n",
      "Epoch: 32900 | loss train: 3.216916561126709\n",
      "Epoch: 33000 | loss train: 97.0606460571289\n",
      "Epoch: 33100 | loss train: 3.7455430030822754\n",
      "Epoch: 33200 | loss train: 23.455371856689453\n",
      "Epoch: 33300 | loss train: 4.773627281188965\n",
      "Epoch: 33400 | loss train: 4.066949367523193\n",
      "Epoch: 33500 | loss train: 23.36260986328125\n",
      "Epoch: 33600 | loss train: 4.5598626136779785\n",
      "Epoch: 33700 | loss train: 79.00459289550781\n",
      "Epoch: 33800 | loss train: 7.518169403076172\n",
      "Epoch: 33900 | loss train: 4.214048385620117\n",
      "Epoch: 34000 | loss train: 120.5472183227539\n",
      "Epoch: 34100 | loss train: 4.18418550491333\n",
      "Epoch: 34200 | loss train: 95.04940032958984\n",
      "Epoch: 34300 | loss train: 6.379016399383545\n",
      "Epoch: 34400 | loss train: 3.521953582763672\n",
      "Epoch: 34500 | loss train: 81.13919830322266\n",
      "Epoch: 34600 | loss train: 3.767672300338745\n",
      "Epoch: 34700 | loss train: 304.07147216796875\n",
      "Epoch: 34800 | loss train: 4.845461368560791\n",
      "Epoch: 34900 | loss train: 3.1759703159332275\n",
      "Epoch: 35000 | loss train: 100.01447296142578\n",
      "Epoch: 35100 | loss train: 17.494428634643555\n",
      "Epoch: 35200 | loss train: 28.636470794677734\n",
      "Epoch: 35300 | loss train: 3.1591908931732178\n",
      "Epoch: 35400 | loss train: 104.96833801269531\n",
      "Epoch: 35500 | loss train: 4.452842712402344\n",
      "Epoch: 35600 | loss train: 3.536620855331421\n",
      "Epoch: 35700 | loss train: 4.684127330780029\n",
      "Epoch: 35800 | loss train: 7.348512649536133\n",
      "Epoch: 35900 | loss train: 3.2755117416381836\n",
      "Epoch: 36000 | loss train: 295.13018798828125\n",
      "Epoch: 36100 | loss train: 4.051463603973389\n",
      "Epoch: 36200 | loss train: 28.7987003326416\n",
      "Epoch: 36300 | loss train: 8.890676498413086\n",
      "Epoch: 36400 | loss train: 3.3052072525024414\n",
      "Epoch: 36500 | loss train: 94.32386779785156\n",
      "Epoch: 36600 | loss train: 25.43449592590332\n",
      "Epoch: 36700 | loss train: 2.9369442462921143\n",
      "Epoch: 36800 | loss train: 159.2743377685547\n",
      "Epoch: 36900 | loss train: 6.565990447998047\n",
      "Epoch: 37000 | loss train: 3.5746312141418457\n",
      "Epoch: 37100 | loss train: 2.985802412033081\n",
      "Epoch: 37200 | loss train: 45.52788162231445\n",
      "Epoch: 37300 | loss train: 3.604294776916504\n",
      "Epoch: 37400 | loss train: 16.415271759033203\n",
      "Epoch: 37500 | loss train: 2.8950700759887695\n",
      "Epoch: 37600 | loss train: 148.0103302001953\n",
      "Epoch: 37700 | loss train: 3.9551126956939697\n",
      "Epoch: 37800 | loss train: 39.60422134399414\n",
      "Epoch: 37900 | loss train: 3.60756254196167\n",
      "Epoch: 38000 | loss train: 3.311086416244507\n",
      "Epoch: 38100 | loss train: 34.21986389160156\n",
      "Epoch: 38200 | loss train: 4.229676246643066\n",
      "Epoch: 38300 | loss train: 3.219655990600586\n",
      "Epoch: 38400 | loss train: 212.394287109375\n",
      "Epoch: 38500 | loss train: 5.3784332275390625\n",
      "Epoch: 38600 | loss train: 3.5846827030181885\n",
      "Epoch: 38700 | loss train: 6.137290000915527\n",
      "Epoch: 38800 | loss train: 6.895514011383057\n",
      "Epoch: 38900 | loss train: 3.526137351989746\n",
      "Epoch: 39000 | loss train: 139.9213409423828\n",
      "Epoch: 39100 | loss train: 124.99671936035156\n",
      "Epoch: 39200 | loss train: 4.536571502685547\n",
      "Epoch: 39300 | loss train: 3.3574814796447754\n",
      "Epoch: 39400 | loss train: 385.9806213378906\n",
      "Epoch: 39500 | loss train: 4.430930137634277\n",
      "Epoch: 39600 | loss train: 90.48021697998047\n",
      "Epoch: 39700 | loss train: 4.945969104766846\n",
      "Epoch: 39800 | loss train: 201.05506896972656\n",
      "Epoch: 39900 | loss train: 4.135984420776367\n",
      "loss_NN=tensor(2962.1584)\n",
      "pred_NN=tensor([[4118.0869],\n",
      "        [5672.3315],\n",
      "        [5783.9824],\n",
      "        [6706.7056]])\n",
      "loss_RF=2496.8440470522387\n",
      "pred_RF=array([6104.72578369, 5109.28616577, 4650.3473938 , 4465.9057251 ])\n",
      "loss_DT=2942.412609364477\n",
      "pred_DT=array([6590.18115234, 6590.18115234, 3066.5715332 , 4671.30078125])\n"
     ]
    }
   ],
   "source": [
    "target_value = \"N\"\n",
    "train_field = \"T\"\n",
    "sample = generate_sample(data_df_13_09_2022, target_value, train_field)\n",
    "X_train, Y_train = create_X_train_Y_train(sample)\n",
    "sample_target = generate_sample(data_df_13_09_2022, target_value, \"BC\")\n",
    "X_target, Y_target = create_X_train_Y_train(sample_target)\n",
    "super_param={\"lr\": 0.001, \"weight_decay\": 1e-5, \"n_epochs\": 40000, \"stop_value\": 0.5}\n",
    "re_run = \"N\"\n",
    "loss_NN, pred_NN = predict_using_neutral_network(X_train, Y_train, X_target, Y_target, f\"{'_'.join(list(train_field))}_model_predict_{target_value}_2.pt\", super_param, re_run)\n",
    "print(f\"{loss_NN=}\")\n",
    "print(f\"{pred_NN=}\")\n",
    "loss_RF, pred_RF = predict_using_random_forest(X_train, Y_train, X_target, Y_target, super_param)\n",
    "print(f\"{loss_RF=}\")\n",
    "print(f\"{pred_RF=}\")\n",
    "loss_DT, pred_DT = predict_using_decision_tree(X_train, Y_train, X_target, Y_target, super_param)\n",
    "print(f\"{loss_DT=}\")\n",
    "print(f\"{pred_DT=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | loss train: 6206.9326171875\n",
      "Epoch: 100 | loss train: 6036.2021484375\n",
      "Epoch: 200 | loss train: 2243.7275390625\n",
      "Epoch: 300 | loss train: 2237.00244140625\n",
      "Epoch: 400 | loss train: 2231.451904296875\n",
      "Epoch: 500 | loss train: 2221.82421875\n",
      "Epoch: 600 | loss train: 2206.134521484375\n",
      "Epoch: 700 | loss train: 2183.83642578125\n",
      "Epoch: 800 | loss train: 2149.5556640625\n",
      "Epoch: 900 | loss train: 2092.3408203125\n",
      "Epoch: 1000 | loss train: 1987.93017578125\n",
      "Epoch: 1100 | loss train: 1894.0159912109375\n",
      "Epoch: 1200 | loss train: 1772.2864990234375\n",
      "Epoch: 1300 | loss train: 1459.420166015625\n",
      "Epoch: 1400 | loss train: 1046.856201171875\n",
      "Epoch: 1500 | loss train: 912.9402465820312\n",
      "Epoch: 1600 | loss train: 824.2400512695312\n",
      "Epoch: 1700 | loss train: 753.7316284179688\n",
      "Epoch: 1800 | loss train: 689.829345703125\n",
      "Epoch: 1900 | loss train: 641.2338256835938\n",
      "Epoch: 2000 | loss train: 594.118896484375\n",
      "Epoch: 2100 | loss train: 561.5919799804688\n",
      "Epoch: 2200 | loss train: 529.2410278320312\n",
      "Epoch: 2300 | loss train: 497.6834411621094\n",
      "Epoch: 2400 | loss train: 467.9897766113281\n",
      "Epoch: 2500 | loss train: 442.4746398925781\n",
      "Epoch: 2600 | loss train: 418.4124755859375\n",
      "Epoch: 2700 | loss train: 393.91064453125\n",
      "Epoch: 2800 | loss train: 375.481201171875\n",
      "Epoch: 2900 | loss train: 358.2833557128906\n",
      "Epoch: 3000 | loss train: 334.20263671875\n",
      "Epoch: 3100 | loss train: 321.8586120605469\n",
      "Epoch: 3200 | loss train: 302.4537048339844\n",
      "Epoch: 3300 | loss train: 287.9517517089844\n",
      "Epoch: 3400 | loss train: 273.6287841796875\n",
      "Epoch: 3500 | loss train: 265.2740783691406\n",
      "Epoch: 3600 | loss train: 249.3104705810547\n",
      "Epoch: 3700 | loss train: 236.30128479003906\n",
      "Epoch: 3800 | loss train: 235.2288055419922\n",
      "Epoch: 3900 | loss train: 221.77127075195312\n",
      "Epoch: 4000 | loss train: 207.71444702148438\n",
      "Epoch: 4100 | loss train: 192.85098266601562\n",
      "Epoch: 4200 | loss train: 195.88113403320312\n",
      "Epoch: 4300 | loss train: 184.53384399414062\n",
      "Epoch: 4400 | loss train: 176.80528259277344\n",
      "Epoch: 4500 | loss train: 171.71397399902344\n",
      "Epoch: 4600 | loss train: 162.9721221923828\n",
      "Epoch: 4700 | loss train: 148.25558471679688\n",
      "Epoch: 4800 | loss train: 147.53244018554688\n",
      "Epoch: 4900 | loss train: 137.84413146972656\n",
      "Epoch: 5000 | loss train: 136.73748779296875\n",
      "Epoch: 5100 | loss train: 123.10103607177734\n",
      "Epoch: 5200 | loss train: 136.76368713378906\n",
      "Epoch: 5300 | loss train: 130.25790405273438\n",
      "Epoch: 5400 | loss train: 105.11297607421875\n",
      "Epoch: 5500 | loss train: 122.77587127685547\n",
      "Epoch: 5600 | loss train: 108.21797943115234\n",
      "Epoch: 5700 | loss train: 132.7029571533203\n",
      "Epoch: 5800 | loss train: 99.70735168457031\n",
      "Epoch: 5900 | loss train: 88.07783508300781\n",
      "Epoch: 6000 | loss train: 92.8280029296875\n",
      "Epoch: 6100 | loss train: 76.10041046142578\n",
      "Epoch: 6200 | loss train: 75.09793853759766\n",
      "Epoch: 6300 | loss train: 114.08333587646484\n",
      "Epoch: 6400 | loss train: 69.98712921142578\n",
      "Epoch: 6500 | loss train: 68.07675170898438\n",
      "Epoch: 6600 | loss train: 65.82917022705078\n",
      "Epoch: 6700 | loss train: 62.655364990234375\n",
      "Epoch: 6800 | loss train: 108.48246765136719\n",
      "Epoch: 6900 | loss train: 56.63014221191406\n",
      "Epoch: 7000 | loss train: 50.816429138183594\n",
      "Epoch: 7100 | loss train: 49.51823425292969\n",
      "Epoch: 7200 | loss train: 57.0140380859375\n",
      "Epoch: 7300 | loss train: 61.05549621582031\n",
      "Epoch: 7400 | loss train: 69.61640167236328\n",
      "Epoch: 7500 | loss train: 54.984806060791016\n",
      "Epoch: 7600 | loss train: 49.202388763427734\n",
      "Epoch: 7700 | loss train: 81.16700744628906\n",
      "Epoch: 7800 | loss train: 68.9463119506836\n",
      "Epoch: 7900 | loss train: 59.792022705078125\n",
      "Epoch: 8000 | loss train: 90.87419128417969\n",
      "Epoch: 8100 | loss train: 69.27814483642578\n",
      "Epoch: 8200 | loss train: 90.89063262939453\n",
      "Epoch: 8300 | loss train: 33.49922180175781\n",
      "Epoch: 8400 | loss train: 28.718400955200195\n",
      "Epoch: 8500 | loss train: 30.250057220458984\n",
      "Epoch: 8600 | loss train: 26.69744300842285\n",
      "Epoch: 8700 | loss train: 25.060707092285156\n",
      "Epoch: 8800 | loss train: 28.809770584106445\n",
      "Epoch: 8900 | loss train: 44.852237701416016\n",
      "Epoch: 9000 | loss train: 21.597379684448242\n",
      "Epoch: 9100 | loss train: 91.58975982666016\n",
      "Epoch: 9200 | loss train: 20.326513290405273\n",
      "Epoch: 9300 | loss train: 36.96138381958008\n",
      "Epoch: 9400 | loss train: 26.111854553222656\n",
      "Epoch: 9500 | loss train: 18.42538070678711\n",
      "Epoch: 9600 | loss train: 36.46658706665039\n",
      "Epoch: 9700 | loss train: 17.848812103271484\n",
      "Epoch: 9800 | loss train: 52.10416793823242\n",
      "Epoch: 9900 | loss train: 39.06178283691406\n",
      "Epoch: 10000 | loss train: 17.21251106262207\n",
      "Epoch: 10100 | loss train: 24.1967830657959\n",
      "Epoch: 10200 | loss train: 23.050811767578125\n",
      "Epoch: 10300 | loss train: 17.237409591674805\n",
      "Epoch: 10400 | loss train: 13.682251930236816\n",
      "Epoch: 10500 | loss train: 15.83917236328125\n",
      "Epoch: 10600 | loss train: 14.001470565795898\n",
      "Epoch: 10700 | loss train: 15.212446212768555\n",
      "Epoch: 10800 | loss train: 12.421037673950195\n",
      "Epoch: 10900 | loss train: 12.942156791687012\n",
      "Epoch: 11000 | loss train: 32.057289123535156\n",
      "Epoch: 11100 | loss train: 12.224190711975098\n",
      "Epoch: 11200 | loss train: 12.982773780822754\n",
      "Epoch: 11300 | loss train: 9.628742218017578\n",
      "Epoch: 11400 | loss train: 26.187013626098633\n",
      "Epoch: 11500 | loss train: 9.63785171508789\n",
      "Epoch: 11600 | loss train: 34.43700408935547\n",
      "Epoch: 11700 | loss train: 9.072285652160645\n",
      "Epoch: 11800 | loss train: 8.049888610839844\n",
      "Epoch: 11900 | loss train: 57.80194091796875\n",
      "Epoch: 12000 | loss train: 8.185391426086426\n",
      "Epoch: 12100 | loss train: 38.289085388183594\n",
      "Epoch: 12200 | loss train: 7.2439799308776855\n",
      "Epoch: 12300 | loss train: 6.737889289855957\n",
      "Epoch: 12400 | loss train: 11.532572746276855\n",
      "Epoch: 12500 | loss train: 46.1746940612793\n",
      "Epoch: 12600 | loss train: 6.716533660888672\n",
      "Epoch: 12700 | loss train: 6.0389628410339355\n",
      "Epoch: 12800 | loss train: 5.637181282043457\n",
      "Epoch: 12900 | loss train: 11.540040016174316\n",
      "Epoch: 13000 | loss train: 6.36719274520874\n",
      "Epoch: 13100 | loss train: 21.195215225219727\n",
      "Epoch: 13200 | loss train: 6.295466423034668\n",
      "Epoch: 13300 | loss train: 10.233160018920898\n",
      "Epoch: 13400 | loss train: 5.22559118270874\n",
      "Epoch: 13500 | loss train: 47.42209243774414\n",
      "Epoch: 13600 | loss train: 75.33998107910156\n",
      "Epoch: 13700 | loss train: 4.705053806304932\n",
      "Epoch: 13800 | loss train: 4.194678783416748\n",
      "Epoch: 13900 | loss train: 12.811800956726074\n",
      "Epoch: 14000 | loss train: 4.709622859954834\n",
      "Epoch: 14100 | loss train: 194.76429748535156\n",
      "Epoch: 14200 | loss train: 4.109594821929932\n",
      "Epoch: 14300 | loss train: 4.866306304931641\n",
      "Epoch: 14400 | loss train: 9.747086524963379\n",
      "Epoch: 14500 | loss train: 4.480899333953857\n",
      "Epoch: 14600 | loss train: 4.081127643585205\n",
      "Epoch: 14700 | loss train: 3.789839029312134\n",
      "Epoch: 14800 | loss train: 24.66961097717285\n",
      "Epoch: 14900 | loss train: 8.997974395751953\n",
      "Epoch: 15000 | loss train: 4.072342872619629\n",
      "Epoch: 15100 | loss train: 3.753727912902832\n",
      "Epoch: 15200 | loss train: 74.68562316894531\n",
      "Epoch: 15300 | loss train: 3.729647636413574\n",
      "Epoch: 15400 | loss train: 3.3687350749969482\n",
      "Epoch: 15500 | loss train: 7.567408084869385\n",
      "Epoch: 15600 | loss train: 16.793195724487305\n",
      "Epoch: 15700 | loss train: 211.22149658203125\n",
      "Epoch: 15800 | loss train: 4.410264492034912\n",
      "Epoch: 15900 | loss train: 3.3681929111480713\n",
      "Epoch: 16000 | loss train: 3.0005850791931152\n",
      "Epoch: 16100 | loss train: 2.754955291748047\n",
      "Epoch: 16200 | loss train: 2.913606882095337\n",
      "Epoch: 16300 | loss train: 16.64837074279785\n",
      "Epoch: 16400 | loss train: 3.07010817527771\n",
      "Epoch: 16500 | loss train: 2.7553305625915527\n",
      "Epoch: 16600 | loss train: 38.715614318847656\n",
      "Epoch: 16700 | loss train: 3.0265257358551025\n",
      "Epoch: 16800 | loss train: 2.5242397785186768\n",
      "Epoch: 16900 | loss train: 2.2931699752807617\n",
      "Epoch: 17000 | loss train: 177.8531951904297\n",
      "Epoch: 17100 | loss train: 2.3382351398468018\n",
      "Epoch: 17200 | loss train: 48.02369689941406\n",
      "Epoch: 17300 | loss train: 2.9377646446228027\n",
      "Epoch: 17400 | loss train: 2.215214967727661\n",
      "Epoch: 17500 | loss train: 8.559920310974121\n",
      "Epoch: 17600 | loss train: 23.099088668823242\n",
      "Epoch: 17700 | loss train: 2.4879722595214844\n",
      "Epoch: 17800 | loss train: 2.0442471504211426\n",
      "Epoch: 17900 | loss train: 1.8156341314315796\n",
      "Epoch: 18000 | loss train: 48.69758987426758\n",
      "Epoch: 18100 | loss train: 2.7204558849334717\n",
      "Epoch: 18200 | loss train: 2.028611183166504\n",
      "Epoch: 18300 | loss train: 7.543212890625\n",
      "Epoch: 18400 | loss train: 6.548333168029785\n",
      "Epoch: 18500 | loss train: 2.054539918899536\n",
      "Epoch: 18600 | loss train: 1.7299997806549072\n",
      "Epoch: 18700 | loss train: 1.6430150270462036\n",
      "Epoch: 18800 | loss train: 7.672943592071533\n",
      "Epoch: 18900 | loss train: 2.1469404697418213\n",
      "Epoch: 19000 | loss train: 1.6969918012619019\n",
      "Epoch: 19100 | loss train: 1.5035789012908936\n",
      "Epoch: 19200 | loss train: 1.826672077178955\n",
      "Epoch: 19300 | loss train: 6.110699653625488\n",
      "Epoch: 19400 | loss train: 1.840763807296753\n",
      "Epoch: 19500 | loss train: 1.5237090587615967\n",
      "Epoch: 19600 | loss train: 1.34691321849823\n",
      "Epoch: 19700 | loss train: 311.4322509765625\n",
      "Epoch: 19800 | loss train: 3.1704368591308594\n",
      "Epoch: 19900 | loss train: 1.9751307964324951\n",
      "Epoch: 20000 | loss train: 1.595057725906372\n",
      "Epoch: 20100 | loss train: 1.3719125986099243\n",
      "Epoch: 20200 | loss train: 1.208233118057251\n",
      "Epoch: 20300 | loss train: 264.8511047363281\n",
      "Epoch: 20400 | loss train: 3.3730270862579346\n",
      "Epoch: 20500 | loss train: 1.6952635049819946\n",
      "Epoch: 20600 | loss train: 1.687110185623169\n",
      "Epoch: 20700 | loss train: 2.2405436038970947\n",
      "Epoch: 20800 | loss train: 1.372149109840393\n",
      "Epoch: 20900 | loss train: 16.427404403686523\n",
      "Epoch: 21000 | loss train: 21.92266273498535\n",
      "Epoch: 21100 | loss train: 1.9842907190322876\n",
      "Epoch: 21200 | loss train: 1.496132493019104\n",
      "Epoch: 21300 | loss train: 1.229175329208374\n",
      "Epoch: 21400 | loss train: 1.0069835186004639\n",
      "Epoch: 21500 | loss train: 0.8388835191726685\n",
      "Epoch: 21600 | loss train: 131.77110290527344\n",
      "Epoch: 21700 | loss train: 3.523954391479492\n",
      "Epoch: 21800 | loss train: 1.9595694541931152\n",
      "Epoch: 21900 | loss train: 1.5343703031539917\n",
      "Epoch: 22000 | loss train: 1.258680820465088\n",
      "Epoch: 22100 | loss train: 1.0873178243637085\n",
      "Epoch: 22200 | loss train: 94.41835021972656\n",
      "Epoch: 22300 | loss train: 2.0103132724761963\n",
      "Epoch: 22400 | loss train: 1.1990509033203125\n",
      "Epoch: 22500 | loss train: 6.002264976501465\n",
      "Epoch: 22600 | loss train: 6.653597831726074\n",
      "Epoch: 22700 | loss train: 1.6704603433609009\n",
      "Epoch: 22800 | loss train: 1.2796128988265991\n",
      "Epoch: 22900 | loss train: 1.0649147033691406\n",
      "Epoch: 23000 | loss train: 0.9318917989730835\n",
      "Epoch: 23100 | loss train: 26.38648223876953\n",
      "Epoch: 23200 | loss train: 2.1623013019561768\n",
      "Epoch: 23300 | loss train: 1.5243885517120361\n",
      "Epoch: 23400 | loss train: 1.1840286254882812\n",
      "Epoch: 23500 | loss train: 0.9747004508972168\n",
      "Epoch: 23600 | loss train: 0.9361554384231567\n",
      "Epoch: 23700 | loss train: 6.314845561981201\n",
      "Epoch: 23800 | loss train: 2.6688647270202637\n",
      "Epoch: 23900 | loss train: 2.082257032394409\n",
      "Epoch: 24000 | loss train: 1.771225094795227\n",
      "Epoch: 24100 | loss train: 1.549829363822937\n",
      "Epoch: 24200 | loss train: 1.3770912885665894\n",
      "Epoch: 24300 | loss train: 1.222823143005371\n",
      "Epoch: 24400 | loss train: 1.460378885269165\n",
      "Epoch: 24500 | loss train: 5.472585678100586\n",
      "Epoch: 24600 | loss train: 2.6686618328094482\n",
      "Epoch: 24700 | loss train: 2.083188533782959\n",
      "Epoch: 24800 | loss train: 1.6870534420013428\n",
      "Epoch: 24900 | loss train: 1.410475730895996\n",
      "Epoch: 25000 | loss train: 7.340936660766602\n",
      "Epoch: 25100 | loss train: 4.022008419036865\n",
      "Epoch: 25200 | loss train: 1.5832873582839966\n",
      "Epoch: 25300 | loss train: 1.2366037368774414\n",
      "Epoch: 25400 | loss train: 1.0387258529663086\n",
      "Epoch: 25500 | loss train: 93.08148193359375\n",
      "Epoch: 25600 | loss train: 3.0735604763031006\n",
      "Epoch: 25700 | loss train: 2.04809308052063\n",
      "Epoch: 25800 | loss train: 1.5907741785049438\n",
      "Epoch: 25900 | loss train: 1.2736738920211792\n",
      "Epoch: 26000 | loss train: 1.0406962633132935\n",
      "Epoch: 26100 | loss train: 0.9016034007072449\n",
      "Epoch: 26200 | loss train: 12.44422721862793\n",
      "Epoch: 26300 | loss train: 2.145223379135132\n",
      "Epoch: 26400 | loss train: 1.9673124551773071\n",
      "Epoch: 26500 | loss train: 1.3102413415908813\n",
      "Epoch: 26600 | loss train: 1.1085643768310547\n",
      "Epoch: 26700 | loss train: 0.9513126611709595\n",
      "Epoch: 26800 | loss train: 0.8220731019973755\n",
      "Epoch: 26900 | loss train: 0.7134880423545837\n",
      "Epoch: 27000 | loss train: 488.6236572265625\n",
      "Epoch: 27100 | loss train: 5.973829746246338\n",
      "Epoch: 27200 | loss train: 3.5035653114318848\n",
      "Epoch: 27300 | loss train: 2.6817920207977295\n",
      "Epoch: 27400 | loss train: 2.2657790184020996\n",
      "Epoch: 27500 | loss train: 1.981323480606079\n",
      "Epoch: 27600 | loss train: 1.7481014728546143\n",
      "Epoch: 27700 | loss train: 1.54037344455719\n",
      "Epoch: 27800 | loss train: 1.552148461341858\n",
      "Epoch: 27900 | loss train: 13.88879108428955\n",
      "Epoch: 28000 | loss train: 1.3200551271438599\n",
      "Epoch: 28100 | loss train: 0.9846081733703613\n",
      "Epoch: 28200 | loss train: 25.140869140625\n",
      "Epoch: 28300 | loss train: 2.4289655685424805\n",
      "Epoch: 28400 | loss train: 1.693300485610962\n",
      "Epoch: 28500 | loss train: 1.4099103212356567\n",
      "Epoch: 28600 | loss train: 1.200275182723999\n",
      "Epoch: 28700 | loss train: 37.66408920288086\n",
      "Epoch: 28800 | loss train: 2.582980155944824\n",
      "Epoch: 28900 | loss train: 1.8951992988586426\n",
      "Epoch: 29000 | loss train: 1.5279335975646973\n",
      "Epoch: 29100 | loss train: 1.315615177154541\n",
      "Epoch: 29200 | loss train: 1.1604477167129517\n",
      "Epoch: 29300 | loss train: 1.035652756690979\n",
      "Epoch: 29400 | loss train: 23.09075355529785\n",
      "Epoch: 29500 | loss train: 2.2532217502593994\n",
      "Epoch: 29600 | loss train: 1.6043950319290161\n",
      "Epoch: 29700 | loss train: 1.2827880382537842\n",
      "Epoch: 29800 | loss train: 1.089807152748108\n",
      "Epoch: 29900 | loss train: 124.88971710205078\n",
      "Epoch: 30000 | loss train: 3.166545867919922\n",
      "Epoch: 30100 | loss train: 1.311751127243042\n",
      "Epoch: 30200 | loss train: 1.0027341842651367\n",
      "Epoch: 30300 | loss train: 0.825811505317688\n",
      "Epoch: 30400 | loss train: 0.7005565166473389\n",
      "Epoch: 30500 | loss train: 0.7001466751098633\n",
      "Epoch: 30600 | loss train: 8.542047500610352\n",
      "Epoch: 30700 | loss train: 3.3087360858917236\n",
      "Epoch: 30800 | loss train: 2.276977300643921\n",
      "Epoch: 30900 | loss train: 1.759139895439148\n",
      "Epoch: 31000 | loss train: 1.388085126876831\n",
      "Epoch: 31100 | loss train: 1.1280796527862549\n",
      "Epoch: 31200 | loss train: 0.9457263946533203\n",
      "Epoch: 31300 | loss train: 0.7946499586105347\n",
      "Epoch: 31400 | loss train: 1.0730654001235962\n",
      "Epoch: 31500 | loss train: 0.996395468711853\n",
      "Epoch: 31600 | loss train: 60.5930290222168\n",
      "Epoch: 31700 | loss train: 2.008195161819458\n",
      "Epoch: 31800 | loss train: 5.110660076141357\n",
      "Epoch: 31900 | loss train: 7.9705328941345215\n",
      "Epoch: 32000 | loss train: 1.7365761995315552\n",
      "Epoch: 32100 | loss train: 1.0294164419174194\n",
      "Epoch: 32200 | loss train: 0.6684606671333313\n",
      "Epoch: 32300 | loss train: 69.49678039550781\n",
      "Epoch: 32400 | loss train: 2.8497302532196045\n",
      "Epoch: 32500 | loss train: 1.4897992610931396\n",
      "Epoch: 32600 | loss train: 0.9498798847198486\n",
      "Epoch: 32700 | loss train: 0.73589688539505\n",
      "Epoch: 32800 | loss train: 27.074392318725586\n",
      "Epoch: 32900 | loss train: 1.8432514667510986\n",
      "Epoch: 33000 | loss train: 1.094372272491455\n",
      "Epoch: 33100 | loss train: 0.7763206958770752\n",
      "Epoch: 33200 | loss train: 0.8702048063278198\n",
      "Epoch: 33300 | loss train: 11.248678207397461\n",
      "Epoch: 33400 | loss train: 1.2694993019104004\n",
      "Epoch: 33500 | loss train: 0.9314183592796326\n",
      "Epoch: 33600 | loss train: 0.7403007745742798\n",
      "Epoch: 33700 | loss train: 0.618767261505127\n",
      "Epoch: 33800 | loss train: 1.4821065664291382\n",
      "Epoch: 33900 | loss train: 0.4519636034965515\n",
      "loss_NN=tensor(4403.2593)\n",
      "pred_NN=tensor([[8345.6074],\n",
      "        [7718.2573],\n",
      "        [6010.6353],\n",
      "        [5867.5044]])\n",
      "loss_RF=3686.0139744961675\n",
      "pred_RF=array([6076.10708252, 6225.93459473, 6778.6585498 , 6541.79444336])\n",
      "loss_DT=3923.154971642367\n",
      "pred_DT=array([8356.9140625 , 6142.71679688, 4605.5078125 , 6142.71679688])\n"
     ]
    }
   ],
   "source": [
    "target_value = \"N\"\n",
    "train_field = \"J\"\n",
    "sample = generate_sample(data_df_13_09_2022, target_value, train_field)\n",
    "X_train, Y_train = create_X_train_Y_train(sample)\n",
    "sample_target = generate_sample(data_df_13_09_2022, target_value, \"BC\")\n",
    "X_target, Y_target = create_X_train_Y_train(sample_target)\n",
    "super_param={\"lr\": 0.0001, \"weight_decay\": 1e-5, \"n_epochs\": 40000, \"stop_value\": 2}\n",
    "re_run = \"Y\"\n",
    "loss_NN, pred_NN = predict_using_neutral_network(X_train, Y_train, X_target, Y_target, f\"{'_'.join(list(train_field))}_model_predict_{target_value}_2.pt\", super_param, re_run)\n",
    "print(f\"{loss_NN=}\")\n",
    "print(f\"{pred_NN=}\")\n",
    "loss_RF, pred_RF = predict_using_random_forest(X_train, Y_train, X_target, Y_target, super_param)\n",
    "print(f\"{loss_RF=}\")\n",
    "print(f\"{pred_RF=}\")\n",
    "loss_DT, pred_DT = predict_using_decision_tree(X_train, Y_train, X_target, Y_target, super_param)\n",
    "print(f\"{loss_DT=}\")\n",
    "print(f\"{pred_DT=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | loss train: 5765.8330078125\n",
      "Epoch: 100 | loss train: 5615.654296875\n",
      "Epoch: 200 | loss train: 2122.3330078125\n",
      "Epoch: 300 | loss train: 2119.249755859375\n",
      "Epoch: 400 | loss train: 2117.529296875\n",
      "Epoch: 500 | loss train: 2114.072509765625\n",
      "Epoch: 600 | loss train: 2108.59228515625\n",
      "Epoch: 700 | loss train: 2099.8623046875\n",
      "Epoch: 800 | loss train: 2084.124755859375\n",
      "Epoch: 900 | loss train: 2049.208740234375\n",
      "Epoch: 1000 | loss train: 1967.3607177734375\n",
      "Epoch: 1100 | loss train: 1754.36572265625\n",
      "Epoch: 1200 | loss train: 1500.2393798828125\n",
      "Epoch: 1300 | loss train: 1340.5423583984375\n",
      "Epoch: 1400 | loss train: 1250.7044677734375\n",
      "Epoch: 1500 | loss train: 1190.007080078125\n",
      "Epoch: 1600 | loss train: 1127.2379150390625\n",
      "Epoch: 1700 | loss train: 1085.2645263671875\n",
      "Epoch: 1800 | loss train: 1046.890869140625\n",
      "Epoch: 1900 | loss train: 1011.197021484375\n",
      "Epoch: 2000 | loss train: 976.3375854492188\n",
      "Epoch: 2100 | loss train: 954.4782104492188\n",
      "Epoch: 2200 | loss train: 922.3305053710938\n",
      "Epoch: 2300 | loss train: 903.95361328125\n",
      "Epoch: 2400 | loss train: 877.4644775390625\n",
      "Epoch: 2500 | loss train: 865.703369140625\n",
      "Epoch: 2600 | loss train: 839.133056640625\n",
      "Epoch: 2700 | loss train: 819.6939086914062\n",
      "Epoch: 2800 | loss train: 820.3958740234375\n",
      "Epoch: 2900 | loss train: 795.4260864257812\n",
      "Epoch: 3000 | loss train: 780.9712524414062\n",
      "Epoch: 3100 | loss train: 758.5512084960938\n",
      "Epoch: 3200 | loss train: 740.1298828125\n",
      "Epoch: 3300 | loss train: 764.86083984375\n",
      "Epoch: 3400 | loss train: 715.309814453125\n",
      "Epoch: 3500 | loss train: 711.144775390625\n",
      "Epoch: 3600 | loss train: 699.0152587890625\n",
      "Epoch: 3700 | loss train: 677.8441772460938\n",
      "Epoch: 3800 | loss train: 665.6901245117188\n",
      "Epoch: 3900 | loss train: 663.04541015625\n",
      "Epoch: 4000 | loss train: 661.2373046875\n",
      "Epoch: 4100 | loss train: 634.1044921875\n",
      "Epoch: 4200 | loss train: 621.9867553710938\n",
      "Epoch: 4300 | loss train: 618.8038330078125\n",
      "Epoch: 4400 | loss train: 610.6131591796875\n",
      "Epoch: 4500 | loss train: 600.3425903320312\n",
      "Epoch: 4600 | loss train: 584.0161743164062\n",
      "Epoch: 4700 | loss train: 573.734375\n",
      "Epoch: 4800 | loss train: 575.8516235351562\n",
      "Epoch: 4900 | loss train: 575.296875\n",
      "Epoch: 5000 | loss train: 551.98974609375\n",
      "Epoch: 5100 | loss train: 560.0457153320312\n",
      "Epoch: 5200 | loss train: 530.6826782226562\n",
      "Epoch: 5300 | loss train: 537.61474609375\n",
      "Epoch: 5400 | loss train: 521.5177612304688\n",
      "Epoch: 5500 | loss train: 520.9141845703125\n",
      "Epoch: 5600 | loss train: 513.9922485351562\n",
      "Epoch: 5700 | loss train: 502.00823974609375\n",
      "Epoch: 5800 | loss train: 491.03961181640625\n",
      "Epoch: 5900 | loss train: 482.8638610839844\n",
      "Epoch: 6000 | loss train: 473.6496887207031\n",
      "Epoch: 6100 | loss train: 461.5992736816406\n",
      "Epoch: 6200 | loss train: 452.17547607421875\n",
      "Epoch: 6300 | loss train: 445.80126953125\n",
      "Epoch: 6400 | loss train: 454.4449462890625\n",
      "Epoch: 6500 | loss train: 443.47442626953125\n",
      "Epoch: 6600 | loss train: 424.7086181640625\n",
      "Epoch: 6700 | loss train: 421.3560791015625\n",
      "Epoch: 6800 | loss train: 438.08709716796875\n",
      "Epoch: 6900 | loss train: 414.09979248046875\n",
      "Epoch: 7000 | loss train: 402.62384033203125\n",
      "Epoch: 7100 | loss train: 389.59722900390625\n",
      "Epoch: 7200 | loss train: 382.3694152832031\n",
      "Epoch: 7300 | loss train: 386.7369079589844\n",
      "Epoch: 7400 | loss train: 375.66046142578125\n",
      "Epoch: 7500 | loss train: 374.9102478027344\n",
      "Epoch: 7600 | loss train: 359.8191833496094\n",
      "Epoch: 7700 | loss train: 352.11492919921875\n",
      "Epoch: 7800 | loss train: 343.83563232421875\n",
      "Epoch: 7900 | loss train: 342.6749572753906\n",
      "Epoch: 8000 | loss train: 341.99383544921875\n",
      "Epoch: 8100 | loss train: 352.05072021484375\n",
      "Epoch: 8200 | loss train: 306.3448486328125\n",
      "Epoch: 8300 | loss train: 303.5601806640625\n",
      "Epoch: 8400 | loss train: 309.2154235839844\n",
      "Epoch: 8500 | loss train: 284.9325866699219\n",
      "Epoch: 8600 | loss train: 341.07708740234375\n",
      "Epoch: 8700 | loss train: 321.2257385253906\n",
      "Epoch: 8800 | loss train: 265.7484130859375\n",
      "Epoch: 8900 | loss train: 259.7854309082031\n",
      "Epoch: 9000 | loss train: 282.96820068359375\n",
      "Epoch: 9100 | loss train: 258.776611328125\n",
      "Epoch: 9200 | loss train: 274.8394470214844\n",
      "Epoch: 9300 | loss train: 242.26918029785156\n",
      "Epoch: 9400 | loss train: 231.8509521484375\n",
      "Epoch: 9500 | loss train: 230.08358764648438\n",
      "Epoch: 9600 | loss train: 224.5263214111328\n",
      "Epoch: 9700 | loss train: 220.42184448242188\n",
      "Epoch: 9800 | loss train: 234.21221923828125\n",
      "Epoch: 9900 | loss train: 213.64276123046875\n",
      "Epoch: 10000 | loss train: 236.4363555908203\n",
      "Epoch: 10100 | loss train: 210.6084442138672\n",
      "Epoch: 10200 | loss train: 204.67340087890625\n",
      "Epoch: 10300 | loss train: 210.73928833007812\n",
      "Epoch: 10400 | loss train: 208.35638427734375\n",
      "Epoch: 10500 | loss train: 283.1607666015625\n",
      "Epoch: 10600 | loss train: 195.134521484375\n",
      "Epoch: 10700 | loss train: 204.6154022216797\n",
      "Epoch: 10800 | loss train: 194.0326690673828\n",
      "Epoch: 10900 | loss train: 192.41697692871094\n",
      "Epoch: 11000 | loss train: 187.9406280517578\n",
      "Epoch: 11100 | loss train: 186.3016357421875\n",
      "Epoch: 11200 | loss train: 185.5201416015625\n",
      "Epoch: 11300 | loss train: 249.07864379882812\n",
      "Epoch: 11400 | loss train: 184.0261993408203\n",
      "Epoch: 11500 | loss train: 205.98411560058594\n",
      "Epoch: 11600 | loss train: 179.11534118652344\n",
      "Epoch: 11700 | loss train: 283.8406066894531\n",
      "Epoch: 11800 | loss train: 176.75115966796875\n",
      "Epoch: 11900 | loss train: 217.13754272460938\n",
      "Epoch: 12000 | loss train: 177.08738708496094\n",
      "Epoch: 12100 | loss train: 203.0707550048828\n",
      "Epoch: 12200 | loss train: 176.09591674804688\n",
      "Epoch: 12300 | loss train: 174.58743286132812\n",
      "Epoch: 12400 | loss train: 195.4953155517578\n",
      "Epoch: 12500 | loss train: 172.32696533203125\n",
      "Epoch: 12600 | loss train: 189.5623016357422\n",
      "Epoch: 12700 | loss train: 170.58787536621094\n",
      "Epoch: 12800 | loss train: 205.42959594726562\n",
      "Epoch: 12900 | loss train: 219.75747680664062\n",
      "Epoch: 13000 | loss train: 169.2636260986328\n",
      "Epoch: 13100 | loss train: 293.071533203125\n",
      "Epoch: 13200 | loss train: 166.51158142089844\n",
      "Epoch: 13300 | loss train: 217.83631896972656\n",
      "Epoch: 13400 | loss train: 176.30064392089844\n",
      "Epoch: 13500 | loss train: 356.09442138671875\n",
      "Epoch: 13600 | loss train: 163.82363891601562\n",
      "Epoch: 13700 | loss train: 169.7122344970703\n",
      "Epoch: 13800 | loss train: 162.66099548339844\n",
      "Epoch: 13900 | loss train: 193.34059143066406\n",
      "Epoch: 14000 | loss train: 160.78501892089844\n",
      "Epoch: 14100 | loss train: 164.60813903808594\n",
      "Epoch: 14200 | loss train: 194.5233154296875\n",
      "Epoch: 14300 | loss train: 159.38624572753906\n",
      "Epoch: 14400 | loss train: 163.97911071777344\n",
      "Epoch: 14500 | loss train: 165.47628784179688\n",
      "Epoch: 14600 | loss train: 157.2061309814453\n",
      "Epoch: 14700 | loss train: 159.7450408935547\n",
      "Epoch: 14800 | loss train: 155.5697479248047\n",
      "Epoch: 14900 | loss train: 154.77305603027344\n",
      "Epoch: 15000 | loss train: 152.9834442138672\n",
      "Epoch: 15100 | loss train: 202.5663604736328\n",
      "Epoch: 15200 | loss train: 167.82373046875\n",
      "Epoch: 15300 | loss train: 161.05650329589844\n",
      "Epoch: 15400 | loss train: 216.80978393554688\n",
      "Epoch: 15500 | loss train: 145.24002075195312\n",
      "Epoch: 15600 | loss train: 183.0500030517578\n",
      "Epoch: 15700 | loss train: 260.5855407714844\n",
      "Epoch: 15800 | loss train: 188.0520782470703\n",
      "Epoch: 15900 | loss train: 164.98988342285156\n",
      "Epoch: 16000 | loss train: 223.53009033203125\n",
      "Epoch: 16100 | loss train: 158.82470703125\n",
      "Epoch: 16200 | loss train: 136.74609375\n",
      "Epoch: 16300 | loss train: 181.29483032226562\n",
      "Epoch: 16400 | loss train: 134.6143035888672\n",
      "Epoch: 16500 | loss train: 188.5281219482422\n",
      "Epoch: 16600 | loss train: 239.55889892578125\n",
      "Epoch: 16700 | loss train: 209.6500244140625\n",
      "Epoch: 16800 | loss train: 133.57093811035156\n",
      "Epoch: 16900 | loss train: 124.35311126708984\n",
      "Epoch: 17000 | loss train: 120.41580963134766\n",
      "Epoch: 17100 | loss train: 343.98834228515625\n",
      "Epoch: 17200 | loss train: 121.74372863769531\n",
      "Epoch: 17300 | loss train: 117.49295806884766\n",
      "Epoch: 17400 | loss train: 137.68502807617188\n",
      "Epoch: 17500 | loss train: 323.56610107421875\n",
      "Epoch: 17600 | loss train: 121.5174331665039\n",
      "Epoch: 17700 | loss train: 115.35220336914062\n",
      "Epoch: 17800 | loss train: 113.18975067138672\n",
      "Epoch: 17900 | loss train: 115.81616973876953\n",
      "Epoch: 18000 | loss train: 136.3217010498047\n",
      "Epoch: 18100 | loss train: 117.3756332397461\n",
      "Epoch: 18200 | loss train: 116.09332275390625\n",
      "Epoch: 18300 | loss train: 112.2650375366211\n",
      "Epoch: 18400 | loss train: 122.37220001220703\n",
      "Epoch: 18500 | loss train: 112.99400329589844\n",
      "Epoch: 18600 | loss train: 110.39703369140625\n",
      "Epoch: 18700 | loss train: 108.47814178466797\n",
      "Epoch: 18800 | loss train: 107.58973693847656\n",
      "Epoch: 18900 | loss train: 106.7542953491211\n",
      "Epoch: 19000 | loss train: 104.38156127929688\n",
      "Epoch: 19100 | loss train: 131.0543212890625\n",
      "Epoch: 19200 | loss train: 118.82172393798828\n",
      "Epoch: 19300 | loss train: 117.50154876708984\n",
      "Epoch: 19400 | loss train: 215.20921325683594\n",
      "Epoch: 19500 | loss train: 101.51327514648438\n",
      "Epoch: 19600 | loss train: 224.96507263183594\n",
      "Epoch: 19700 | loss train: 261.81427001953125\n",
      "Epoch: 19800 | loss train: 107.41770935058594\n",
      "Epoch: 19900 | loss train: 112.23426818847656\n",
      "Epoch: 20000 | loss train: 103.43158721923828\n",
      "Epoch: 20100 | loss train: 130.03407287597656\n",
      "Epoch: 20200 | loss train: 104.177001953125\n",
      "Epoch: 20300 | loss train: 110.77674102783203\n",
      "Epoch: 20400 | loss train: 97.7364501953125\n",
      "Epoch: 20500 | loss train: 104.9264144897461\n",
      "Epoch: 20600 | loss train: 113.89054870605469\n",
      "Epoch: 20700 | loss train: 106.4757308959961\n",
      "Epoch: 20800 | loss train: 95.83220672607422\n",
      "Epoch: 20900 | loss train: 96.90167236328125\n",
      "Epoch: 21000 | loss train: 146.374755859375\n",
      "Epoch: 21100 | loss train: 98.65437316894531\n",
      "Epoch: 21200 | loss train: 106.53714752197266\n",
      "Epoch: 21300 | loss train: 211.1023406982422\n",
      "Epoch: 21400 | loss train: 98.32369232177734\n",
      "Epoch: 21500 | loss train: 103.38676452636719\n",
      "Epoch: 21600 | loss train: 98.61428833007812\n",
      "Epoch: 21700 | loss train: 92.29987335205078\n",
      "Epoch: 21800 | loss train: 93.08919525146484\n",
      "Epoch: 21900 | loss train: 89.63807678222656\n",
      "Epoch: 22000 | loss train: 94.9905776977539\n",
      "Epoch: 22100 | loss train: 116.64920043945312\n",
      "Epoch: 22200 | loss train: 89.4853744506836\n",
      "Epoch: 22300 | loss train: 102.64569854736328\n",
      "Epoch: 22400 | loss train: 319.9705810546875\n",
      "Epoch: 22500 | loss train: 93.00308990478516\n",
      "Epoch: 22600 | loss train: 160.64105224609375\n",
      "Epoch: 22700 | loss train: 133.2012176513672\n",
      "Epoch: 22800 | loss train: 108.79562377929688\n",
      "Epoch: 22900 | loss train: 85.84848022460938\n",
      "Epoch: 23000 | loss train: 85.77841186523438\n",
      "Epoch: 23100 | loss train: 96.50819396972656\n",
      "Epoch: 23200 | loss train: 83.10396575927734\n",
      "Epoch: 23300 | loss train: 89.12328338623047\n",
      "Epoch: 23400 | loss train: 83.49286651611328\n",
      "Epoch: 23500 | loss train: 156.6353759765625\n",
      "Epoch: 23600 | loss train: 325.5814514160156\n",
      "Epoch: 23700 | loss train: 81.6624984741211\n",
      "Epoch: 23800 | loss train: 81.09951782226562\n",
      "Epoch: 23900 | loss train: 80.07176971435547\n",
      "Epoch: 24000 | loss train: 82.92095184326172\n",
      "Epoch: 24100 | loss train: 82.0141830444336\n",
      "Epoch: 24200 | loss train: 81.51744842529297\n",
      "Epoch: 24300 | loss train: 77.82589721679688\n",
      "Epoch: 24400 | loss train: 106.32254028320312\n",
      "Epoch: 24500 | loss train: 85.55912017822266\n",
      "Epoch: 24600 | loss train: 76.06869506835938\n",
      "Epoch: 24700 | loss train: 89.25296020507812\n",
      "Epoch: 24800 | loss train: 239.46080017089844\n",
      "Epoch: 24900 | loss train: 75.04396057128906\n",
      "Epoch: 25000 | loss train: 77.26380157470703\n",
      "Epoch: 25100 | loss train: 78.24357604980469\n",
      "Epoch: 25200 | loss train: 88.13362884521484\n",
      "Epoch: 25300 | loss train: 75.2802505493164\n",
      "Epoch: 25400 | loss train: 72.30072784423828\n",
      "Epoch: 25500 | loss train: 87.66757202148438\n",
      "Epoch: 25600 | loss train: 73.030029296875\n",
      "Epoch: 25700 | loss train: 77.09451293945312\n",
      "Epoch: 25800 | loss train: 71.51741027832031\n",
      "Epoch: 25900 | loss train: 289.8846740722656\n",
      "Epoch: 26000 | loss train: 71.61434936523438\n",
      "Epoch: 26100 | loss train: 69.72187042236328\n",
      "Epoch: 26200 | loss train: 70.8056869506836\n",
      "Epoch: 26300 | loss train: 79.74258422851562\n",
      "Epoch: 26400 | loss train: 97.8823013305664\n",
      "Epoch: 26500 | loss train: 67.24229431152344\n",
      "Epoch: 26600 | loss train: 90.18617248535156\n",
      "Epoch: 26700 | loss train: 78.79988861083984\n",
      "Epoch: 26800 | loss train: 66.92845916748047\n",
      "Epoch: 26900 | loss train: 64.27944946289062\n",
      "Epoch: 27000 | loss train: 63.57086181640625\n",
      "Epoch: 27100 | loss train: 73.1962890625\n",
      "Epoch: 27200 | loss train: 70.06172180175781\n",
      "Epoch: 27300 | loss train: 63.39607620239258\n",
      "Epoch: 27400 | loss train: 61.23557662963867\n",
      "Epoch: 27500 | loss train: 97.60747528076172\n",
      "Epoch: 27600 | loss train: 71.81595611572266\n",
      "Epoch: 27700 | loss train: 95.24136352539062\n",
      "Epoch: 27800 | loss train: 60.15049743652344\n",
      "Epoch: 27900 | loss train: 59.24772262573242\n",
      "Epoch: 28000 | loss train: 58.31000518798828\n",
      "Epoch: 28100 | loss train: 59.73017501831055\n",
      "Epoch: 28200 | loss train: 59.67802810668945\n",
      "Epoch: 28300 | loss train: 61.50865173339844\n",
      "Epoch: 28400 | loss train: 56.689369201660156\n",
      "Epoch: 28500 | loss train: 59.468448638916016\n",
      "Epoch: 28600 | loss train: 56.421531677246094\n",
      "Epoch: 28700 | loss train: 86.38164520263672\n",
      "Epoch: 28800 | loss train: 53.638729095458984\n",
      "Epoch: 28900 | loss train: 52.108001708984375\n",
      "Epoch: 29000 | loss train: 53.47935104370117\n",
      "Epoch: 29100 | loss train: 54.53096389770508\n",
      "Epoch: 29200 | loss train: 244.3556365966797\n",
      "Epoch: 29300 | loss train: 49.53887176513672\n",
      "Epoch: 29400 | loss train: 49.54111099243164\n",
      "Epoch: 29500 | loss train: 61.27712631225586\n",
      "Epoch: 29600 | loss train: 81.57449340820312\n",
      "Epoch: 29700 | loss train: 46.607078552246094\n",
      "Epoch: 29800 | loss train: 46.02170181274414\n",
      "Epoch: 29900 | loss train: 51.46685791015625\n",
      "Epoch: 30000 | loss train: 68.79014587402344\n",
      "Epoch: 30100 | loss train: 51.14098358154297\n",
      "Epoch: 30200 | loss train: 42.125823974609375\n",
      "Epoch: 30300 | loss train: 43.00692367553711\n",
      "Epoch: 30400 | loss train: 44.224422454833984\n",
      "Epoch: 30500 | loss train: 228.3631134033203\n",
      "Epoch: 30600 | loss train: 40.88521194458008\n",
      "Epoch: 30700 | loss train: 38.87192916870117\n",
      "Epoch: 30800 | loss train: 37.8282470703125\n",
      "Epoch: 30900 | loss train: 69.65277862548828\n",
      "Epoch: 31000 | loss train: 44.18207931518555\n",
      "Epoch: 31100 | loss train: 43.19307327270508\n",
      "Epoch: 31200 | loss train: 33.713069915771484\n",
      "Epoch: 31300 | loss train: 31.968111038208008\n",
      "Epoch: 31400 | loss train: 33.837223052978516\n",
      "Epoch: 31500 | loss train: 35.504364013671875\n",
      "Epoch: 31600 | loss train: 31.47589874267578\n",
      "Epoch: 31700 | loss train: 34.500064849853516\n",
      "Epoch: 31800 | loss train: 27.726137161254883\n",
      "Epoch: 31900 | loss train: 27.098407745361328\n",
      "Epoch: 32000 | loss train: 25.57857894897461\n",
      "Epoch: 32100 | loss train: 32.365108489990234\n",
      "Epoch: 32200 | loss train: 33.56134033203125\n",
      "Epoch: 32300 | loss train: 23.294958114624023\n",
      "Epoch: 32400 | loss train: 20.97250747680664\n",
      "Epoch: 32500 | loss train: 21.861047744750977\n",
      "Epoch: 32600 | loss train: 42.24125671386719\n",
      "Epoch: 32700 | loss train: 37.76601028442383\n",
      "Epoch: 32800 | loss train: 18.197458267211914\n",
      "Epoch: 32900 | loss train: 17.480106353759766\n",
      "Epoch: 33000 | loss train: 17.673198699951172\n",
      "Epoch: 33100 | loss train: 160.7481231689453\n",
      "Epoch: 33200 | loss train: 19.918411254882812\n",
      "Epoch: 33300 | loss train: 48.74637985229492\n",
      "Epoch: 33400 | loss train: 14.598959922790527\n",
      "Epoch: 33500 | loss train: 12.401556968688965\n",
      "Epoch: 33600 | loss train: 11.10205364227295\n",
      "Epoch: 33700 | loss train: 10.581815719604492\n",
      "Epoch: 33800 | loss train: 23.49600601196289\n",
      "Epoch: 33900 | loss train: 22.24843406677246\n",
      "Epoch: 34000 | loss train: 10.511144638061523\n",
      "Epoch: 34100 | loss train: 9.8533353805542\n",
      "Epoch: 34200 | loss train: 15.55483627319336\n",
      "Epoch: 34300 | loss train: 49.01954650878906\n",
      "Epoch: 34400 | loss train: 8.533451080322266\n",
      "Epoch: 34500 | loss train: 13.911886215209961\n",
      "Epoch: 34600 | loss train: 309.473876953125\n",
      "Epoch: 34700 | loss train: 15.569262504577637\n",
      "Epoch: 34800 | loss train: 9.907109260559082\n",
      "Epoch: 34900 | loss train: 8.217201232910156\n",
      "Epoch: 35000 | loss train: 6.97486686706543\n",
      "Epoch: 35100 | loss train: 6.107461452484131\n",
      "Epoch: 35200 | loss train: 5.9927239418029785\n",
      "Epoch: 35300 | loss train: 13.543455123901367\n",
      "Epoch: 35400 | loss train: 17.831445693969727\n",
      "Epoch: 35500 | loss train: 9.110368728637695\n",
      "Epoch: 35600 | loss train: 7.954382419586182\n",
      "Epoch: 35700 | loss train: 7.255115509033203\n",
      "Epoch: 35800 | loss train: 46.0548210144043\n",
      "Epoch: 35900 | loss train: 7.8434271812438965\n",
      "Epoch: 36000 | loss train: 5.278979301452637\n",
      "Epoch: 36100 | loss train: 4.438122749328613\n",
      "Epoch: 36200 | loss train: 3.9336283206939697\n",
      "Epoch: 36300 | loss train: 576.8645629882812\n",
      "Epoch: 36400 | loss train: 12.561368942260742\n",
      "Epoch: 36500 | loss train: 7.1368021965026855\n",
      "Epoch: 36600 | loss train: 5.695152759552002\n",
      "Epoch: 36700 | loss train: 4.891411304473877\n",
      "Epoch: 36800 | loss train: 4.3526129722595215\n",
      "Epoch: 36900 | loss train: 3.954512119293213\n",
      "Epoch: 37000 | loss train: 229.4445037841797\n",
      "Epoch: 37100 | loss train: 10.58817195892334\n",
      "Epoch: 37200 | loss train: 7.036097049713135\n",
      "Epoch: 37300 | loss train: 5.631948947906494\n",
      "Epoch: 37400 | loss train: 4.7895426750183105\n",
      "Epoch: 37500 | loss train: 4.309535503387451\n",
      "Epoch: 37600 | loss train: 75.81122589111328\n",
      "Epoch: 37700 | loss train: 14.220958709716797\n",
      "Epoch: 37800 | loss train: 8.893596649169922\n",
      "Epoch: 37900 | loss train: 6.173771381378174\n",
      "Epoch: 38000 | loss train: 4.81298303604126\n",
      "Epoch: 38100 | loss train: 3.955047369003296\n",
      "Epoch: 38200 | loss train: 3.496852159500122\n",
      "Epoch: 38300 | loss train: 200.89990234375\n",
      "Epoch: 38400 | loss train: 5.674600124359131\n",
      "Epoch: 38500 | loss train: 3.904200792312622\n",
      "Epoch: 38600 | loss train: 45.208580017089844\n",
      "Epoch: 38700 | loss train: 12.898045539855957\n",
      "Epoch: 38800 | loss train: 9.696467399597168\n",
      "Epoch: 38900 | loss train: 8.229765892028809\n",
      "Epoch: 39000 | loss train: 7.242936134338379\n",
      "Epoch: 39100 | loss train: 6.483840465545654\n",
      "Epoch: 39200 | loss train: 6.507153511047363\n",
      "Epoch: 39300 | loss train: 22.546106338500977\n",
      "Epoch: 39400 | loss train: 138.97300720214844\n",
      "Epoch: 39500 | loss train: 12.315332412719727\n",
      "Epoch: 39600 | loss train: 8.140642166137695\n",
      "Epoch: 39700 | loss train: 6.922538757324219\n",
      "Epoch: 39800 | loss train: 6.3393168449401855\n",
      "Epoch: 39900 | loss train: 6.035305023193359\n",
      "loss_NN=tensor(6143.2378)\n",
      "pred_NN=tensor([[12636.2314],\n",
      "        [ 8857.8271],\n",
      "        [ 5098.3394],\n",
      "        [ 5231.9829]])\n",
      "loss_RF=2585.7663263899076\n",
      "pred_RF=array([5535.88842896, 5593.73553589, 4979.35848877, 5039.04884033])\n",
      "loss_DT=2722.122663394157\n",
      "pred_DT=array([4787.27099609, 6428.39550781, 5185.14404297, 5365.20800781])\n"
     ]
    }
   ],
   "source": [
    "target_value = \"N\"\n",
    "train_field = [\"T\", \"J\"]\n",
    "sample = generate_sample(data_df_13_09_2022, target_value, train_field)\n",
    "X_train, Y_train = create_X_train_Y_train(sample)\n",
    "sample_target = generate_sample(data_df_13_09_2022, target_value, \"BC\")\n",
    "X_target, Y_target = create_X_train_Y_train(sample_target)\n",
    "super_param={\"lr\": 0.0001, \"weight_decay\": 1e-5, \"n_epochs\": 40000, \"stop_value\": 0.5}\n",
    "re_run = \"N\"\n",
    "loss_NN, pred_NN = predict_using_neutral_network(X_train, Y_train, X_target, Y_target, f\"{'_'.join(list(train_field))}_model_predict_{target_value}_2.pt\", super_param, re_run)\n",
    "print(f\"{loss_NN=}\")\n",
    "print(f\"{pred_NN=}\")\n",
    "loss_RF, pred_RF = predict_using_random_forest(X_train, Y_train, X_target, Y_target, super_param)\n",
    "print(f\"{loss_RF=}\")\n",
    "print(f\"{pred_RF=}\")\n",
    "loss_DT, pred_DT = predict_using_decision_tree(X_train, Y_train, X_target, Y_target, super_param)\n",
    "print(f\"{loss_DT=}\")\n",
    "print(f\"{pred_DT=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dự đoán Photpho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | loss train: 5990.28173828125\n",
      "Epoch: 100 | loss train: 5776.95263671875\n",
      "Epoch: 200 | loss train: 790.884765625\n",
      "Epoch: 300 | loss train: 780.4788818359375\n",
      "Epoch: 400 | loss train: 772.1525268554688\n",
      "Epoch: 500 | loss train: 756.95263671875\n",
      "Epoch: 600 | loss train: 730.9122924804688\n",
      "Epoch: 700 | loss train: 693.7506713867188\n",
      "Epoch: 800 | loss train: 657.3466186523438\n",
      "Epoch: 900 | loss train: 602.9769287109375\n",
      "Epoch: 1000 | loss train: 502.7089538574219\n",
      "Epoch: 1100 | loss train: 408.2283630371094\n",
      "Epoch: 1200 | loss train: 342.7757873535156\n",
      "Epoch: 1300 | loss train: 293.5379943847656\n",
      "Epoch: 1400 | loss train: 257.544921875\n",
      "Epoch: 1500 | loss train: 228.84620666503906\n",
      "Epoch: 1600 | loss train: 206.94920349121094\n",
      "Epoch: 1700 | loss train: 185.49302673339844\n",
      "Epoch: 1800 | loss train: 170.70848083496094\n",
      "Epoch: 1900 | loss train: 157.85256958007812\n",
      "Epoch: 2000 | loss train: 143.68569946289062\n",
      "Epoch: 2100 | loss train: 131.73370361328125\n",
      "Epoch: 2200 | loss train: 121.45365142822266\n",
      "Epoch: 2300 | loss train: 112.94918823242188\n",
      "Epoch: 2400 | loss train: 105.48165893554688\n",
      "Epoch: 2500 | loss train: 98.94636535644531\n",
      "Epoch: 2600 | loss train: 89.94210815429688\n",
      "Epoch: 2700 | loss train: 83.06602478027344\n",
      "Epoch: 2800 | loss train: 80.05289459228516\n",
      "Epoch: 2900 | loss train: 79.3885726928711\n",
      "Epoch: 3000 | loss train: 72.94625091552734\n",
      "Epoch: 3100 | loss train: 67.8971176147461\n",
      "Epoch: 3200 | loss train: 57.99324417114258\n",
      "Epoch: 3300 | loss train: 55.105133056640625\n",
      "Epoch: 3400 | loss train: 51.67655944824219\n",
      "Epoch: 3500 | loss train: 46.97575378417969\n",
      "Epoch: 3600 | loss train: 46.999916076660156\n",
      "Epoch: 3700 | loss train: 44.57522201538086\n",
      "Epoch: 3800 | loss train: 47.680049896240234\n",
      "Epoch: 3900 | loss train: 35.73627471923828\n",
      "Epoch: 4000 | loss train: 50.66077423095703\n",
      "Epoch: 4100 | loss train: 37.734920501708984\n",
      "Epoch: 4200 | loss train: 37.954559326171875\n",
      "Epoch: 4300 | loss train: 27.46084213256836\n",
      "Epoch: 4400 | loss train: 33.59081268310547\n",
      "Epoch: 4500 | loss train: 28.960594177246094\n",
      "Epoch: 4600 | loss train: 22.83330726623535\n",
      "Epoch: 4700 | loss train: 23.449913024902344\n",
      "Epoch: 4800 | loss train: 23.405012130737305\n",
      "Epoch: 4900 | loss train: 19.583452224731445\n",
      "Epoch: 5000 | loss train: 18.56195068359375\n",
      "Epoch: 5100 | loss train: 18.499629974365234\n",
      "Epoch: 5200 | loss train: 33.714599609375\n",
      "Epoch: 5300 | loss train: 23.212278366088867\n",
      "Epoch: 5400 | loss train: 14.611011505126953\n",
      "Epoch: 5500 | loss train: 27.08869743347168\n",
      "Epoch: 5600 | loss train: 14.28447151184082\n",
      "Epoch: 5700 | loss train: 20.584129333496094\n",
      "Epoch: 5800 | loss train: 20.1551456451416\n",
      "Epoch: 5900 | loss train: 21.34608268737793\n",
      "Epoch: 6000 | loss train: 12.841957092285156\n",
      "Epoch: 6100 | loss train: 13.28921890258789\n",
      "Epoch: 6200 | loss train: 12.08712387084961\n",
      "Epoch: 6300 | loss train: 10.50987720489502\n",
      "Epoch: 6400 | loss train: 13.42858600616455\n",
      "Epoch: 6500 | loss train: 9.111068725585938\n",
      "Epoch: 6600 | loss train: 9.54140567779541\n",
      "Epoch: 6700 | loss train: 20.726961135864258\n",
      "Epoch: 6800 | loss train: 10.914115905761719\n",
      "Epoch: 6900 | loss train: 7.448849201202393\n",
      "Epoch: 7000 | loss train: 28.999069213867188\n",
      "Epoch: 7100 | loss train: 8.629878044128418\n",
      "Epoch: 7200 | loss train: 8.96767520904541\n",
      "Epoch: 7300 | loss train: 6.215734004974365\n",
      "Epoch: 7400 | loss train: 11.068344116210938\n",
      "Epoch: 7500 | loss train: 17.778715133666992\n",
      "Epoch: 7600 | loss train: 5.722878456115723\n",
      "Epoch: 7700 | loss train: 6.311455249786377\n",
      "Epoch: 7800 | loss train: 8.468140602111816\n",
      "Epoch: 7900 | loss train: 35.79712677001953\n",
      "Epoch: 8000 | loss train: 4.766195774078369\n",
      "Epoch: 8100 | loss train: 5.131072521209717\n",
      "Epoch: 8200 | loss train: 13.892882347106934\n",
      "Epoch: 8300 | loss train: 16.93619728088379\n",
      "Epoch: 8400 | loss train: 73.07284545898438\n",
      "Epoch: 8500 | loss train: 5.139078617095947\n",
      "Epoch: 8600 | loss train: 3.9896137714385986\n",
      "Epoch: 8700 | loss train: 11.545699119567871\n",
      "Epoch: 8800 | loss train: 13.481049537658691\n",
      "Epoch: 8900 | loss train: 3.510432481765747\n",
      "Epoch: 9000 | loss train: 5.893708229064941\n",
      "Epoch: 9100 | loss train: 3.1619675159454346\n",
      "Epoch: 9200 | loss train: 5.52604866027832\n",
      "Epoch: 9300 | loss train: 15.649608612060547\n",
      "Epoch: 9400 | loss train: 2.865682601928711\n",
      "Epoch: 9500 | loss train: 3.428497791290283\n",
      "Epoch: 9600 | loss train: 2.6619186401367188\n",
      "Epoch: 9700 | loss train: 26.566001892089844\n",
      "Epoch: 9800 | loss train: 2.4904139041900635\n",
      "Epoch: 9900 | loss train: 9.345717430114746\n",
      "Epoch: 10000 | loss train: 2.341529369354248\n",
      "Epoch: 10100 | loss train: 55.25373840332031\n",
      "Epoch: 10200 | loss train: 2.2844645977020264\n",
      "Epoch: 10300 | loss train: 2.45025372505188\n",
      "Epoch: 10400 | loss train: 2.3712472915649414\n",
      "Epoch: 10500 | loss train: 2.0410680770874023\n",
      "Epoch: 10600 | loss train: 43.67237091064453\n",
      "Epoch: 10700 | loss train: 2.045408010482788\n",
      "Epoch: 10800 | loss train: 2.4295265674591064\n",
      "Epoch: 10900 | loss train: 2.07822322845459\n",
      "Epoch: 11000 | loss train: 1.7629576921463013\n",
      "Epoch: 11100 | loss train: 6.117070198059082\n",
      "Epoch: 11200 | loss train: 1.7419360876083374\n",
      "Epoch: 11300 | loss train: 9.569573402404785\n",
      "Epoch: 11400 | loss train: 1.8143945932388306\n",
      "Epoch: 11500 | loss train: 1.583003044128418\n",
      "Epoch: 11600 | loss train: 42.68931198120117\n",
      "Epoch: 11700 | loss train: 1.7094930410385132\n",
      "Epoch: 11800 | loss train: 1.5255236625671387\n",
      "Epoch: 11900 | loss train: 1.4037327766418457\n",
      "Epoch: 12000 | loss train: 13.75083065032959\n",
      "Epoch: 12100 | loss train: 1.5019264221191406\n",
      "Epoch: 12200 | loss train: 1.356412649154663\n",
      "Epoch: 12300 | loss train: 4.2623701095581055\n",
      "Epoch: 12400 | loss train: 2.279041290283203\n",
      "Epoch: 12500 | loss train: 1.3017672300338745\n",
      "Epoch: 12600 | loss train: 1.197197437286377\n",
      "Epoch: 12700 | loss train: 33.79319381713867\n",
      "Epoch: 12800 | loss train: 1.403036117553711\n",
      "Epoch: 12900 | loss train: 1.2262060642242432\n",
      "Epoch: 13000 | loss train: 1.135970115661621\n",
      "Epoch: 13100 | loss train: 1.1140938997268677\n",
      "Epoch: 13200 | loss train: 2.767796039581299\n",
      "Epoch: 13300 | loss train: 1.097780704498291\n",
      "Epoch: 13400 | loss train: 82.90399932861328\n",
      "Epoch: 13500 | loss train: 1.3337332010269165\n",
      "Epoch: 13600 | loss train: 1.0716273784637451\n",
      "Epoch: 13700 | loss train: 0.9758455753326416\n",
      "Epoch: 13800 | loss train: 37.43848419189453\n",
      "Epoch: 13900 | loss train: 1.1736366748809814\n",
      "Epoch: 14000 | loss train: 0.9991688132286072\n",
      "Epoch: 14100 | loss train: 0.9092370271682739\n",
      "Epoch: 14200 | loss train: 27.463945388793945\n",
      "Epoch: 14300 | loss train: 1.1637507677078247\n",
      "Epoch: 14400 | loss train: 0.9591168761253357\n",
      "Epoch: 14500 | loss train: 0.8660082817077637\n",
      "Epoch: 14600 | loss train: 0.8056483864784241\n",
      "Epoch: 14700 | loss train: 2.944783926010132\n",
      "Epoch: 14800 | loss train: 0.9706712961196899\n",
      "Epoch: 14900 | loss train: 0.8508281707763672\n",
      "Epoch: 15000 | loss train: 0.7804670333862305\n",
      "Epoch: 15100 | loss train: 4.13461446762085\n",
      "Epoch: 15200 | loss train: 0.9309688210487366\n",
      "Epoch: 15300 | loss train: 0.7962286472320557\n",
      "Epoch: 15400 | loss train: 29.577350616455078\n",
      "Epoch: 15500 | loss train: 0.9727431535720825\n",
      "Epoch: 15600 | loss train: 0.8012108206748962\n",
      "Epoch: 15700 | loss train: 8.164693832397461\n",
      "Epoch: 15800 | loss train: 1.1658742427825928\n",
      "Epoch: 15900 | loss train: 0.8362593054771423\n",
      "Epoch: 16000 | loss train: 0.7285922765731812\n",
      "Epoch: 16100 | loss train: 2.490013837814331\n",
      "Epoch: 16200 | loss train: 0.8697503209114075\n",
      "Epoch: 16300 | loss train: 0.7270996570587158\n",
      "Epoch: 16400 | loss train: 9.02626895904541\n",
      "Epoch: 16500 | loss train: 0.8456035256385803\n",
      "Epoch: 16600 | loss train: 0.7068160176277161\n",
      "Epoch: 16700 | loss train: 72.971923828125\n",
      "Epoch: 16800 | loss train: 1.051849603652954\n",
      "Epoch: 16900 | loss train: 0.777206301689148\n",
      "Epoch: 17000 | loss train: 0.6709307432174683\n",
      "Epoch: 17100 | loss train: 92.99069213867188\n",
      "Epoch: 17200 | loss train: 1.1045455932617188\n",
      "Epoch: 17300 | loss train: 0.7193296551704407\n",
      "Epoch: 17400 | loss train: 0.6267775893211365\n",
      "Epoch: 17500 | loss train: 7.53071928024292\n",
      "Epoch: 17600 | loss train: 0.8450757265090942\n",
      "Epoch: 17700 | loss train: 0.677776038646698\n",
      "Epoch: 17800 | loss train: 0.7142114639282227\n",
      "Epoch: 17900 | loss train: 2.265683174133301\n",
      "Epoch: 18000 | loss train: 0.7962785363197327\n",
      "Epoch: 18100 | loss train: 0.6507729887962341\n",
      "Epoch: 18200 | loss train: 0.5758388638496399\n",
      "Epoch: 18300 | loss train: 5.414114475250244\n",
      "Epoch: 18400 | loss train: 0.7460178732872009\n",
      "Epoch: 18500 | loss train: 0.615287721157074\n",
      "Epoch: 18600 | loss train: 0.5463337302207947\n",
      "Epoch: 18700 | loss train: 53.900672912597656\n",
      "Epoch: 18800 | loss train: 1.6174932718276978\n",
      "Epoch: 18900 | loss train: 0.7637622952461243\n",
      "Epoch: 19000 | loss train: 0.6288315653800964\n",
      "Epoch: 19100 | loss train: 0.5553263425827026\n",
      "Epoch: 19200 | loss train: 0.5036422610282898\n",
      "Epoch: 19300 | loss train: 0.46251580119132996\n",
      "loss_NN=tensor(1075.8458)\n",
      "pred_NN=tensor([[6021.8184],\n",
      "        [5772.1479],\n",
      "        [6420.8955],\n",
      "        [5364.3618]])\n",
      "loss_RF=711.6598287668098\n",
      "pred_RF=array([6235.14006836, 6312.41830078, 5845.09383301, 6084.25864258])\n",
      "loss_DT=788.6942718848296\n",
      "pred_DT=array([5539.64746094, 7303.16357422, 4766.24169922, 6433.33984375])\n"
     ]
    }
   ],
   "source": [
    "target_value = \"P\"\n",
    "train_field = \"T\"\n",
    "sample = generate_sample(data_df_13_09_2022, target_value, train_field)\n",
    "X_train, Y_train = create_X_train_Y_train(sample)\n",
    "sample_target = generate_sample(data_df_13_09_2022, target_value, \"BC\")\n",
    "X_target, Y_target = create_X_train_Y_train(sample_target)\n",
    "super_param={\"lr\": 0.0001, \"weight_decay\": 1e-5, \"n_epochs\": 40000, \"stop_value\": 0.5}\n",
    "re_run = \"N\"\n",
    "loss_NN, pred_NN = predict_using_neutral_network(X_train, Y_train, X_target, Y_target, f\"{'_'.join(list(train_field))}_model_predict_{target_value}_2.pt\", super_param, re_run)\n",
    "print(f\"{loss_NN=}\")\n",
    "print(f\"{pred_NN=}\")\n",
    "loss_RF, pred_RF = predict_using_random_forest(X_train, Y_train, X_target, Y_target, super_param)\n",
    "print(f\"{loss_RF=}\")\n",
    "print(f\"{pred_RF=}\")\n",
    "loss_DT, pred_DT = predict_using_decision_tree(X_train, Y_train, X_target, Y_target, super_param)\n",
    "print(f\"{loss_DT=}\")\n",
    "print(f\"{pred_DT=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | loss train: 6503.40087890625\n",
      "Epoch: 100 | loss train: 6276.53271484375\n",
      "Epoch: 200 | loss train: 677.7886962890625\n",
      "Epoch: 300 | loss train: 666.8861083984375\n",
      "Epoch: 400 | loss train: 665.167236328125\n",
      "Epoch: 500 | loss train: 661.84619140625\n",
      "Epoch: 600 | loss train: 654.8848876953125\n",
      "Epoch: 700 | loss train: 639.7025756835938\n",
      "Epoch: 800 | loss train: 616.78369140625\n",
      "Epoch: 900 | loss train: 588.9366455078125\n",
      "Epoch: 1000 | loss train: 560.0297241210938\n",
      "Epoch: 1100 | loss train: 533.3748168945312\n",
      "Epoch: 1200 | loss train: 498.17816162109375\n",
      "Epoch: 1300 | loss train: 451.0477294921875\n",
      "Epoch: 1400 | loss train: 381.18450927734375\n",
      "Epoch: 1500 | loss train: 319.8168640136719\n",
      "Epoch: 1600 | loss train: 276.1628112792969\n",
      "Epoch: 1700 | loss train: 243.33944702148438\n",
      "Epoch: 1800 | loss train: 218.06478881835938\n",
      "Epoch: 1900 | loss train: 199.37164306640625\n",
      "Epoch: 2000 | loss train: 184.5328826904297\n",
      "Epoch: 2100 | loss train: 173.39894104003906\n",
      "Epoch: 2200 | loss train: 162.86688232421875\n",
      "Epoch: 2300 | loss train: 153.92420959472656\n",
      "Epoch: 2400 | loss train: 145.93252563476562\n",
      "Epoch: 2500 | loss train: 138.65664672851562\n",
      "Epoch: 2600 | loss train: 132.87918090820312\n",
      "Epoch: 2700 | loss train: 127.21375274658203\n",
      "Epoch: 2800 | loss train: 120.58660125732422\n",
      "Epoch: 2900 | loss train: 114.15029907226562\n",
      "Epoch: 3000 | loss train: 108.2554702758789\n",
      "Epoch: 3100 | loss train: 103.3200454711914\n",
      "Epoch: 3200 | loss train: 98.46672821044922\n",
      "Epoch: 3300 | loss train: 95.96139526367188\n",
      "Epoch: 3400 | loss train: 89.43186950683594\n",
      "Epoch: 3500 | loss train: 85.38786315917969\n",
      "Epoch: 3600 | loss train: 81.54097747802734\n",
      "Epoch: 3700 | loss train: 77.88011169433594\n",
      "Epoch: 3800 | loss train: 74.56906127929688\n",
      "Epoch: 3900 | loss train: 73.4908218383789\n",
      "Epoch: 4000 | loss train: 68.3448715209961\n",
      "Epoch: 4100 | loss train: 65.4338607788086\n",
      "Epoch: 4200 | loss train: 63.85430908203125\n",
      "Epoch: 4300 | loss train: 60.76932907104492\n",
      "Epoch: 4400 | loss train: 60.20155334472656\n",
      "Epoch: 4500 | loss train: 60.50728988647461\n",
      "Epoch: 4600 | loss train: 55.01728057861328\n",
      "Epoch: 4700 | loss train: 50.84933090209961\n",
      "Epoch: 4800 | loss train: 48.50482940673828\n",
      "Epoch: 4900 | loss train: 46.82575988769531\n",
      "Epoch: 5000 | loss train: 44.818904876708984\n",
      "Epoch: 5100 | loss train: 43.12460708618164\n",
      "Epoch: 5200 | loss train: 41.58323287963867\n",
      "Epoch: 5300 | loss train: 40.12922668457031\n",
      "Epoch: 5400 | loss train: 38.67223358154297\n",
      "Epoch: 5500 | loss train: 44.57282638549805\n",
      "Epoch: 5600 | loss train: 35.87847137451172\n",
      "Epoch: 5700 | loss train: 34.53840637207031\n",
      "Epoch: 5800 | loss train: 33.58458709716797\n",
      "Epoch: 5900 | loss train: 32.254722595214844\n",
      "Epoch: 6000 | loss train: 31.1378116607666\n",
      "Epoch: 6100 | loss train: 30.014493942260742\n",
      "Epoch: 6200 | loss train: 29.117101669311523\n",
      "Epoch: 6300 | loss train: 28.15918731689453\n",
      "Epoch: 6400 | loss train: 27.22928810119629\n",
      "Epoch: 6500 | loss train: 26.28900146484375\n",
      "Epoch: 6600 | loss train: 26.979393005371094\n",
      "Epoch: 6700 | loss train: 24.7064266204834\n",
      "Epoch: 6800 | loss train: 23.93309783935547\n",
      "Epoch: 6900 | loss train: 23.153621673583984\n",
      "Epoch: 7000 | loss train: 22.954641342163086\n",
      "Epoch: 7100 | loss train: 23.719606399536133\n",
      "Epoch: 7200 | loss train: 20.9952392578125\n",
      "Epoch: 7300 | loss train: 20.346813201904297\n",
      "Epoch: 7400 | loss train: 19.70899200439453\n",
      "Epoch: 7500 | loss train: 39.65611267089844\n",
      "Epoch: 7600 | loss train: 18.553630828857422\n",
      "Epoch: 7700 | loss train: 18.003002166748047\n",
      "Epoch: 7800 | loss train: 17.451501846313477\n",
      "Epoch: 7900 | loss train: 17.069002151489258\n",
      "Epoch: 8000 | loss train: 16.44524383544922\n",
      "Epoch: 8100 | loss train: 17.996843338012695\n",
      "Epoch: 8200 | loss train: 15.527573585510254\n",
      "Epoch: 8300 | loss train: 15.07209587097168\n",
      "Epoch: 8400 | loss train: 15.596760749816895\n",
      "Epoch: 8500 | loss train: 14.248231887817383\n",
      "Epoch: 8600 | loss train: 82.55036163330078\n",
      "Epoch: 8700 | loss train: 13.473687171936035\n",
      "Epoch: 8800 | loss train: 13.088345527648926\n",
      "Epoch: 8900 | loss train: 13.159281730651855\n",
      "Epoch: 9000 | loss train: 12.393767356872559\n",
      "Epoch: 9100 | loss train: 32.61915588378906\n",
      "Epoch: 9200 | loss train: 11.751798629760742\n",
      "Epoch: 9300 | loss train: 11.42750358581543\n",
      "Epoch: 9400 | loss train: 11.86850643157959\n",
      "Epoch: 9500 | loss train: 10.840471267700195\n",
      "Epoch: 9600 | loss train: 20.850322723388672\n",
      "Epoch: 9700 | loss train: 10.28416919708252\n",
      "Epoch: 9800 | loss train: 10.004841804504395\n",
      "Epoch: 9900 | loss train: 9.836942672729492\n",
      "Epoch: 10000 | loss train: 9.512490272521973\n",
      "Epoch: 10100 | loss train: 57.36578369140625\n",
      "Epoch: 10200 | loss train: 9.057405471801758\n",
      "Epoch: 10300 | loss train: 8.802474021911621\n",
      "Epoch: 10400 | loss train: 9.291916847229004\n",
      "Epoch: 10500 | loss train: 8.376612663269043\n",
      "Epoch: 10600 | loss train: 43.01997375488281\n",
      "Epoch: 10700 | loss train: 7.990170001983643\n",
      "Epoch: 10800 | loss train: 7.7624125480651855\n",
      "Epoch: 10900 | loss train: 7.948184013366699\n",
      "Epoch: 11000 | loss train: 7.386735439300537\n",
      "Epoch: 11100 | loss train: 9.377824783325195\n",
      "Epoch: 11200 | loss train: 7.036090850830078\n",
      "Epoch: 11300 | loss train: 48.05118942260742\n",
      "Epoch: 11400 | loss train: 6.703653812408447\n",
      "Epoch: 11500 | loss train: 37.637142181396484\n",
      "Epoch: 11600 | loss train: 6.393006801605225\n",
      "Epoch: 11700 | loss train: 6.261695384979248\n",
      "Epoch: 11800 | loss train: 6.131961822509766\n",
      "Epoch: 11900 | loss train: 5.929281711578369\n",
      "Epoch: 12000 | loss train: 6.4664788246154785\n",
      "Epoch: 12100 | loss train: 5.660398960113525\n",
      "Epoch: 12200 | loss train: 75.80059814453125\n",
      "Epoch: 12300 | loss train: 5.422494888305664\n",
      "Epoch: 12400 | loss train: 5.284858703613281\n",
      "Epoch: 12500 | loss train: 5.303684234619141\n",
      "Epoch: 12600 | loss train: 5.025943279266357\n",
      "Epoch: 12700 | loss train: 6.685428619384766\n",
      "Epoch: 12800 | loss train: 4.796029090881348\n",
      "Epoch: 12900 | loss train: 7.066349983215332\n",
      "Epoch: 13000 | loss train: 4.580548286437988\n",
      "Epoch: 13100 | loss train: 10.532890319824219\n",
      "Epoch: 13200 | loss train: 4.375865459442139\n",
      "Epoch: 13300 | loss train: 12.712381362915039\n",
      "Epoch: 13400 | loss train: 4.1837239265441895\n",
      "Epoch: 13500 | loss train: 11.489823341369629\n",
      "Epoch: 13600 | loss train: 4.01427698135376\n",
      "Epoch: 13700 | loss train: 3.901895523071289\n",
      "Epoch: 13800 | loss train: 6.079573154449463\n",
      "Epoch: 13900 | loss train: 3.730013132095337\n",
      "Epoch: 14000 | loss train: 4.029277324676514\n",
      "Epoch: 14100 | loss train: 3.5667946338653564\n",
      "Epoch: 14200 | loss train: 19.166149139404297\n",
      "Epoch: 14300 | loss train: 3.420466899871826\n",
      "Epoch: 14400 | loss train: 3.3369972705841064\n",
      "Epoch: 14500 | loss train: 4.910525321960449\n",
      "Epoch: 14600 | loss train: 3.191718578338623\n",
      "Epoch: 14700 | loss train: 4.139227867126465\n",
      "Epoch: 14800 | loss train: 3.052290916442871\n",
      "Epoch: 14900 | loss train: 5.5106401443481445\n",
      "Epoch: 15000 | loss train: 2.9230899810791016\n",
      "Epoch: 15100 | loss train: 18.701786041259766\n",
      "Epoch: 15200 | loss train: 2.805016279220581\n",
      "Epoch: 15300 | loss train: 2.7566003799438477\n",
      "Epoch: 15400 | loss train: 2.772305965423584\n",
      "Epoch: 15500 | loss train: 2.6221957206726074\n",
      "Epoch: 15600 | loss train: 3.17897891998291\n",
      "Epoch: 15700 | loss train: 2.5177149772644043\n",
      "Epoch: 15800 | loss train: 2.4544708728790283\n",
      "Epoch: 15900 | loss train: 3.5360147953033447\n",
      "Epoch: 16000 | loss train: 2.3570914268493652\n",
      "Epoch: 16100 | loss train: 15.840167999267578\n",
      "Epoch: 16200 | loss train: 2.262685537338257\n",
      "Epoch: 16300 | loss train: 2.208122968673706\n",
      "Epoch: 16400 | loss train: 2.8919482231140137\n",
      "Epoch: 16500 | loss train: 2.1311488151550293\n",
      "Epoch: 16600 | loss train: 2.0796432495117188\n",
      "Epoch: 16700 | loss train: 15.075709342956543\n",
      "Epoch: 16800 | loss train: 1.9996438026428223\n",
      "Epoch: 16900 | loss train: 1.977242350578308\n",
      "Epoch: 17000 | loss train: 2.042680263519287\n",
      "Epoch: 17100 | loss train: 1.8740674257278442\n",
      "Epoch: 17200 | loss train: 33.02880096435547\n",
      "Epoch: 17300 | loss train: 1.8156180381774902\n",
      "Epoch: 17400 | loss train: 28.370668411254883\n",
      "Epoch: 17500 | loss train: 1.755867838859558\n",
      "Epoch: 17600 | loss train: 57.26001739501953\n",
      "Epoch: 17700 | loss train: 1.7037475109100342\n",
      "Epoch: 17800 | loss train: 1.6878443956375122\n",
      "Epoch: 17900 | loss train: 2.1545705795288086\n",
      "Epoch: 18000 | loss train: 1.5558518171310425\n",
      "Epoch: 18100 | loss train: 5.103060245513916\n",
      "Epoch: 18200 | loss train: 1.5021495819091797\n",
      "Epoch: 18300 | loss train: 20.426700592041016\n",
      "Epoch: 18400 | loss train: 1.5520967245101929\n",
      "Epoch: 18500 | loss train: 6.80248498916626\n",
      "Epoch: 18600 | loss train: 1.5965733528137207\n",
      "Epoch: 18700 | loss train: 1.3633328676223755\n",
      "Epoch: 18800 | loss train: 2.080751419067383\n",
      "Epoch: 18900 | loss train: 1.3050048351287842\n",
      "Epoch: 19000 | loss train: 25.89238166809082\n",
      "Epoch: 19100 | loss train: 1.2733272314071655\n",
      "Epoch: 19200 | loss train: 1.2325246334075928\n",
      "Epoch: 19300 | loss train: 8.787557601928711\n",
      "Epoch: 19400 | loss train: 1.1866793632507324\n",
      "Epoch: 19500 | loss train: 34.9088020324707\n",
      "Epoch: 19600 | loss train: 1.3164992332458496\n",
      "Epoch: 19700 | loss train: 1.121602177619934\n",
      "Epoch: 19800 | loss train: 39.32134246826172\n",
      "Epoch: 19900 | loss train: 1.1056287288665771\n",
      "Epoch: 20000 | loss train: 1.0578340291976929\n",
      "Epoch: 20100 | loss train: 38.4783821105957\n",
      "Epoch: 20200 | loss train: 1.0955408811569214\n",
      "Epoch: 20300 | loss train: 1.0081803798675537\n",
      "Epoch: 20400 | loss train: 2.525852918624878\n",
      "Epoch: 20500 | loss train: 0.96772301197052\n",
      "Epoch: 20600 | loss train: 3.700593948364258\n",
      "Epoch: 20700 | loss train: 0.9329128265380859\n",
      "Epoch: 20800 | loss train: 2.292527675628662\n",
      "Epoch: 20900 | loss train: 0.8991280794143677\n",
      "Epoch: 21000 | loss train: 21.281953811645508\n",
      "Epoch: 21100 | loss train: 0.8902579545974731\n",
      "Epoch: 21200 | loss train: 0.8559442162513733\n",
      "Epoch: 21300 | loss train: 34.258697509765625\n",
      "Epoch: 21400 | loss train: 0.8800588846206665\n",
      "Epoch: 21500 | loss train: 0.8088290095329285\n",
      "Epoch: 21600 | loss train: 6.874974727630615\n",
      "Epoch: 21700 | loss train: 0.7925892472267151\n",
      "Epoch: 21800 | loss train: 0.7755305767059326\n",
      "Epoch: 21900 | loss train: 1.1322695016860962\n",
      "Epoch: 22000 | loss train: 0.7561437487602234\n",
      "Epoch: 22100 | loss train: 0.8486259579658508\n",
      "Epoch: 22200 | loss train: 1.38227117061615\n",
      "Epoch: 22300 | loss train: 0.7237117290496826\n",
      "Epoch: 22400 | loss train: 0.9114146828651428\n",
      "Epoch: 22500 | loss train: 0.6901873350143433\n",
      "Epoch: 22600 | loss train: 5.430115699768066\n",
      "Epoch: 22700 | loss train: 0.6725577116012573\n",
      "Epoch: 22800 | loss train: 5.112247943878174\n",
      "Epoch: 22900 | loss train: 0.6602993011474609\n",
      "Epoch: 23000 | loss train: 24.728740692138672\n",
      "Epoch: 23100 | loss train: 0.6565811634063721\n",
      "Epoch: 23200 | loss train: 81.83805847167969\n",
      "Epoch: 23300 | loss train: 0.6660880446434021\n",
      "Epoch: 23400 | loss train: 0.6015943884849548\n",
      "Epoch: 23500 | loss train: 1.746245265007019\n",
      "Epoch: 23600 | loss train: 0.5869951844215393\n",
      "Epoch: 23700 | loss train: 6.495604515075684\n",
      "Epoch: 23800 | loss train: 0.5726781487464905\n",
      "Epoch: 23900 | loss train: 57.649505615234375\n",
      "Epoch: 24000 | loss train: 0.6264272332191467\n",
      "Epoch: 24100 | loss train: 0.5420414805412292\n",
      "Epoch: 24200 | loss train: 0.5232445597648621\n",
      "Epoch: 24300 | loss train: 1.1284747123718262\n",
      "Epoch: 24400 | loss train: 0.5679928660392761\n",
      "Epoch: 24500 | loss train: 0.5134080648422241\n",
      "Epoch: 24600 | loss train: 14.271867752075195\n",
      "Epoch: 24700 | loss train: 56.580142974853516\n",
      "Epoch: 24800 | loss train: 0.6774807572364807\n",
      "Epoch: 24900 | loss train: 7.427395820617676\n",
      "Epoch: 25000 | loss train: 0.49564889073371887\n",
      "loss_NN=tensor(1264.1978)\n",
      "pred_NN=tensor([[7200.6904],\n",
      "        [5995.5044],\n",
      "        [6137.1938],\n",
      "        [5086.9526]])\n",
      "loss_RF=682.2421852328629\n",
      "pred_RF=array([6426.14666992, 6384.76099609, 6228.64490723, 6066.27245117])\n",
      "loss_DT=340.1831284567497\n",
      "pred_DT=array([5609.47509766, 7439.69628906, 5679.90380859, 6474.81933594])\n"
     ]
    }
   ],
   "source": [
    "target_value = \"P\"\n",
    "train_field = \"J\"\n",
    "sample = generate_sample(data_df_13_09_2022, target_value, train_field)\n",
    "X_train, Y_train = create_X_train_Y_train(sample)\n",
    "sample_target = generate_sample(data_df_13_09_2022, target_value, \"BC\")\n",
    "X_target, Y_target = create_X_train_Y_train(sample_target)\n",
    "super_param={\"lr\": 0.0001, \"weight_decay\": 1e-5, \"n_epochs\": 40000, \"stop_value\": 0.5}\n",
    "re_run = \"N\"\n",
    "loss_NN, pred_NN = predict_using_neutral_network(X_train, Y_train, X_target, Y_target, f\"{'_'.join(list(train_field))}_model_predict_{target_value}_2.pt\", super_param, re_run)\n",
    "print(f\"{loss_NN=}\")\n",
    "print(f\"{pred_NN=}\")\n",
    "loss_RF, pred_RF = predict_using_random_forest(X_train, Y_train, X_target, Y_target, super_param)\n",
    "print(f\"{loss_RF=}\")\n",
    "print(f\"{pred_RF=}\")\n",
    "loss_DT, pred_DT = predict_using_decision_tree(X_train, Y_train, X_target, Y_target, super_param)\n",
    "print(f\"{loss_DT=}\")\n",
    "print(f\"{pred_DT=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | loss train: 6252.166015625\n",
      "Epoch: 100 | loss train: 6154.83056640625\n",
      "Epoch: 200 | loss train: 732.2145385742188\n",
      "Epoch: 300 | loss train: 728.9524536132812\n",
      "Epoch: 400 | loss train: 727.1348876953125\n",
      "Epoch: 500 | loss train: 724.9122314453125\n",
      "Epoch: 600 | loss train: 722.19482421875\n",
      "Epoch: 700 | loss train: 718.915771484375\n",
      "Epoch: 800 | loss train: 714.572509765625\n",
      "Epoch: 900 | loss train: 708.0503540039062\n",
      "Epoch: 1000 | loss train: 692.5648193359375\n",
      "Epoch: 1100 | loss train: 670.03369140625\n",
      "Epoch: 1200 | loss train: 649.7552490234375\n",
      "Epoch: 1300 | loss train: 633.380859375\n",
      "Epoch: 1400 | loss train: 609.5718994140625\n",
      "Epoch: 1500 | loss train: 572.3823852539062\n",
      "Epoch: 1600 | loss train: 515.6957397460938\n",
      "Epoch: 1700 | loss train: 458.8998107910156\n",
      "Epoch: 1800 | loss train: 413.69305419921875\n",
      "Epoch: 1900 | loss train: 387.0946960449219\n",
      "Epoch: 2000 | loss train: 356.6089782714844\n",
      "Epoch: 2100 | loss train: 338.2781066894531\n",
      "Epoch: 2200 | loss train: 318.5914001464844\n",
      "Epoch: 2300 | loss train: 304.62152099609375\n",
      "Epoch: 2400 | loss train: 294.03350830078125\n",
      "Epoch: 2500 | loss train: 287.90789794921875\n",
      "Epoch: 2600 | loss train: 277.033935546875\n",
      "Epoch: 2700 | loss train: 270.8095703125\n",
      "Epoch: 2800 | loss train: 267.04229736328125\n",
      "Epoch: 2900 | loss train: 262.49188232421875\n",
      "Epoch: 3000 | loss train: 257.24700927734375\n",
      "Epoch: 3100 | loss train: 251.7549591064453\n",
      "Epoch: 3200 | loss train: 248.288330078125\n",
      "Epoch: 3300 | loss train: 245.90333557128906\n",
      "Epoch: 3400 | loss train: 241.3889617919922\n",
      "Epoch: 3500 | loss train: 238.5270538330078\n",
      "Epoch: 3600 | loss train: 235.6173095703125\n",
      "Epoch: 3700 | loss train: 233.1697998046875\n",
      "Epoch: 3800 | loss train: 230.88937377929688\n",
      "Epoch: 3900 | loss train: 230.19973754882812\n",
      "Epoch: 4000 | loss train: 229.70904541015625\n",
      "Epoch: 4100 | loss train: 223.4120330810547\n",
      "Epoch: 4200 | loss train: 223.1733856201172\n",
      "Epoch: 4300 | loss train: 219.19818115234375\n",
      "Epoch: 4400 | loss train: 217.04287719726562\n",
      "Epoch: 4500 | loss train: 219.8592987060547\n",
      "Epoch: 4600 | loss train: 212.74508666992188\n",
      "Epoch: 4700 | loss train: 211.3408966064453\n",
      "Epoch: 4800 | loss train: 210.678466796875\n",
      "Epoch: 4900 | loss train: 207.94158935546875\n",
      "Epoch: 5000 | loss train: 208.53643798828125\n",
      "Epoch: 5100 | loss train: 203.0051727294922\n",
      "Epoch: 5200 | loss train: 206.8227081298828\n",
      "Epoch: 5300 | loss train: 200.37274169921875\n",
      "Epoch: 5400 | loss train: 199.63284301757812\n",
      "Epoch: 5500 | loss train: 198.3219451904297\n",
      "Epoch: 5600 | loss train: 194.36012268066406\n",
      "Epoch: 5700 | loss train: 196.05616760253906\n",
      "Epoch: 5800 | loss train: 197.86781311035156\n",
      "Epoch: 5900 | loss train: 194.4832000732422\n",
      "Epoch: 6000 | loss train: 188.868408203125\n",
      "Epoch: 6100 | loss train: 186.31724548339844\n",
      "Epoch: 6200 | loss train: 187.43695068359375\n",
      "Epoch: 6300 | loss train: 184.3780975341797\n",
      "Epoch: 6400 | loss train: 183.0242462158203\n",
      "Epoch: 6500 | loss train: 183.73834228515625\n",
      "Epoch: 6600 | loss train: 180.07257080078125\n",
      "Epoch: 6700 | loss train: 177.47836303710938\n",
      "Epoch: 6800 | loss train: 176.96669006347656\n",
      "Epoch: 6900 | loss train: 184.364013671875\n",
      "Epoch: 7000 | loss train: 179.32821655273438\n",
      "Epoch: 7100 | loss train: 173.84613037109375\n",
      "Epoch: 7200 | loss train: 170.56283569335938\n",
      "Epoch: 7300 | loss train: 174.21839904785156\n",
      "Epoch: 7400 | loss train: 169.16262817382812\n",
      "Epoch: 7500 | loss train: 170.15997314453125\n",
      "Epoch: 7600 | loss train: 167.40780639648438\n",
      "Epoch: 7700 | loss train: 163.9189910888672\n",
      "Epoch: 7800 | loss train: 166.91319274902344\n",
      "Epoch: 7900 | loss train: 162.16404724121094\n",
      "Epoch: 8000 | loss train: 164.48223876953125\n",
      "Epoch: 8100 | loss train: 162.8327178955078\n",
      "Epoch: 8200 | loss train: 158.35691833496094\n",
      "Epoch: 8300 | loss train: 156.60826110839844\n",
      "Epoch: 8400 | loss train: 158.1596221923828\n",
      "Epoch: 8500 | loss train: 155.13760375976562\n",
      "Epoch: 8600 | loss train: 152.53919982910156\n",
      "Epoch: 8700 | loss train: 150.3435821533203\n",
      "Epoch: 8800 | loss train: 153.47975158691406\n",
      "Epoch: 8900 | loss train: 147.61741638183594\n",
      "Epoch: 9000 | loss train: 153.05712890625\n",
      "Epoch: 9100 | loss train: 146.9884033203125\n",
      "Epoch: 9200 | loss train: 147.24659729003906\n",
      "Epoch: 9300 | loss train: 147.90525817871094\n",
      "Epoch: 9400 | loss train: 145.4634552001953\n",
      "Epoch: 9500 | loss train: 139.10574340820312\n",
      "Epoch: 9600 | loss train: 140.62144470214844\n",
      "Epoch: 9700 | loss train: 139.903564453125\n",
      "Epoch: 9800 | loss train: 134.87506103515625\n",
      "Epoch: 9900 | loss train: 141.69549560546875\n",
      "Epoch: 10000 | loss train: 133.39366149902344\n",
      "Epoch: 10100 | loss train: 130.78250122070312\n",
      "Epoch: 10200 | loss train: 129.43153381347656\n",
      "Epoch: 10300 | loss train: 133.5356903076172\n",
      "Epoch: 10400 | loss train: 127.60545349121094\n",
      "Epoch: 10500 | loss train: 129.14370727539062\n",
      "Epoch: 10600 | loss train: 126.2599868774414\n",
      "Epoch: 10700 | loss train: 130.33824157714844\n",
      "Epoch: 10800 | loss train: 124.93791198730469\n",
      "Epoch: 10900 | loss train: 128.5883331298828\n",
      "Epoch: 11000 | loss train: 119.21907043457031\n",
      "Epoch: 11100 | loss train: 122.8803939819336\n",
      "Epoch: 11200 | loss train: 116.62005615234375\n",
      "Epoch: 11300 | loss train: 116.14278411865234\n",
      "Epoch: 11400 | loss train: 114.49845123291016\n",
      "Epoch: 11500 | loss train: 115.32943725585938\n",
      "Epoch: 11600 | loss train: 118.04818725585938\n",
      "Epoch: 11700 | loss train: 110.53955078125\n",
      "Epoch: 11800 | loss train: 115.53184509277344\n",
      "Epoch: 11900 | loss train: 106.79977416992188\n",
      "Epoch: 12000 | loss train: 107.34017181396484\n",
      "Epoch: 12100 | loss train: 106.27815246582031\n",
      "Epoch: 12200 | loss train: 111.30168151855469\n",
      "Epoch: 12300 | loss train: 115.37114715576172\n",
      "Epoch: 12400 | loss train: 102.52008819580078\n",
      "Epoch: 12500 | loss train: 107.18844604492188\n",
      "Epoch: 12600 | loss train: 99.904296875\n",
      "Epoch: 12700 | loss train: 102.59014892578125\n",
      "Epoch: 12800 | loss train: 97.60503387451172\n",
      "Epoch: 12900 | loss train: 124.15873718261719\n",
      "Epoch: 13000 | loss train: 95.37164306640625\n",
      "Epoch: 13100 | loss train: 104.49435424804688\n",
      "Epoch: 13200 | loss train: 95.2029037475586\n",
      "Epoch: 13300 | loss train: 98.0754623413086\n",
      "Epoch: 13400 | loss train: 90.3053207397461\n",
      "Epoch: 13500 | loss train: 98.44158935546875\n",
      "Epoch: 13600 | loss train: 89.32412719726562\n",
      "Epoch: 13700 | loss train: 90.41836547851562\n",
      "Epoch: 13800 | loss train: 107.89588165283203\n",
      "Epoch: 13900 | loss train: 86.32209014892578\n",
      "Epoch: 14000 | loss train: 92.36636352539062\n",
      "Epoch: 14100 | loss train: 93.26518249511719\n",
      "Epoch: 14200 | loss train: 93.57667541503906\n",
      "Epoch: 14300 | loss train: 82.15556335449219\n",
      "Epoch: 14400 | loss train: 78.54463195800781\n",
      "Epoch: 14500 | loss train: 77.3824234008789\n",
      "Epoch: 14600 | loss train: 81.73017883300781\n",
      "Epoch: 14700 | loss train: 80.49122619628906\n",
      "Epoch: 14800 | loss train: 80.1964340209961\n",
      "Epoch: 14900 | loss train: 88.30667114257812\n",
      "Epoch: 15000 | loss train: 110.95463562011719\n",
      "Epoch: 15100 | loss train: 74.8033447265625\n",
      "Epoch: 15200 | loss train: 71.83673858642578\n",
      "Epoch: 15300 | loss train: 69.95037078857422\n",
      "Epoch: 15400 | loss train: 72.25273132324219\n",
      "Epoch: 15500 | loss train: 65.41984558105469\n",
      "Epoch: 15600 | loss train: 69.17071533203125\n",
      "Epoch: 15700 | loss train: 65.19542694091797\n",
      "Epoch: 15800 | loss train: 67.86768341064453\n",
      "Epoch: 15900 | loss train: 62.205848693847656\n",
      "Epoch: 16000 | loss train: 63.54288864135742\n",
      "Epoch: 16100 | loss train: 67.69233703613281\n",
      "Epoch: 16200 | loss train: 94.71105194091797\n",
      "Epoch: 16300 | loss train: 61.40673065185547\n",
      "Epoch: 16400 | loss train: 74.3742904663086\n",
      "Epoch: 16500 | loss train: 56.95555114746094\n",
      "Epoch: 16600 | loss train: 57.604286193847656\n",
      "Epoch: 16700 | loss train: 61.06019973754883\n",
      "Epoch: 16800 | loss train: 52.617095947265625\n",
      "Epoch: 16900 | loss train: 63.66263961791992\n",
      "Epoch: 17000 | loss train: 58.98655700683594\n",
      "Epoch: 17100 | loss train: 50.545066833496094\n",
      "Epoch: 17200 | loss train: 66.3133544921875\n",
      "Epoch: 17300 | loss train: 56.96647262573242\n",
      "Epoch: 17400 | loss train: 47.1507682800293\n",
      "Epoch: 17500 | loss train: 67.4164810180664\n",
      "Epoch: 17600 | loss train: 50.42294692993164\n",
      "Epoch: 17700 | loss train: 50.892337799072266\n",
      "Epoch: 17800 | loss train: 44.6794548034668\n",
      "Epoch: 17900 | loss train: 83.53052520751953\n",
      "Epoch: 18000 | loss train: 48.17335891723633\n",
      "Epoch: 18100 | loss train: 43.014461517333984\n",
      "Epoch: 18200 | loss train: 42.19144058227539\n",
      "Epoch: 18300 | loss train: 58.42659378051758\n",
      "Epoch: 18400 | loss train: 58.135677337646484\n",
      "Epoch: 18500 | loss train: 45.808353424072266\n",
      "Epoch: 18600 | loss train: 42.3335075378418\n",
      "Epoch: 18700 | loss train: 39.0851936340332\n",
      "Epoch: 18800 | loss train: 42.75858688354492\n",
      "Epoch: 18900 | loss train: 50.78713607788086\n",
      "Epoch: 19000 | loss train: 53.3455696105957\n",
      "Epoch: 19100 | loss train: 37.06465530395508\n",
      "Epoch: 19200 | loss train: 43.2403450012207\n",
      "Epoch: 19300 | loss train: 43.40141677856445\n",
      "Epoch: 19400 | loss train: 44.777339935302734\n",
      "Epoch: 19500 | loss train: 95.13484191894531\n",
      "Epoch: 19600 | loss train: 57.50538635253906\n",
      "Epoch: 19700 | loss train: 40.82884216308594\n",
      "Epoch: 19800 | loss train: 42.59847640991211\n",
      "Epoch: 19900 | loss train: 79.34751892089844\n",
      "Epoch: 20000 | loss train: 85.59117126464844\n",
      "Epoch: 20100 | loss train: 35.02920913696289\n",
      "Epoch: 20200 | loss train: 58.46832275390625\n",
      "Epoch: 20300 | loss train: 38.6288948059082\n",
      "Epoch: 20400 | loss train: 36.500389099121094\n",
      "Epoch: 20500 | loss train: 31.17428207397461\n",
      "Epoch: 20600 | loss train: 30.861764907836914\n",
      "Epoch: 20700 | loss train: 60.52744674682617\n",
      "Epoch: 20800 | loss train: 33.160640716552734\n",
      "Epoch: 20900 | loss train: 31.67465591430664\n",
      "Epoch: 21000 | loss train: 58.967044830322266\n",
      "Epoch: 21100 | loss train: 46.917320251464844\n",
      "Epoch: 21200 | loss train: 103.4590835571289\n",
      "Epoch: 21300 | loss train: 28.419620513916016\n",
      "Epoch: 21400 | loss train: 28.95583152770996\n",
      "Epoch: 21500 | loss train: 38.18376541137695\n",
      "Epoch: 21600 | loss train: 37.9450569152832\n",
      "Epoch: 21700 | loss train: 37.94480514526367\n",
      "Epoch: 21800 | loss train: 28.375606536865234\n",
      "Epoch: 21900 | loss train: 47.09952926635742\n",
      "Epoch: 22000 | loss train: 32.239463806152344\n",
      "Epoch: 22100 | loss train: 50.60795974731445\n",
      "Epoch: 22200 | loss train: 66.37448120117188\n",
      "Epoch: 22300 | loss train: 42.016780853271484\n",
      "Epoch: 22400 | loss train: 41.02265167236328\n",
      "Epoch: 22500 | loss train: 48.18008804321289\n",
      "Epoch: 22600 | loss train: 48.58677291870117\n",
      "Epoch: 22700 | loss train: 40.56886291503906\n",
      "Epoch: 22800 | loss train: 25.091171264648438\n",
      "Epoch: 22900 | loss train: 53.13690185546875\n",
      "Epoch: 23000 | loss train: 23.884035110473633\n",
      "Epoch: 23100 | loss train: 35.170169830322266\n",
      "Epoch: 23200 | loss train: 23.124774932861328\n",
      "Epoch: 23300 | loss train: 22.34142303466797\n",
      "Epoch: 23400 | loss train: 25.23775291442871\n",
      "Epoch: 23500 | loss train: 31.184757232666016\n",
      "Epoch: 23600 | loss train: 28.020036697387695\n",
      "Epoch: 23700 | loss train: 63.51830291748047\n",
      "Epoch: 23800 | loss train: 49.255348205566406\n",
      "Epoch: 23900 | loss train: 32.54161071777344\n",
      "Epoch: 24000 | loss train: 29.2517032623291\n",
      "Epoch: 24100 | loss train: 21.387609481811523\n",
      "Epoch: 24200 | loss train: 28.758699417114258\n",
      "Epoch: 24300 | loss train: 40.40394973754883\n",
      "Epoch: 24400 | loss train: 21.561494827270508\n",
      "Epoch: 24500 | loss train: 23.160184860229492\n",
      "Epoch: 24600 | loss train: 32.28493881225586\n",
      "Epoch: 24700 | loss train: 42.18690872192383\n",
      "Epoch: 24800 | loss train: 35.45607376098633\n",
      "Epoch: 24900 | loss train: 28.382585525512695\n",
      "Epoch: 25000 | loss train: 24.036231994628906\n",
      "Epoch: 25100 | loss train: 19.641887664794922\n",
      "Epoch: 25200 | loss train: 25.037553787231445\n",
      "Epoch: 25300 | loss train: 19.138349533081055\n",
      "Epoch: 25400 | loss train: 29.119035720825195\n",
      "Epoch: 25500 | loss train: 21.335527420043945\n",
      "Epoch: 25600 | loss train: 22.94143295288086\n",
      "Epoch: 25700 | loss train: 22.251270294189453\n",
      "Epoch: 25800 | loss train: 25.160865783691406\n",
      "Epoch: 25900 | loss train: 108.82665252685547\n",
      "Epoch: 26000 | loss train: 16.802751541137695\n",
      "Epoch: 26100 | loss train: 23.87508773803711\n",
      "Epoch: 26200 | loss train: 27.231767654418945\n",
      "Epoch: 26300 | loss train: 62.03447341918945\n",
      "Epoch: 26400 | loss train: 16.578388214111328\n",
      "Epoch: 26500 | loss train: 37.13393783569336\n",
      "Epoch: 26600 | loss train: 16.97151756286621\n",
      "Epoch: 26700 | loss train: 17.76120948791504\n",
      "Epoch: 26800 | loss train: 23.171634674072266\n",
      "Epoch: 26900 | loss train: 30.09910774230957\n",
      "Epoch: 27000 | loss train: 36.63042068481445\n",
      "Epoch: 27100 | loss train: 22.678081512451172\n",
      "Epoch: 27200 | loss train: 47.24058532714844\n",
      "Epoch: 27300 | loss train: 30.89756965637207\n",
      "Epoch: 27400 | loss train: 18.346181869506836\n",
      "Epoch: 27500 | loss train: 18.895994186401367\n",
      "Epoch: 27600 | loss train: 15.019153594970703\n",
      "Epoch: 27700 | loss train: 14.705817222595215\n",
      "Epoch: 27800 | loss train: 35.617698669433594\n",
      "Epoch: 27900 | loss train: 17.683414459228516\n",
      "Epoch: 28000 | loss train: 20.926387786865234\n",
      "Epoch: 28100 | loss train: 31.461700439453125\n",
      "Epoch: 28200 | loss train: 47.71915054321289\n",
      "Epoch: 28300 | loss train: 14.408247947692871\n",
      "Epoch: 28400 | loss train: 50.762508392333984\n",
      "Epoch: 28500 | loss train: 17.83795166015625\n",
      "Epoch: 28600 | loss train: 42.738075256347656\n",
      "Epoch: 28700 | loss train: 58.822235107421875\n",
      "Epoch: 28800 | loss train: 13.040090560913086\n",
      "Epoch: 28900 | loss train: 12.938982009887695\n",
      "Epoch: 29000 | loss train: 12.877832412719727\n",
      "Epoch: 29100 | loss train: 31.781091690063477\n",
      "Epoch: 29200 | loss train: 28.115793228149414\n",
      "Epoch: 29300 | loss train: 20.417272567749023\n",
      "Epoch: 29400 | loss train: 12.679061889648438\n",
      "Epoch: 29500 | loss train: 77.56363677978516\n",
      "Epoch: 29600 | loss train: 25.8050594329834\n",
      "Epoch: 29700 | loss train: 72.07974243164062\n",
      "Epoch: 29800 | loss train: 22.684261322021484\n",
      "Epoch: 29900 | loss train: 12.777551651000977\n",
      "Epoch: 30000 | loss train: 19.44191551208496\n",
      "Epoch: 30100 | loss train: 22.268465042114258\n",
      "Epoch: 30200 | loss train: 18.45395278930664\n",
      "Epoch: 30300 | loss train: 21.61146354675293\n",
      "Epoch: 30400 | loss train: 16.778242111206055\n",
      "Epoch: 30500 | loss train: 24.968809127807617\n",
      "Epoch: 30600 | loss train: 32.715999603271484\n",
      "Epoch: 30700 | loss train: 11.343753814697266\n",
      "Epoch: 30800 | loss train: 19.74523162841797\n",
      "Epoch: 30900 | loss train: 15.171586036682129\n",
      "Epoch: 31000 | loss train: 22.440561294555664\n",
      "Epoch: 31100 | loss train: 12.88719367980957\n",
      "Epoch: 31200 | loss train: 10.769906044006348\n",
      "Epoch: 31300 | loss train: 13.67430591583252\n",
      "Epoch: 31400 | loss train: 13.118072509765625\n",
      "Epoch: 31500 | loss train: 38.879730224609375\n",
      "Epoch: 31600 | loss train: 24.324684143066406\n",
      "Epoch: 31700 | loss train: 10.41753101348877\n",
      "Epoch: 31800 | loss train: 22.17938804626465\n",
      "Epoch: 31900 | loss train: 11.028644561767578\n",
      "Epoch: 32000 | loss train: 10.157572746276855\n",
      "Epoch: 32100 | loss train: 10.283448219299316\n",
      "Epoch: 32200 | loss train: 99.56489562988281\n",
      "Epoch: 32300 | loss train: 14.980060577392578\n",
      "Epoch: 32400 | loss train: 15.310173034667969\n",
      "Epoch: 32500 | loss train: 11.48780632019043\n",
      "Epoch: 32600 | loss train: 10.791902542114258\n",
      "Epoch: 32700 | loss train: 19.396297454833984\n",
      "Epoch: 32800 | loss train: 9.739053726196289\n",
      "Epoch: 32900 | loss train: 9.614249229431152\n",
      "Epoch: 33000 | loss train: 16.34816551208496\n",
      "Epoch: 33100 | loss train: 9.432580947875977\n",
      "Epoch: 33200 | loss train: 25.256797790527344\n",
      "Epoch: 33300 | loss train: 31.010072708129883\n",
      "Epoch: 33400 | loss train: 31.336856842041016\n",
      "Epoch: 33500 | loss train: 55.49910354614258\n",
      "Epoch: 33600 | loss train: 9.225811958312988\n",
      "Epoch: 33700 | loss train: 82.32188415527344\n",
      "Epoch: 33800 | loss train: 8.939005851745605\n",
      "Epoch: 33900 | loss train: 17.70596694946289\n",
      "Epoch: 34000 | loss train: 10.690834999084473\n",
      "Epoch: 34100 | loss train: 36.79524612426758\n",
      "Epoch: 34200 | loss train: 8.646571159362793\n",
      "Epoch: 34300 | loss train: 9.219605445861816\n",
      "Epoch: 34400 | loss train: 18.096660614013672\n",
      "Epoch: 34500 | loss train: 9.657766342163086\n",
      "Epoch: 34600 | loss train: 14.051462173461914\n",
      "Epoch: 34700 | loss train: 8.48033332824707\n",
      "Epoch: 34800 | loss train: 8.640177726745605\n",
      "Epoch: 34900 | loss train: 8.364954948425293\n",
      "Epoch: 35000 | loss train: 9.77756404876709\n",
      "Epoch: 35100 | loss train: 10.52620792388916\n",
      "Epoch: 35200 | loss train: 9.363139152526855\n",
      "Epoch: 35300 | loss train: 20.045955657958984\n",
      "Epoch: 35400 | loss train: 28.197397232055664\n",
      "Epoch: 35500 | loss train: 7.947756290435791\n",
      "Epoch: 35600 | loss train: 68.2800521850586\n",
      "Epoch: 35700 | loss train: 7.815476894378662\n",
      "Epoch: 35800 | loss train: 7.9798126220703125\n",
      "Epoch: 35900 | loss train: 95.55097198486328\n",
      "Epoch: 36000 | loss train: 108.8390121459961\n",
      "Epoch: 36100 | loss train: 10.734770774841309\n",
      "Epoch: 36200 | loss train: 13.931946754455566\n",
      "Epoch: 36300 | loss train: 7.607666492462158\n",
      "Epoch: 36400 | loss train: 30.44891357421875\n",
      "Epoch: 36500 | loss train: 12.11300277709961\n",
      "Epoch: 36600 | loss train: 80.97483825683594\n",
      "Epoch: 36700 | loss train: 7.448409080505371\n",
      "Epoch: 36800 | loss train: 70.78020477294922\n",
      "Epoch: 36900 | loss train: 80.9392318725586\n",
      "Epoch: 37000 | loss train: 19.17400550842285\n",
      "Epoch: 37100 | loss train: 65.69564056396484\n",
      "Epoch: 37200 | loss train: 7.123351573944092\n",
      "Epoch: 37300 | loss train: 8.37290096282959\n",
      "Epoch: 37400 | loss train: 7.016773223876953\n",
      "Epoch: 37500 | loss train: 17.20525550842285\n",
      "Epoch: 37600 | loss train: 7.039506435394287\n",
      "Epoch: 37700 | loss train: 7.396024703979492\n",
      "Epoch: 37800 | loss train: 29.422306060791016\n",
      "Epoch: 37900 | loss train: 146.9452667236328\n",
      "Epoch: 38000 | loss train: 6.9371514320373535\n",
      "Epoch: 38100 | loss train: 86.61430358886719\n",
      "Epoch: 38200 | loss train: 6.890850067138672\n",
      "Epoch: 38300 | loss train: 6.746417045593262\n",
      "Epoch: 38400 | loss train: 11.405440330505371\n",
      "Epoch: 38500 | loss train: 11.567052841186523\n",
      "Epoch: 38600 | loss train: 10.34449577331543\n",
      "Epoch: 38700 | loss train: 71.4052734375\n",
      "Epoch: 38800 | loss train: 6.59973669052124\n",
      "Epoch: 38900 | loss train: 6.642913818359375\n",
      "Epoch: 39000 | loss train: 11.407333374023438\n",
      "Epoch: 39100 | loss train: 7.054587364196777\n",
      "Epoch: 39200 | loss train: 11.084114074707031\n",
      "Epoch: 39300 | loss train: 16.006155014038086\n",
      "Epoch: 39400 | loss train: 30.0150203704834\n",
      "Epoch: 39500 | loss train: 6.18953800201416\n",
      "Epoch: 39600 | loss train: 33.96858215332031\n",
      "Epoch: 39700 | loss train: 6.184817790985107\n",
      "Epoch: 39800 | loss train: 18.222314834594727\n",
      "Epoch: 39900 | loss train: 6.68890380859375\n",
      "loss_NN=tensor(1464.6201)\n",
      "pred_NN=tensor([[7157.1050],\n",
      "        [5722.8149],\n",
      "        [5361.9702],\n",
      "        [4908.5347]])\n",
      "loss_RF=755.294086952265\n",
      "pred_RF=array([6350.49380371, 6175.68631348, 6052.60022461, 6128.08745605])\n",
      "loss_DT=1348.8788985854317\n",
      "pred_DT=array([6166.06738281, 5569.74902344, 4766.24169922, 5546.46972656])\n"
     ]
    }
   ],
   "source": [
    "target_value = \"P\"\n",
    "train_field = [\"T\", \"J\"]\n",
    "sample = generate_sample(data_df_13_09_2022, target_value, train_field)\n",
    "X_train, Y_train = create_X_train_Y_train(sample)\n",
    "sample_target = generate_sample(data_df_13_09_2022, target_value, \"BC\")\n",
    "X_target, Y_target = create_X_train_Y_train(sample_target)\n",
    "super_param={\"lr\": 0.0001, \"weight_decay\": 1e-5, \"n_epochs\": 40000, \"stop_value\": 0.5}\n",
    "re_run = \"N\"\n",
    "loss_NN, pred_NN = predict_using_neutral_network(X_train, Y_train, X_target, Y_target, f\"{'_'.join(list(train_field))}_model_predict_{target_value}_2.pt\", super_param, re_run)\n",
    "print(f\"{loss_NN=}\")\n",
    "print(f\"{pred_NN=}\")\n",
    "loss_RF, pred_RF = predict_using_random_forest(X_train, Y_train, X_target, Y_target, super_param)\n",
    "print(f\"{loss_RF=}\")\n",
    "print(f\"{pred_RF=}\")\n",
    "loss_DT, pred_DT = predict_using_decision_tree(X_train, Y_train, X_target, Y_target, super_param)\n",
    "print(f\"{loss_DT=}\")\n",
    "print(f\"{pred_DT=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dự đoán Kali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | loss train: 14027.763671875\n",
      "Epoch: 100 | loss train: 13909.31640625\n",
      "Epoch: 200 | loss train: 2374.192138671875\n",
      "Epoch: 300 | loss train: 2273.330810546875\n",
      "Epoch: 400 | loss train: 2264.25341796875\n",
      "Epoch: 500 | loss train: 2250.5673828125\n",
      "Epoch: 600 | loss train: 2230.7548828125\n",
      "Epoch: 700 | loss train: 2208.5166015625\n",
      "Epoch: 800 | loss train: 2183.508056640625\n",
      "Epoch: 900 | loss train: 2148.603271484375\n",
      "Epoch: 1000 | loss train: 2102.12939453125\n",
      "Epoch: 1100 | loss train: 2038.3936767578125\n",
      "Epoch: 1200 | loss train: 1927.8798828125\n",
      "Epoch: 1300 | loss train: 1733.3897705078125\n",
      "Epoch: 1400 | loss train: 1412.4935302734375\n",
      "Epoch: 1500 | loss train: 1156.4136962890625\n",
      "Epoch: 1600 | loss train: 1001.6028442382812\n",
      "Epoch: 1700 | loss train: 883.654052734375\n",
      "Epoch: 1800 | loss train: 795.28173828125\n",
      "Epoch: 1900 | loss train: 727.9817504882812\n",
      "Epoch: 2000 | loss train: 674.204345703125\n",
      "Epoch: 2100 | loss train: 629.7575073242188\n",
      "Epoch: 2200 | loss train: 593.949462890625\n",
      "Epoch: 2300 | loss train: 563.5167846679688\n",
      "Epoch: 2400 | loss train: 545.8043212890625\n",
      "Epoch: 2500 | loss train: 507.9198303222656\n",
      "Epoch: 2600 | loss train: 484.2220764160156\n",
      "Epoch: 2700 | loss train: 464.33709716796875\n",
      "Epoch: 2800 | loss train: 443.3216247558594\n",
      "Epoch: 2900 | loss train: 426.54266357421875\n",
      "Epoch: 3000 | loss train: 411.5047912597656\n",
      "Epoch: 3100 | loss train: 398.0845642089844\n",
      "Epoch: 3200 | loss train: 383.0641174316406\n",
      "Epoch: 3300 | loss train: 370.27227783203125\n",
      "Epoch: 3400 | loss train: 359.96417236328125\n",
      "Epoch: 3500 | loss train: 360.70013427734375\n",
      "Epoch: 3600 | loss train: 339.7327575683594\n",
      "Epoch: 3700 | loss train: 329.2994689941406\n",
      "Epoch: 3800 | loss train: 326.9718322753906\n",
      "Epoch: 3900 | loss train: 318.2822570800781\n",
      "Epoch: 4000 | loss train: 309.70782470703125\n",
      "Epoch: 4100 | loss train: 305.9333801269531\n",
      "Epoch: 4200 | loss train: 296.4833984375\n",
      "Epoch: 4300 | loss train: 280.1690368652344\n",
      "Epoch: 4400 | loss train: 273.3016662597656\n",
      "Epoch: 4500 | loss train: 266.6802978515625\n",
      "Epoch: 4600 | loss train: 264.1667785644531\n",
      "Epoch: 4700 | loss train: 260.880126953125\n",
      "Epoch: 4800 | loss train: 250.42355346679688\n",
      "Epoch: 4900 | loss train: 249.7251739501953\n",
      "Epoch: 5000 | loss train: 243.91065979003906\n",
      "Epoch: 5100 | loss train: 242.27159118652344\n",
      "Epoch: 5200 | loss train: 234.2947998046875\n",
      "Epoch: 5300 | loss train: 230.84303283691406\n",
      "Epoch: 5400 | loss train: 227.26214599609375\n",
      "Epoch: 5500 | loss train: 222.5921173095703\n",
      "Epoch: 5600 | loss train: 215.08241271972656\n",
      "Epoch: 5700 | loss train: 220.9624786376953\n",
      "Epoch: 5800 | loss train: 210.28512573242188\n",
      "Epoch: 5900 | loss train: 203.65548706054688\n",
      "Epoch: 6000 | loss train: 206.8372039794922\n",
      "Epoch: 6100 | loss train: 196.58187866210938\n",
      "Epoch: 6200 | loss train: 223.82984924316406\n",
      "Epoch: 6300 | loss train: 198.4811248779297\n",
      "Epoch: 6400 | loss train: 195.77745056152344\n",
      "Epoch: 6500 | loss train: 183.12559509277344\n",
      "Epoch: 6600 | loss train: 186.18905639648438\n",
      "Epoch: 6700 | loss train: 197.281982421875\n",
      "Epoch: 6800 | loss train: 173.8290557861328\n",
      "Epoch: 6900 | loss train: 169.63792419433594\n",
      "Epoch: 7000 | loss train: 171.2896728515625\n",
      "Epoch: 7100 | loss train: 162.4866180419922\n",
      "Epoch: 7200 | loss train: 163.42628479003906\n",
      "Epoch: 7300 | loss train: 159.86209106445312\n",
      "Epoch: 7400 | loss train: 162.96908569335938\n",
      "Epoch: 7500 | loss train: 152.6307830810547\n",
      "Epoch: 7600 | loss train: 159.08006286621094\n",
      "Epoch: 7700 | loss train: 144.0760955810547\n",
      "Epoch: 7800 | loss train: 141.69432067871094\n",
      "Epoch: 7900 | loss train: 137.13467407226562\n",
      "Epoch: 8000 | loss train: 144.47535705566406\n",
      "Epoch: 8100 | loss train: 134.94715881347656\n",
      "Epoch: 8200 | loss train: 137.30364990234375\n",
      "Epoch: 8300 | loss train: 131.33755493164062\n",
      "Epoch: 8400 | loss train: 156.03436279296875\n",
      "Epoch: 8500 | loss train: 136.0274200439453\n",
      "Epoch: 8600 | loss train: 132.33856201171875\n",
      "Epoch: 8700 | loss train: 115.5656509399414\n",
      "Epoch: 8800 | loss train: 126.57615661621094\n",
      "Epoch: 8900 | loss train: 141.71038818359375\n",
      "Epoch: 9000 | loss train: 147.27967834472656\n",
      "Epoch: 9100 | loss train: 106.66851806640625\n",
      "Epoch: 9200 | loss train: 126.02141571044922\n",
      "Epoch: 9300 | loss train: 114.30374145507812\n",
      "Epoch: 9400 | loss train: 110.11261749267578\n",
      "Epoch: 9500 | loss train: 101.59859466552734\n",
      "Epoch: 9600 | loss train: 111.7392578125\n",
      "Epoch: 9700 | loss train: 104.4433364868164\n",
      "Epoch: 9800 | loss train: 132.4737091064453\n",
      "Epoch: 9900 | loss train: 87.50675964355469\n",
      "Epoch: 10000 | loss train: 85.81465911865234\n",
      "Epoch: 10100 | loss train: 108.51669311523438\n",
      "Epoch: 10200 | loss train: 95.25792694091797\n",
      "Epoch: 10300 | loss train: 78.45086669921875\n",
      "Epoch: 10400 | loss train: 126.28520965576172\n",
      "Epoch: 10500 | loss train: 85.21367645263672\n",
      "Epoch: 10600 | loss train: 97.37821960449219\n",
      "Epoch: 10700 | loss train: 82.64665222167969\n",
      "Epoch: 10800 | loss train: 72.09239959716797\n",
      "Epoch: 10900 | loss train: 66.15261840820312\n",
      "Epoch: 11000 | loss train: 64.96454620361328\n",
      "Epoch: 11100 | loss train: 66.44583129882812\n",
      "Epoch: 11200 | loss train: 58.9333610534668\n",
      "Epoch: 11300 | loss train: 107.1037826538086\n",
      "Epoch: 11400 | loss train: 57.005123138427734\n",
      "Epoch: 11500 | loss train: 70.94011688232422\n",
      "Epoch: 11600 | loss train: 53.9228401184082\n",
      "Epoch: 11700 | loss train: 52.110172271728516\n",
      "Epoch: 11800 | loss train: 111.0387954711914\n",
      "Epoch: 11900 | loss train: 50.42266082763672\n",
      "Epoch: 12000 | loss train: 58.20878219604492\n",
      "Epoch: 12100 | loss train: 49.77031326293945\n",
      "Epoch: 12200 | loss train: 46.310302734375\n",
      "Epoch: 12300 | loss train: 50.709869384765625\n",
      "Epoch: 12400 | loss train: 53.176788330078125\n",
      "Epoch: 12500 | loss train: 39.03288650512695\n",
      "Epoch: 12600 | loss train: 39.93263626098633\n",
      "Epoch: 12700 | loss train: 36.76949691772461\n",
      "Epoch: 12800 | loss train: 36.200355529785156\n",
      "Epoch: 12900 | loss train: 63.92919158935547\n",
      "Epoch: 13000 | loss train: 33.41337585449219\n",
      "Epoch: 13100 | loss train: 177.42129516601562\n",
      "Epoch: 13200 | loss train: 31.371980667114258\n",
      "Epoch: 13300 | loss train: 30.9937801361084\n",
      "Epoch: 13400 | loss train: 34.54865646362305\n",
      "Epoch: 13500 | loss train: 28.66588592529297\n",
      "Epoch: 13600 | loss train: 201.2315673828125\n",
      "Epoch: 13700 | loss train: 27.000484466552734\n",
      "Epoch: 13800 | loss train: 26.80081558227539\n",
      "Epoch: 13900 | loss train: 89.66555786132812\n",
      "Epoch: 14000 | loss train: 24.568283081054688\n",
      "Epoch: 14100 | loss train: 58.32411575317383\n",
      "Epoch: 14200 | loss train: 23.08120346069336\n",
      "Epoch: 14300 | loss train: 46.47740173339844\n",
      "Epoch: 14400 | loss train: 22.342147827148438\n",
      "Epoch: 14500 | loss train: 29.06623649597168\n",
      "Epoch: 14600 | loss train: 20.795490264892578\n",
      "Epoch: 14700 | loss train: 20.391807556152344\n",
      "Epoch: 14800 | loss train: 86.461669921875\n",
      "Epoch: 14900 | loss train: 19.799453735351562\n",
      "Epoch: 15000 | loss train: 18.79317855834961\n",
      "Epoch: 15100 | loss train: 27.877288818359375\n",
      "Epoch: 15200 | loss train: 18.003942489624023\n",
      "Epoch: 15300 | loss train: 33.284175872802734\n",
      "Epoch: 15400 | loss train: 16.17359161376953\n",
      "Epoch: 15500 | loss train: 15.946379661560059\n",
      "Epoch: 15600 | loss train: 17.27933692932129\n",
      "Epoch: 15700 | loss train: 22.293018341064453\n",
      "Epoch: 15800 | loss train: 34.043785095214844\n",
      "Epoch: 15900 | loss train: 13.972424507141113\n",
      "Epoch: 16000 | loss train: 95.70170593261719\n",
      "Epoch: 16100 | loss train: 16.767709732055664\n",
      "Epoch: 16200 | loss train: 14.863306999206543\n",
      "Epoch: 16300 | loss train: 103.19878387451172\n",
      "Epoch: 16400 | loss train: 12.171628952026367\n",
      "Epoch: 16500 | loss train: 12.895981788635254\n",
      "Epoch: 16600 | loss train: 75.4407730102539\n",
      "Epoch: 16700 | loss train: 12.646560668945312\n",
      "Epoch: 16800 | loss train: 11.96311092376709\n",
      "Epoch: 16900 | loss train: 10.430115699768066\n",
      "Epoch: 17000 | loss train: 114.65365600585938\n",
      "Epoch: 17100 | loss train: 10.495899200439453\n",
      "Epoch: 17200 | loss train: 9.936734199523926\n",
      "Epoch: 17300 | loss train: 18.73352813720703\n",
      "Epoch: 17400 | loss train: 34.00590133666992\n",
      "Epoch: 17500 | loss train: 9.215229988098145\n",
      "Epoch: 17600 | loss train: 25.215831756591797\n",
      "Epoch: 17700 | loss train: 9.083784103393555\n",
      "Epoch: 17800 | loss train: 35.45801544189453\n",
      "Epoch: 17900 | loss train: 8.234959602355957\n",
      "Epoch: 18000 | loss train: 27.46663475036621\n",
      "Epoch: 18100 | loss train: 24.399267196655273\n",
      "Epoch: 18200 | loss train: 7.796842098236084\n",
      "Epoch: 18300 | loss train: 179.8886260986328\n",
      "Epoch: 18400 | loss train: 7.5478339195251465\n",
      "Epoch: 18500 | loss train: 161.36331176757812\n",
      "Epoch: 18600 | loss train: 22.889028549194336\n",
      "Epoch: 18700 | loss train: 10.958744049072266\n",
      "Epoch: 18800 | loss train: 15.317731857299805\n",
      "Epoch: 18900 | loss train: 6.538657188415527\n",
      "Epoch: 19000 | loss train: 45.82333755493164\n",
      "Epoch: 19100 | loss train: 7.60286283493042\n",
      "Epoch: 19200 | loss train: 15.83488941192627\n",
      "Epoch: 19300 | loss train: 5.4653730392456055\n",
      "Epoch: 19400 | loss train: 77.04379272460938\n",
      "Epoch: 19500 | loss train: 6.373598575592041\n",
      "Epoch: 19600 | loss train: 5.7706074714660645\n",
      "Epoch: 19700 | loss train: 11.704723358154297\n",
      "Epoch: 19800 | loss train: 4.977744102478027\n",
      "Epoch: 19900 | loss train: 7.3658127784729\n",
      "Epoch: 20000 | loss train: 16.337617874145508\n",
      "Epoch: 20100 | loss train: 4.66373872756958\n",
      "Epoch: 20200 | loss train: 5.154947757720947\n",
      "Epoch: 20300 | loss train: 6.451664924621582\n",
      "Epoch: 20400 | loss train: 4.387578010559082\n",
      "Epoch: 20500 | loss train: 54.68004608154297\n",
      "Epoch: 20600 | loss train: 3.8491718769073486\n",
      "Epoch: 20700 | loss train: 9.001618385314941\n",
      "Epoch: 20800 | loss train: 6.747913360595703\n",
      "Epoch: 20900 | loss train: 4.964581489562988\n",
      "Epoch: 21000 | loss train: 4.603731155395508\n",
      "Epoch: 21100 | loss train: 39.372718811035156\n",
      "Epoch: 21200 | loss train: 5.94706916809082\n",
      "Epoch: 21300 | loss train: 77.28435516357422\n",
      "Epoch: 21400 | loss train: 4.471677780151367\n",
      "Epoch: 21500 | loss train: 4.052258491516113\n",
      "Epoch: 21600 | loss train: 21.673885345458984\n",
      "Epoch: 21700 | loss train: 6.33437967300415\n",
      "Epoch: 21800 | loss train: 3.7950243949890137\n",
      "Epoch: 21900 | loss train: 44.52116775512695\n",
      "Epoch: 22000 | loss train: 4.079372882843018\n",
      "Epoch: 22100 | loss train: 13.886744499206543\n",
      "Epoch: 22200 | loss train: 5.324485778808594\n",
      "Epoch: 22300 | loss train: 3.553027391433716\n",
      "Epoch: 22400 | loss train: 3.29137921333313\n",
      "Epoch: 22500 | loss train: 124.35327911376953\n",
      "Epoch: 22600 | loss train: 3.342449903488159\n",
      "Epoch: 22700 | loss train: 5.100222110748291\n",
      "Epoch: 22800 | loss train: 5.380271911621094\n",
      "Epoch: 22900 | loss train: 2.973209857940674\n",
      "Epoch: 23000 | loss train: 217.271240234375\n",
      "Epoch: 23100 | loss train: 3.6127004623413086\n",
      "Epoch: 23200 | loss train: 2.967541217803955\n",
      "Epoch: 23300 | loss train: 32.74287033081055\n",
      "Epoch: 23400 | loss train: 3.5415568351745605\n",
      "Epoch: 23500 | loss train: 2.729332447052002\n",
      "Epoch: 23600 | loss train: 14.087493896484375\n",
      "Epoch: 23700 | loss train: 3.0543887615203857\n",
      "Epoch: 23800 | loss train: 2.647402048110962\n",
      "Epoch: 23900 | loss train: 18.346691131591797\n",
      "Epoch: 24000 | loss train: 5.819418907165527\n",
      "Epoch: 24100 | loss train: 2.4923269748687744\n",
      "Epoch: 24200 | loss train: 194.9771270751953\n",
      "Epoch: 24300 | loss train: 3.3656134605407715\n",
      "Epoch: 24400 | loss train: 2.3746697902679443\n",
      "Epoch: 24500 | loss train: 217.71572875976562\n",
      "Epoch: 24600 | loss train: 3.3598945140838623\n",
      "Epoch: 24700 | loss train: 2.3259499073028564\n",
      "Epoch: 24800 | loss train: 57.69950866699219\n",
      "Epoch: 24900 | loss train: 4.0200300216674805\n",
      "Epoch: 25000 | loss train: 2.269744396209717\n",
      "Epoch: 25100 | loss train: 15.330636978149414\n",
      "Epoch: 25200 | loss train: 2.8462696075439453\n",
      "Epoch: 25300 | loss train: 2.254354476928711\n",
      "Epoch: 25400 | loss train: 1.9684150218963623\n",
      "Epoch: 25500 | loss train: 71.98914337158203\n",
      "Epoch: 25600 | loss train: 2.9701483249664307\n",
      "Epoch: 25700 | loss train: 2.2923502922058105\n",
      "Epoch: 25800 | loss train: 2.406317710876465\n",
      "Epoch: 25900 | loss train: 9.380374908447266\n",
      "Epoch: 26000 | loss train: 2.163775682449341\n",
      "Epoch: 26100 | loss train: 179.8579864501953\n",
      "Epoch: 26200 | loss train: 3.923586368560791\n",
      "Epoch: 26300 | loss train: 2.3451919555664062\n",
      "Epoch: 26400 | loss train: 1.9466805458068848\n",
      "Epoch: 26500 | loss train: 186.7869873046875\n",
      "Epoch: 26600 | loss train: 3.7267212867736816\n",
      "Epoch: 26700 | loss train: 2.3986454010009766\n",
      "Epoch: 26800 | loss train: 2.234222412109375\n",
      "Epoch: 26900 | loss train: 29.91391944885254\n",
      "Epoch: 27000 | loss train: 3.5533547401428223\n",
      "Epoch: 27100 | loss train: 2.7335879802703857\n",
      "Epoch: 27200 | loss train: 10.374151229858398\n",
      "Epoch: 27300 | loss train: 6.232293128967285\n",
      "Epoch: 27400 | loss train: 2.5628554821014404\n",
      "Epoch: 27500 | loss train: 2.0867090225219727\n",
      "Epoch: 27600 | loss train: 14.222856521606445\n",
      "Epoch: 27700 | loss train: 2.03222393989563\n",
      "Epoch: 27800 | loss train: 51.92322540283203\n",
      "Epoch: 27900 | loss train: 3.74684739112854\n",
      "Epoch: 28000 | loss train: 2.2662177085876465\n",
      "Epoch: 28100 | loss train: 1.4319264888763428\n",
      "Epoch: 28200 | loss train: 7.009762287139893\n",
      "Epoch: 28300 | loss train: 3.136958360671997\n",
      "Epoch: 28400 | loss train: 2.5274360179901123\n",
      "Epoch: 28500 | loss train: 2.0371251106262207\n",
      "Epoch: 28600 | loss train: 49.28510665893555\n",
      "Epoch: 28700 | loss train: 10.699166297912598\n",
      "Epoch: 28800 | loss train: 9.514702796936035\n",
      "Epoch: 28900 | loss train: 8.671464920043945\n",
      "Epoch: 29000 | loss train: 15.093246459960938\n",
      "Epoch: 29100 | loss train: 8.587185859680176\n",
      "Epoch: 29200 | loss train: 6.666676998138428\n",
      "Epoch: 29300 | loss train: 6.925691604614258\n",
      "Epoch: 29400 | loss train: 5.890060901641846\n",
      "Epoch: 29500 | loss train: 5.602527618408203\n",
      "Epoch: 29600 | loss train: 9.158863067626953\n",
      "Epoch: 29700 | loss train: 5.081226348876953\n",
      "Epoch: 29800 | loss train: 4.657880783081055\n",
      "Epoch: 29900 | loss train: 18.35308074951172\n",
      "Epoch: 30000 | loss train: 4.591297149658203\n",
      "Epoch: 30100 | loss train: 4.158690452575684\n",
      "Epoch: 30200 | loss train: 12.255895614624023\n",
      "Epoch: 30300 | loss train: 4.980375289916992\n",
      "Epoch: 30400 | loss train: 4.072023391723633\n",
      "Epoch: 30500 | loss train: 3.676914930343628\n",
      "Epoch: 30600 | loss train: 27.145023345947266\n",
      "Epoch: 30700 | loss train: 4.085916042327881\n",
      "Epoch: 30800 | loss train: 3.446955680847168\n",
      "Epoch: 30900 | loss train: 3.128521680831909\n",
      "Epoch: 31000 | loss train: 304.54498291015625\n",
      "Epoch: 31100 | loss train: 4.013185024261475\n",
      "Epoch: 31200 | loss train: 3.1770260334014893\n",
      "Epoch: 31300 | loss train: 2.8569865226745605\n",
      "Epoch: 31400 | loss train: 15.706340789794922\n",
      "Epoch: 31500 | loss train: 6.600636959075928\n",
      "Epoch: 31600 | loss train: 2.9428799152374268\n",
      "Epoch: 31700 | loss train: 2.6062448024749756\n",
      "Epoch: 31800 | loss train: 2.371967077255249\n",
      "Epoch: 31900 | loss train: 53.83961868286133\n",
      "Epoch: 32000 | loss train: 3.780174970626831\n",
      "Epoch: 32100 | loss train: 2.7961981296539307\n",
      "Epoch: 32200 | loss train: 2.4310102462768555\n",
      "Epoch: 32300 | loss train: 2.1967082023620605\n",
      "Epoch: 32400 | loss train: 6.333661079406738\n",
      "Epoch: 32500 | loss train: 5.655114650726318\n",
      "Epoch: 32600 | loss train: 2.500138282775879\n",
      "Epoch: 32700 | loss train: 2.212496042251587\n",
      "Epoch: 32800 | loss train: 131.45912170410156\n",
      "Epoch: 32900 | loss train: 3.076408624649048\n",
      "Epoch: 33000 | loss train: 2.4123032093048096\n",
      "Epoch: 33100 | loss train: 2.1307859420776367\n",
      "Epoch: 33200 | loss train: 1.9259185791015625\n",
      "Epoch: 33300 | loss train: 28.164316177368164\n",
      "Epoch: 33400 | loss train: 3.443225383758545\n",
      "Epoch: 33500 | loss train: 2.488279342651367\n",
      "Epoch: 33600 | loss train: 2.147712230682373\n",
      "Epoch: 33700 | loss train: 1.9052366018295288\n",
      "Epoch: 33800 | loss train: 175.8759765625\n",
      "Epoch: 33900 | loss train: 2.931730031967163\n",
      "Epoch: 34000 | loss train: 2.1988303661346436\n",
      "Epoch: 34100 | loss train: 1.917500376701355\n",
      "Epoch: 34200 | loss train: 3.3355941772460938\n",
      "Epoch: 34300 | loss train: 15.899309158325195\n",
      "Epoch: 34400 | loss train: 2.0555269718170166\n",
      "Epoch: 34500 | loss train: 1.7540078163146973\n",
      "Epoch: 34600 | loss train: 34.8687744140625\n",
      "Epoch: 34700 | loss train: 2.659013271331787\n",
      "Epoch: 34800 | loss train: 2.0644516944885254\n",
      "Epoch: 34900 | loss train: 1.783986210823059\n",
      "Epoch: 35000 | loss train: 369.6567687988281\n",
      "Epoch: 35100 | loss train: 2.6037790775299072\n",
      "Epoch: 35200 | loss train: 1.9521833658218384\n",
      "Epoch: 35300 | loss train: 1.6882922649383545\n",
      "Epoch: 35400 | loss train: 60.25397872924805\n",
      "Epoch: 35500 | loss train: 2.4928882122039795\n",
      "Epoch: 35600 | loss train: 1.9463796615600586\n",
      "Epoch: 35700 | loss train: 1.7090582847595215\n",
      "Epoch: 35800 | loss train: 1.7315571308135986\n",
      "Epoch: 35900 | loss train: 6.764256000518799\n",
      "Epoch: 36000 | loss train: 2.369781970977783\n",
      "Epoch: 36100 | loss train: 1.9717644453048706\n",
      "Epoch: 36200 | loss train: 1.6072417497634888\n",
      "Epoch: 36300 | loss train: 1.4160057306289673\n",
      "Epoch: 36400 | loss train: 53.87047576904297\n",
      "Epoch: 36500 | loss train: 2.37805438041687\n",
      "Epoch: 36600 | loss train: 1.8370651006698608\n",
      "Epoch: 36700 | loss train: 1.5898385047912598\n",
      "Epoch: 36800 | loss train: 187.04229736328125\n",
      "Epoch: 36900 | loss train: 3.658355712890625\n",
      "Epoch: 37000 | loss train: 2.004143476486206\n",
      "Epoch: 37100 | loss train: 1.6639420986175537\n",
      "Epoch: 37200 | loss train: 1.4443656206130981\n",
      "Epoch: 37300 | loss train: 528.3428955078125\n",
      "Epoch: 37400 | loss train: 4.206643581390381\n",
      "Epoch: 37500 | loss train: 2.3841261863708496\n",
      "Epoch: 37600 | loss train: 1.8498564958572388\n",
      "Epoch: 37700 | loss train: 1.639133095741272\n",
      "Epoch: 37800 | loss train: 13.819138526916504\n",
      "Epoch: 37900 | loss train: 2.3514113426208496\n",
      "Epoch: 38000 | loss train: 1.8901585340499878\n",
      "Epoch: 38100 | loss train: 5.245162010192871\n",
      "Epoch: 38200 | loss train: 2.6239407062530518\n",
      "Epoch: 38300 | loss train: 2.1169350147247314\n",
      "Epoch: 38400 | loss train: 1.839603304862976\n",
      "Epoch: 38500 | loss train: 1.6078883409500122\n",
      "Epoch: 38600 | loss train: 151.1410675048828\n",
      "Epoch: 38700 | loss train: 3.669804573059082\n",
      "Epoch: 38800 | loss train: 2.3151955604553223\n",
      "Epoch: 38900 | loss train: 1.8769084215164185\n",
      "Epoch: 39000 | loss train: 3.21106219291687\n",
      "Epoch: 39100 | loss train: 3.2408487796783447\n",
      "Epoch: 39200 | loss train: 3.981719732284546\n",
      "Epoch: 39300 | loss train: 2.019059658050537\n",
      "Epoch: 39400 | loss train: 1.6033347845077515\n",
      "Epoch: 39500 | loss train: 3.895784854888916\n",
      "Epoch: 39600 | loss train: 3.398635149002075\n",
      "Epoch: 39700 | loss train: 1.9058667421340942\n",
      "Epoch: 39800 | loss train: 1.5704962015151978\n",
      "Epoch: 39900 | loss train: 13.564836502075195\n",
      "loss_NN=tensor(3373.6860)\n",
      "pred_NN=tensor([[14138.0518],\n",
      "        [15276.0156],\n",
      "        [16483.0625],\n",
      "        [12319.3516]])\n",
      "loss_RF=3636.216459035959\n",
      "pred_RF=array([15289.56792969, 14541.26634766, 15011.82064453, 14907.53152344])\n",
      "loss_DT=5751.010627256029\n",
      "pred_DT=array([17643.171875  , 12505.99316406, 17331.13476562, 17331.13476562])\n"
     ]
    }
   ],
   "source": [
    "target_value = \"K\"\n",
    "train_field = \"T\"\n",
    "sample = generate_sample(data_df_13_09_2022, target_value, train_field)\n",
    "X_train, Y_train = create_X_train_Y_train(sample)\n",
    "sample_target = generate_sample(data_df_13_09_2022, target_value, \"BC\")\n",
    "X_target, Y_target = create_X_train_Y_train(sample_target)\n",
    "super_param={\"lr\": 0.0001, \"weight_decay\": 1e-5, \"n_epochs\": 40000, \"stop_value\": 0.5}\n",
    "re_run = \"N\"\n",
    "loss_NN, pred_NN = predict_using_neutral_network(X_train, Y_train, X_target, Y_target, f\"{'_'.join(list(train_field))}_model_predict_{target_value}_2.pt\", super_param, re_run)\n",
    "print(f\"{loss_NN=}\")\n",
    "print(f\"{pred_NN=}\")\n",
    "loss_RF, pred_RF = predict_using_random_forest(X_train, Y_train, X_target, Y_target, super_param)\n",
    "print(f\"{loss_RF=}\")\n",
    "print(f\"{pred_RF=}\")\n",
    "loss_DT, pred_DT = predict_using_decision_tree(X_train, Y_train, X_target, Y_target, super_param)\n",
    "print(f\"{loss_DT=}\")\n",
    "print(f\"{pred_DT=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | loss train: 16023.5732421875\n",
      "Epoch: 100 | loss train: 15777.1513671875\n",
      "Epoch: 200 | loss train: 2454.764892578125\n",
      "Epoch: 300 | loss train: 2403.05908203125\n",
      "Epoch: 400 | loss train: 2390.8759765625\n",
      "Epoch: 500 | loss train: 2372.331787109375\n",
      "Epoch: 600 | loss train: 2338.00390625\n",
      "Epoch: 700 | loss train: 2286.755615234375\n",
      "Epoch: 800 | loss train: 2202.562744140625\n",
      "Epoch: 900 | loss train: 2059.382080078125\n",
      "Epoch: 1000 | loss train: 1936.7081298828125\n",
      "Epoch: 1100 | loss train: 1870.8236083984375\n",
      "Epoch: 1200 | loss train: 1770.5252685546875\n",
      "Epoch: 1300 | loss train: 1602.1688232421875\n",
      "Epoch: 1400 | loss train: 1365.5596923828125\n",
      "Epoch: 1500 | loss train: 1136.3135986328125\n",
      "Epoch: 1600 | loss train: 987.8839111328125\n",
      "Epoch: 1700 | loss train: 885.2874755859375\n",
      "Epoch: 1800 | loss train: 800.898681640625\n",
      "Epoch: 1900 | loss train: 729.6976928710938\n",
      "Epoch: 2000 | loss train: 667.7543334960938\n",
      "Epoch: 2100 | loss train: 623.2646484375\n",
      "Epoch: 2200 | loss train: 570.1717529296875\n",
      "Epoch: 2300 | loss train: 526.009033203125\n",
      "Epoch: 2400 | loss train: 492.2618408203125\n",
      "Epoch: 2500 | loss train: 458.62445068359375\n",
      "Epoch: 2600 | loss train: 429.3703918457031\n",
      "Epoch: 2700 | loss train: 404.1136474609375\n",
      "Epoch: 2800 | loss train: 382.398193359375\n",
      "Epoch: 2900 | loss train: 360.714599609375\n",
      "Epoch: 3000 | loss train: 343.8215026855469\n",
      "Epoch: 3100 | loss train: 329.42266845703125\n",
      "Epoch: 3200 | loss train: 310.9736022949219\n",
      "Epoch: 3300 | loss train: 295.11370849609375\n",
      "Epoch: 3400 | loss train: 286.50079345703125\n",
      "Epoch: 3500 | loss train: 268.0071105957031\n",
      "Epoch: 3600 | loss train: 265.7811279296875\n",
      "Epoch: 3700 | loss train: 260.6852111816406\n",
      "Epoch: 3800 | loss train: 239.00021362304688\n",
      "Epoch: 3900 | loss train: 226.494140625\n",
      "Epoch: 4000 | loss train: 217.01515197753906\n",
      "Epoch: 4100 | loss train: 209.27610778808594\n",
      "Epoch: 4200 | loss train: 202.69110107421875\n",
      "Epoch: 4300 | loss train: 198.21823120117188\n",
      "Epoch: 4400 | loss train: 187.5326690673828\n",
      "Epoch: 4500 | loss train: 182.48582458496094\n",
      "Epoch: 4600 | loss train: 187.12319946289062\n",
      "Epoch: 4700 | loss train: 191.23387145996094\n",
      "Epoch: 4800 | loss train: 184.3898468017578\n",
      "Epoch: 4900 | loss train: 165.98793029785156\n",
      "Epoch: 5000 | loss train: 184.0470428466797\n",
      "Epoch: 5100 | loss train: 155.82138061523438\n",
      "Epoch: 5200 | loss train: 157.1102294921875\n",
      "Epoch: 5300 | loss train: 139.38572692871094\n",
      "Epoch: 5400 | loss train: 157.89117431640625\n",
      "Epoch: 5500 | loss train: 136.48008728027344\n",
      "Epoch: 5600 | loss train: 143.36920166015625\n",
      "Epoch: 5700 | loss train: 130.2965545654297\n",
      "Epoch: 5800 | loss train: 122.6114730834961\n",
      "Epoch: 5900 | loss train: 126.42176055908203\n",
      "Epoch: 6000 | loss train: 114.84272003173828\n",
      "Epoch: 6100 | loss train: 135.379638671875\n",
      "Epoch: 6200 | loss train: 108.875732421875\n",
      "Epoch: 6300 | loss train: 127.17564392089844\n",
      "Epoch: 6400 | loss train: 103.87527465820312\n",
      "Epoch: 6500 | loss train: 129.73583984375\n",
      "Epoch: 6600 | loss train: 97.78388977050781\n",
      "Epoch: 6700 | loss train: 95.21318817138672\n",
      "Epoch: 6800 | loss train: 93.81071472167969\n",
      "Epoch: 6900 | loss train: 88.33224487304688\n",
      "Epoch: 7000 | loss train: 86.733154296875\n",
      "Epoch: 7100 | loss train: 83.90631866455078\n",
      "Epoch: 7200 | loss train: 100.05107879638672\n",
      "Epoch: 7300 | loss train: 79.74394226074219\n",
      "Epoch: 7400 | loss train: 77.9990463256836\n",
      "Epoch: 7500 | loss train: 77.43045043945312\n",
      "Epoch: 7600 | loss train: 74.1595458984375\n",
      "Epoch: 7700 | loss train: 72.2452163696289\n",
      "Epoch: 7800 | loss train: 82.18341064453125\n",
      "Epoch: 7900 | loss train: 68.85206604003906\n",
      "Epoch: 8000 | loss train: 73.72122192382812\n",
      "Epoch: 8100 | loss train: 65.63507080078125\n",
      "Epoch: 8200 | loss train: 93.62504577636719\n",
      "Epoch: 8300 | loss train: 62.455238342285156\n",
      "Epoch: 8400 | loss train: 63.09791564941406\n",
      "Epoch: 8500 | loss train: 131.94956970214844\n",
      "Epoch: 8600 | loss train: 56.11142349243164\n",
      "Epoch: 8700 | loss train: 62.419456481933594\n",
      "Epoch: 8800 | loss train: 59.02696228027344\n",
      "Epoch: 8900 | loss train: 64.5599365234375\n",
      "Epoch: 9000 | loss train: 50.21033477783203\n",
      "Epoch: 9100 | loss train: 56.977081298828125\n",
      "Epoch: 9200 | loss train: 50.192665100097656\n",
      "Epoch: 9300 | loss train: 52.69505310058594\n",
      "Epoch: 9400 | loss train: 53.7508430480957\n",
      "Epoch: 9500 | loss train: 47.00719451904297\n",
      "Epoch: 9600 | loss train: 69.30049133300781\n",
      "Epoch: 9700 | loss train: 122.62386322021484\n",
      "Epoch: 9800 | loss train: 44.757137298583984\n",
      "Epoch: 9900 | loss train: 73.51822662353516\n",
      "Epoch: 10000 | loss train: 104.4598388671875\n",
      "Epoch: 10100 | loss train: 53.521854400634766\n",
      "Epoch: 10200 | loss train: 37.50184631347656\n",
      "Epoch: 10300 | loss train: 47.38206481933594\n",
      "Epoch: 10400 | loss train: 37.24736785888672\n",
      "Epoch: 10500 | loss train: 164.82896423339844\n",
      "Epoch: 10600 | loss train: 143.71002197265625\n",
      "Epoch: 10700 | loss train: 47.76871871948242\n",
      "Epoch: 10800 | loss train: 90.0662612915039\n",
      "Epoch: 10900 | loss train: 92.25569152832031\n",
      "Epoch: 11000 | loss train: 68.53959655761719\n",
      "Epoch: 11100 | loss train: 30.066322326660156\n",
      "Epoch: 11200 | loss train: 172.70953369140625\n",
      "Epoch: 11300 | loss train: 27.86805534362793\n",
      "Epoch: 11400 | loss train: 33.095794677734375\n",
      "Epoch: 11500 | loss train: 29.42229652404785\n",
      "Epoch: 11600 | loss train: 31.38190460205078\n",
      "Epoch: 11700 | loss train: 100.84661865234375\n",
      "Epoch: 11800 | loss train: 26.574983596801758\n",
      "Epoch: 11900 | loss train: 35.71480941772461\n",
      "Epoch: 12000 | loss train: 25.131664276123047\n",
      "Epoch: 12100 | loss train: 23.179723739624023\n",
      "Epoch: 12200 | loss train: 87.046875\n",
      "Epoch: 12300 | loss train: 24.9290714263916\n",
      "Epoch: 12400 | loss train: 29.05677032470703\n",
      "Epoch: 12500 | loss train: 26.783889770507812\n",
      "Epoch: 12600 | loss train: 41.0359992980957\n",
      "Epoch: 12700 | loss train: 24.709753036499023\n",
      "Epoch: 12800 | loss train: 25.731630325317383\n",
      "Epoch: 12900 | loss train: 24.383142471313477\n",
      "Epoch: 13000 | loss train: 22.768306732177734\n",
      "Epoch: 13100 | loss train: 107.43473052978516\n",
      "Epoch: 13200 | loss train: 17.78090476989746\n",
      "Epoch: 13300 | loss train: 21.17763328552246\n",
      "Epoch: 13400 | loss train: 60.54440689086914\n",
      "Epoch: 13500 | loss train: 16.82014274597168\n",
      "Epoch: 13600 | loss train: 44.099517822265625\n",
      "Epoch: 13700 | loss train: 26.802099227905273\n",
      "Epoch: 13800 | loss train: 16.5848331451416\n",
      "Epoch: 13900 | loss train: 37.0858154296875\n",
      "Epoch: 14000 | loss train: 79.2214126586914\n",
      "Epoch: 14100 | loss train: 32.885520935058594\n",
      "Epoch: 14200 | loss train: 48.32225036621094\n",
      "Epoch: 14300 | loss train: 55.45111846923828\n",
      "Epoch: 14400 | loss train: 87.42353057861328\n",
      "Epoch: 14500 | loss train: 18.67410659790039\n",
      "Epoch: 14600 | loss train: 13.748332977294922\n",
      "Epoch: 14700 | loss train: 13.697263717651367\n",
      "Epoch: 14800 | loss train: 24.541208267211914\n",
      "Epoch: 14900 | loss train: 126.83370208740234\n",
      "Epoch: 15000 | loss train: 93.50750732421875\n",
      "Epoch: 15100 | loss train: 11.307698249816895\n",
      "Epoch: 15200 | loss train: 10.80939769744873\n",
      "Epoch: 15300 | loss train: 10.817876815795898\n",
      "Epoch: 15400 | loss train: 49.797576904296875\n",
      "Epoch: 15500 | loss train: 76.90013122558594\n",
      "Epoch: 15600 | loss train: 106.27114868164062\n",
      "Epoch: 15700 | loss train: 9.698348045349121\n",
      "Epoch: 15800 | loss train: 9.411538124084473\n",
      "Epoch: 15900 | loss train: 12.416125297546387\n",
      "Epoch: 16000 | loss train: 12.39583969116211\n",
      "Epoch: 16100 | loss train: 56.31666564941406\n",
      "Epoch: 16200 | loss train: 8.797223091125488\n",
      "Epoch: 16300 | loss train: 18.705915451049805\n",
      "Epoch: 16400 | loss train: 8.6444673538208\n",
      "Epoch: 16500 | loss train: 9.391761779785156\n",
      "Epoch: 16600 | loss train: 23.055795669555664\n",
      "Epoch: 16700 | loss train: 9.233296394348145\n",
      "Epoch: 16800 | loss train: 8.333157539367676\n",
      "Epoch: 16900 | loss train: 13.49213695526123\n",
      "Epoch: 17000 | loss train: 15.82241153717041\n",
      "Epoch: 17100 | loss train: 7.453054904937744\n",
      "Epoch: 17200 | loss train: 12.794742584228516\n",
      "Epoch: 17300 | loss train: 7.094661235809326\n",
      "Epoch: 17400 | loss train: 9.133402824401855\n",
      "Epoch: 17500 | loss train: 7.343137741088867\n",
      "Epoch: 17600 | loss train: 8.707210540771484\n",
      "Epoch: 17700 | loss train: 140.52745056152344\n",
      "Epoch: 17800 | loss train: 6.1320366859436035\n",
      "Epoch: 17900 | loss train: 6.998708724975586\n",
      "Epoch: 18000 | loss train: 8.14661693572998\n",
      "Epoch: 18100 | loss train: 20.929912567138672\n",
      "Epoch: 18200 | loss train: 6.196746826171875\n",
      "Epoch: 18300 | loss train: 5.2836103439331055\n",
      "Epoch: 18400 | loss train: 27.382734298706055\n",
      "Epoch: 18500 | loss train: 275.8475036621094\n",
      "Epoch: 18600 | loss train: 5.389096736907959\n",
      "Epoch: 18700 | loss train: 343.7498779296875\n",
      "Epoch: 18800 | loss train: 5.204708099365234\n",
      "Epoch: 18900 | loss train: 4.6347551345825195\n",
      "Epoch: 19000 | loss train: 26.780302047729492\n",
      "Epoch: 19100 | loss train: 4.427496433258057\n",
      "Epoch: 19200 | loss train: 15.5062837600708\n",
      "Epoch: 19300 | loss train: 4.408049583435059\n",
      "Epoch: 19400 | loss train: 5.843387603759766\n",
      "Epoch: 19500 | loss train: 4.44930362701416\n",
      "Epoch: 19600 | loss train: 7.914726734161377\n",
      "Epoch: 19700 | loss train: 16.197118759155273\n",
      "Epoch: 19800 | loss train: 6.3017778396606445\n",
      "Epoch: 19900 | loss train: 5.005152702331543\n",
      "Epoch: 20000 | loss train: 41.195003509521484\n",
      "Epoch: 20100 | loss train: 3.7547566890716553\n",
      "Epoch: 20200 | loss train: 3.518259286880493\n",
      "Epoch: 20300 | loss train: 22.31736946105957\n",
      "Epoch: 20400 | loss train: 3.46403169631958\n",
      "Epoch: 20500 | loss train: 185.93162536621094\n",
      "Epoch: 20600 | loss train: 4.1185622215271\n",
      "Epoch: 20700 | loss train: 3.2971582412719727\n",
      "Epoch: 20800 | loss train: 3.1203134059906006\n",
      "Epoch: 20900 | loss train: 24.161727905273438\n",
      "Epoch: 21000 | loss train: 3.2275962829589844\n",
      "Epoch: 21100 | loss train: 2.9534873962402344\n",
      "Epoch: 21200 | loss train: 21.57182502746582\n",
      "Epoch: 21300 | loss train: 6.9202399253845215\n",
      "Epoch: 21400 | loss train: 3.933037519454956\n",
      "Epoch: 21500 | loss train: 3.412198305130005\n",
      "Epoch: 21600 | loss train: 3.1297149658203125\n",
      "Epoch: 21700 | loss train: 135.75430297851562\n",
      "Epoch: 21800 | loss train: 3.088388442993164\n",
      "Epoch: 21900 | loss train: 62.2296257019043\n",
      "Epoch: 22000 | loss train: 3.335155725479126\n",
      "Epoch: 22100 | loss train: 11.946158409118652\n",
      "Epoch: 22200 | loss train: 19.3377685546875\n",
      "Epoch: 22300 | loss train: 4.085702896118164\n",
      "Epoch: 22400 | loss train: 2.822190284729004\n",
      "Epoch: 22500 | loss train: 29.09385871887207\n",
      "Epoch: 22600 | loss train: 3.4095230102539062\n",
      "Epoch: 22700 | loss train: 3.1326839923858643\n",
      "Epoch: 22800 | loss train: 4.633103847503662\n",
      "Epoch: 22900 | loss train: 2.8327903747558594\n",
      "Epoch: 23000 | loss train: 20.834209442138672\n",
      "Epoch: 23100 | loss train: 3.4037539958953857\n",
      "Epoch: 23200 | loss train: 33.98869323730469\n",
      "Epoch: 23300 | loss train: 7.8240861892700195\n",
      "Epoch: 23400 | loss train: 4.954944133758545\n",
      "Epoch: 23500 | loss train: 3.753758430480957\n",
      "Epoch: 23600 | loss train: 3.2873384952545166\n",
      "Epoch: 23700 | loss train: 12.98371410369873\n",
      "Epoch: 23800 | loss train: 3.132624864578247\n",
      "Epoch: 23900 | loss train: 2.6878557205200195\n",
      "Epoch: 24000 | loss train: 2.4423775672912598\n",
      "Epoch: 24100 | loss train: 34.0598030090332\n",
      "Epoch: 24200 | loss train: 2.6857900619506836\n",
      "Epoch: 24300 | loss train: 2.376251459121704\n",
      "Epoch: 24400 | loss train: 33.14008712768555\n",
      "Epoch: 24500 | loss train: 2.708573579788208\n",
      "Epoch: 24600 | loss train: 101.2997055053711\n",
      "Epoch: 24700 | loss train: 2.6333634853363037\n",
      "Epoch: 24800 | loss train: 2.179783821105957\n",
      "Epoch: 24900 | loss train: 64.28840637207031\n",
      "Epoch: 25000 | loss train: 4.503453254699707\n",
      "Epoch: 25100 | loss train: 4.084431171417236\n",
      "Epoch: 25200 | loss train: 37.91183090209961\n",
      "Epoch: 25300 | loss train: 3.830723762512207\n",
      "Epoch: 25400 | loss train: 9.160249710083008\n",
      "Epoch: 25500 | loss train: 3.5762391090393066\n",
      "Epoch: 25600 | loss train: 3.329190731048584\n",
      "Epoch: 25700 | loss train: 16.240915298461914\n",
      "Epoch: 25800 | loss train: 3.198970079421997\n",
      "Epoch: 25900 | loss train: 59.813602447509766\n",
      "Epoch: 26000 | loss train: 3.4626173973083496\n",
      "Epoch: 26100 | loss train: 2.978358030319214\n",
      "Epoch: 26200 | loss train: 5.855557918548584\n",
      "Epoch: 26300 | loss train: 6.280848503112793\n",
      "Epoch: 26400 | loss train: 2.7676358222961426\n",
      "Epoch: 26500 | loss train: 38.81209182739258\n",
      "Epoch: 26600 | loss train: 2.717219829559326\n",
      "Epoch: 26700 | loss train: 95.24431610107422\n",
      "Epoch: 26800 | loss train: 2.693389415740967\n",
      "Epoch: 26900 | loss train: 18.12154197692871\n",
      "Epoch: 27000 | loss train: 5.152698040008545\n",
      "Epoch: 27100 | loss train: 2.3982088565826416\n",
      "Epoch: 27200 | loss train: 2.1920831203460693\n",
      "Epoch: 27300 | loss train: 23.759824752807617\n",
      "Epoch: 27400 | loss train: 2.4943792819976807\n",
      "Epoch: 27500 | loss train: 2.2162740230560303\n",
      "Epoch: 27600 | loss train: 2.0616791248321533\n",
      "Epoch: 27700 | loss train: 9.130703926086426\n",
      "Epoch: 27800 | loss train: 2.2043039798736572\n",
      "Epoch: 27900 | loss train: 1.9899197816848755\n",
      "Epoch: 28000 | loss train: 4.06099271774292\n",
      "Epoch: 28100 | loss train: 1.9344370365142822\n",
      "Epoch: 28200 | loss train: 17.32721710205078\n",
      "Epoch: 28300 | loss train: 2.0106492042541504\n",
      "Epoch: 28400 | loss train: 27.35329818725586\n",
      "Epoch: 28500 | loss train: 2.2075142860412598\n",
      "Epoch: 28600 | loss train: 1.8523095846176147\n",
      "Epoch: 28700 | loss train: 29.759769439697266\n",
      "Epoch: 28800 | loss train: 2.911829948425293\n",
      "Epoch: 28900 | loss train: 2.510711431503296\n",
      "Epoch: 29000 | loss train: 2.2798805236816406\n",
      "Epoch: 29100 | loss train: 21.093612670898438\n",
      "Epoch: 29200 | loss train: 1.957546591758728\n",
      "Epoch: 29300 | loss train: 1.6991605758666992\n",
      "Epoch: 29400 | loss train: 1.5515742301940918\n",
      "Epoch: 29500 | loss train: 17.84116554260254\n",
      "Epoch: 29600 | loss train: 2.5564069747924805\n",
      "Epoch: 29700 | loss train: 1.9881871938705444\n",
      "Epoch: 29800 | loss train: 1.7653098106384277\n",
      "Epoch: 29900 | loss train: 1.618166446685791\n",
      "Epoch: 30000 | loss train: 1.5042955875396729\n",
      "Epoch: 30100 | loss train: 1.4085618257522583\n",
      "Epoch: 30200 | loss train: 28.562679290771484\n",
      "Epoch: 30300 | loss train: 2.8797566890716553\n",
      "Epoch: 30400 | loss train: 2.013634443283081\n",
      "Epoch: 30500 | loss train: 1.7353652715682983\n",
      "Epoch: 30600 | loss train: 1.5602715015411377\n",
      "Epoch: 30700 | loss train: 1.4453918933868408\n",
      "Epoch: 30800 | loss train: 1.3361084461212158\n",
      "Epoch: 30900 | loss train: 82.75070190429688\n",
      "Epoch: 31000 | loss train: 3.128164291381836\n",
      "Epoch: 31100 | loss train: 1.998609185218811\n",
      "Epoch: 31200 | loss train: 1.834538221359253\n",
      "Epoch: 31300 | loss train: 1.495988130569458\n",
      "Epoch: 31400 | loss train: 1.3642913103103638\n",
      "Epoch: 31500 | loss train: 9.268548011779785\n",
      "Epoch: 31600 | loss train: 1.4951355457305908\n",
      "Epoch: 31700 | loss train: 16.375349044799805\n",
      "Epoch: 31800 | loss train: 1.721454381942749\n",
      "Epoch: 31900 | loss train: 1.4453262090682983\n",
      "Epoch: 32000 | loss train: 149.7961883544922\n",
      "Epoch: 32100 | loss train: 2.0656182765960693\n",
      "Epoch: 32200 | loss train: 1.5076804161071777\n",
      "Epoch: 32300 | loss train: 1.3154206275939941\n",
      "Epoch: 32400 | loss train: 1.176170825958252\n",
      "Epoch: 32500 | loss train: 24.970441818237305\n",
      "Epoch: 32600 | loss train: 2.9905877113342285\n",
      "Epoch: 32700 | loss train: 1.969956398010254\n",
      "Epoch: 32800 | loss train: 1.626574158668518\n",
      "Epoch: 32900 | loss train: 1.4244074821472168\n",
      "Epoch: 33000 | loss train: 1.2823361158370972\n",
      "Epoch: 33100 | loss train: 1.168405294418335\n",
      "Epoch: 33200 | loss train: 229.80960083007812\n",
      "Epoch: 33300 | loss train: 2.136072874069214\n",
      "Epoch: 33400 | loss train: 1.3749109506607056\n",
      "Epoch: 33500 | loss train: 3.5466113090515137\n",
      "Epoch: 33600 | loss train: 1.7074198722839355\n",
      "Epoch: 33700 | loss train: 1.400498867034912\n",
      "Epoch: 33800 | loss train: 1.2508115768432617\n",
      "Epoch: 33900 | loss train: 5.510112762451172\n",
      "Epoch: 34000 | loss train: 331.1051330566406\n",
      "Epoch: 34100 | loss train: 2.6184334754943848\n",
      "Epoch: 34200 | loss train: 1.4732755422592163\n",
      "Epoch: 34300 | loss train: 1.2625311613082886\n",
      "Epoch: 34400 | loss train: 1.154071569442749\n",
      "Epoch: 34500 | loss train: 8.024267196655273\n",
      "Epoch: 34600 | loss train: 2.0620100498199463\n",
      "Epoch: 34700 | loss train: 1.501602053642273\n",
      "Epoch: 34800 | loss train: 1.2759712934494019\n",
      "Epoch: 34900 | loss train: 1.1335629224777222\n",
      "Epoch: 35000 | loss train: 56.52027130126953\n",
      "Epoch: 35100 | loss train: 3.834991455078125\n",
      "Epoch: 35200 | loss train: 4.172608852386475\n",
      "Epoch: 35300 | loss train: 1.376847267150879\n",
      "Epoch: 35400 | loss train: 33.53103256225586\n",
      "Epoch: 35500 | loss train: 4.6929426193237305\n",
      "Epoch: 35600 | loss train: 1.4107297658920288\n",
      "Epoch: 35700 | loss train: 1.2447410821914673\n",
      "Epoch: 35800 | loss train: 1.109700322151184\n",
      "Epoch: 35900 | loss train: 100.75838470458984\n",
      "Epoch: 36000 | loss train: 3.6478381156921387\n",
      "Epoch: 36100 | loss train: 1.8734279870986938\n",
      "Epoch: 36200 | loss train: 1.4721086025238037\n",
      "Epoch: 36300 | loss train: 1.2217167615890503\n",
      "Epoch: 36400 | loss train: 1.064117670059204\n",
      "Epoch: 36500 | loss train: 207.76220703125\n",
      "Epoch: 36600 | loss train: 1.5466945171356201\n",
      "Epoch: 36700 | loss train: 26.172544479370117\n",
      "Epoch: 36800 | loss train: 96.4883804321289\n",
      "Epoch: 36900 | loss train: 1.3593931198120117\n",
      "Epoch: 37000 | loss train: 22.8590087890625\n",
      "Epoch: 37100 | loss train: 4.121344089508057\n",
      "Epoch: 37200 | loss train: 1.6905750036239624\n",
      "Epoch: 37300 | loss train: 1.3438842296600342\n",
      "Epoch: 37400 | loss train: 1.156501054763794\n",
      "Epoch: 37500 | loss train: 36.62890625\n",
      "Epoch: 37600 | loss train: 1.724304437637329\n",
      "Epoch: 37700 | loss train: 1.325207233428955\n",
      "Epoch: 37800 | loss train: 1.1357158422470093\n",
      "Epoch: 37900 | loss train: 107.49119567871094\n",
      "Epoch: 38000 | loss train: 3.613431453704834\n",
      "Epoch: 38100 | loss train: 1.418355107307434\n",
      "Epoch: 38200 | loss train: 1.1791783571243286\n",
      "Epoch: 38300 | loss train: 14.994789123535156\n",
      "Epoch: 38400 | loss train: 1.3695533275604248\n",
      "Epoch: 38500 | loss train: 17.495058059692383\n",
      "Epoch: 38600 | loss train: 1.4225702285766602\n",
      "Epoch: 38700 | loss train: 55.94413757324219\n",
      "Epoch: 38800 | loss train: 1.8661613464355469\n",
      "Epoch: 38900 | loss train: 1.4577038288116455\n",
      "Epoch: 39000 | loss train: 206.4713134765625\n",
      "Epoch: 39100 | loss train: 2.6619436740875244\n",
      "Epoch: 39200 | loss train: 1.700241208076477\n",
      "Epoch: 39300 | loss train: 1.4381908178329468\n",
      "Epoch: 39400 | loss train: 10.497678756713867\n",
      "Epoch: 39500 | loss train: 1.5779246091842651\n",
      "Epoch: 39600 | loss train: 1.2839254140853882\n",
      "Epoch: 39700 | loss train: 1.0972347259521484\n",
      "Epoch: 39800 | loss train: 103.05952453613281\n",
      "Epoch: 39900 | loss train: 1.9122893810272217\n",
      "loss_NN=tensor(3771.3486)\n",
      "pred_NN=tensor([[13122.1465],\n",
      "        [15141.3535],\n",
      "        [16507.9395],\n",
      "        [13737.7012]])\n",
      "loss_RF=4005.78659333256\n",
      "pred_RF=array([15394.07774414, 15596.20305664, 15609.93550781, 15576.28625977])\n",
      "loss_DT=5800.5805016577515\n",
      "pred_DT=array([17620.95117188, 17302.        , 16422.01757812, 18897.9140625 ])\n"
     ]
    }
   ],
   "source": [
    "target_value = \"K\"\n",
    "train_field = \"J\"\n",
    "sample = generate_sample(data_df_13_09_2022, target_value, train_field)\n",
    "X_train, Y_train = create_X_train_Y_train(sample)\n",
    "sample_target = generate_sample(data_df_13_09_2022, target_value, \"BC\")\n",
    "X_target, Y_target = create_X_train_Y_train(sample_target)\n",
    "super_param={\"lr\": 0.0001, \"weight_decay\": 1e-5, \"n_epochs\": 40000, \"stop_value\": 0.5}\n",
    "re_run = \"N\"\n",
    "loss_NN, pred_NN = predict_using_neutral_network(X_train, Y_train, X_target, Y_target, f\"{'_'.join(list(train_field))}_model_predict_{target_value}_2.pt\", super_param, re_run)\n",
    "print(f\"{loss_NN=}\")\n",
    "print(f\"{pred_NN=}\")\n",
    "loss_RF, pred_RF = predict_using_random_forest(X_train, Y_train, X_target, Y_target, super_param)\n",
    "print(f\"{loss_RF=}\")\n",
    "print(f\"{pred_RF=}\")\n",
    "loss_DT, pred_DT = predict_using_decision_tree(X_train, Y_train, X_target, Y_target, super_param)\n",
    "print(f\"{loss_DT=}\")\n",
    "print(f\"{pred_DT=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | loss train: 15058.7001953125\n",
      "Epoch: 100 | loss train: 14664.556640625\n",
      "Epoch: 200 | loss train: 2396.90380859375\n",
      "Epoch: 300 | loss train: 2379.38232421875\n",
      "Epoch: 400 | loss train: 2375.681640625\n",
      "Epoch: 500 | loss train: 2371.4873046875\n",
      "Epoch: 600 | loss train: 2366.86474609375\n",
      "Epoch: 700 | loss train: 2361.862548828125\n",
      "Epoch: 800 | loss train: 2356.522216796875\n",
      "Epoch: 900 | loss train: 2350.876220703125\n",
      "Epoch: 1000 | loss train: 2344.953369140625\n",
      "Epoch: 1100 | loss train: 2338.775390625\n",
      "Epoch: 1200 | loss train: 2332.324462890625\n",
      "Epoch: 1300 | loss train: 2325.575927734375\n",
      "Epoch: 1400 | loss train: 2318.4794921875\n",
      "Epoch: 1500 | loss train: 2308.687255859375\n",
      "Epoch: 1600 | loss train: 2293.666748046875\n",
      "Epoch: 1700 | loss train: 2262.30029296875\n",
      "Epoch: 1800 | loss train: 2223.971435546875\n",
      "Epoch: 1900 | loss train: 2181.40234375\n",
      "Epoch: 2000 | loss train: 2123.669189453125\n",
      "Epoch: 2100 | loss train: 1995.9837646484375\n",
      "Epoch: 2200 | loss train: 1754.3455810546875\n",
      "Epoch: 2300 | loss train: 1544.503173828125\n",
      "Epoch: 2400 | loss train: 1430.0599365234375\n",
      "Epoch: 2500 | loss train: 1346.539794921875\n",
      "Epoch: 2600 | loss train: 1279.8812255859375\n",
      "Epoch: 2700 | loss train: 1226.2149658203125\n",
      "Epoch: 2800 | loss train: 1187.013427734375\n",
      "Epoch: 2900 | loss train: 1149.1087646484375\n",
      "Epoch: 3000 | loss train: 1114.4559326171875\n",
      "Epoch: 3100 | loss train: 1081.614501953125\n",
      "Epoch: 3200 | loss train: 1062.674072265625\n",
      "Epoch: 3300 | loss train: 1027.48486328125\n",
      "Epoch: 3400 | loss train: 1003.7394409179688\n",
      "Epoch: 3500 | loss train: 974.2816772460938\n",
      "Epoch: 3600 | loss train: 954.60400390625\n",
      "Epoch: 3700 | loss train: 929.2688598632812\n",
      "Epoch: 3800 | loss train: 909.4996948242188\n",
      "Epoch: 3900 | loss train: 884.1265258789062\n",
      "Epoch: 4000 | loss train: 860.2567749023438\n",
      "Epoch: 4100 | loss train: 841.2698974609375\n",
      "Epoch: 4200 | loss train: 818.5283813476562\n",
      "Epoch: 4300 | loss train: 811.0982055664062\n",
      "Epoch: 4400 | loss train: 793.2608642578125\n",
      "Epoch: 4500 | loss train: 763.6578979492188\n",
      "Epoch: 4600 | loss train: 747.1547241210938\n",
      "Epoch: 4700 | loss train: 731.3800048828125\n",
      "Epoch: 4800 | loss train: 714.4351806640625\n",
      "Epoch: 4900 | loss train: 703.504150390625\n",
      "Epoch: 5000 | loss train: 696.288818359375\n",
      "Epoch: 5100 | loss train: 677.4041748046875\n",
      "Epoch: 5200 | loss train: 653.4371337890625\n",
      "Epoch: 5300 | loss train: 644.1626586914062\n",
      "Epoch: 5400 | loss train: 628.5006713867188\n",
      "Epoch: 5500 | loss train: 619.1553955078125\n",
      "Epoch: 5600 | loss train: 606.4160766601562\n",
      "Epoch: 5700 | loss train: 600.4185791015625\n",
      "Epoch: 5800 | loss train: 596.2108764648438\n",
      "Epoch: 5900 | loss train: 565.4323120117188\n",
      "Epoch: 6000 | loss train: 554.1140747070312\n",
      "Epoch: 6100 | loss train: 563.711669921875\n",
      "Epoch: 6200 | loss train: 530.1765747070312\n",
      "Epoch: 6300 | loss train: 528.3341674804688\n",
      "Epoch: 6400 | loss train: 509.5629577636719\n",
      "Epoch: 6500 | loss train: 498.9181823730469\n",
      "Epoch: 6600 | loss train: 488.29010009765625\n",
      "Epoch: 6700 | loss train: 489.2829284667969\n",
      "Epoch: 6800 | loss train: 495.4652404785156\n",
      "Epoch: 6900 | loss train: 461.3608703613281\n",
      "Epoch: 7000 | loss train: 465.5701599121094\n",
      "Epoch: 7100 | loss train: 468.28076171875\n",
      "Epoch: 7200 | loss train: 443.9723815917969\n",
      "Epoch: 7300 | loss train: 434.0345764160156\n",
      "Epoch: 7400 | loss train: 420.4039306640625\n",
      "Epoch: 7500 | loss train: 433.5493469238281\n",
      "Epoch: 7600 | loss train: 434.672119140625\n",
      "Epoch: 7700 | loss train: 402.0037841796875\n",
      "Epoch: 7800 | loss train: 411.27740478515625\n",
      "Epoch: 7900 | loss train: 396.5610656738281\n",
      "Epoch: 8000 | loss train: 374.2167663574219\n",
      "Epoch: 8100 | loss train: 370.70782470703125\n",
      "Epoch: 8200 | loss train: 372.4689636230469\n",
      "Epoch: 8300 | loss train: 384.36260986328125\n",
      "Epoch: 8400 | loss train: 363.0730895996094\n",
      "Epoch: 8500 | loss train: 348.8051452636719\n",
      "Epoch: 8600 | loss train: 338.512939453125\n",
      "Epoch: 8700 | loss train: 327.5548400878906\n",
      "Epoch: 8800 | loss train: 340.12493896484375\n",
      "Epoch: 8900 | loss train: 324.712158203125\n",
      "Epoch: 9000 | loss train: 329.5780944824219\n",
      "Epoch: 9100 | loss train: 329.45037841796875\n",
      "Epoch: 9200 | loss train: 351.650634765625\n",
      "Epoch: 9300 | loss train: 339.2957763671875\n",
      "Epoch: 9400 | loss train: 289.2773132324219\n",
      "Epoch: 9500 | loss train: 308.24444580078125\n",
      "Epoch: 9600 | loss train: 294.0782165527344\n",
      "Epoch: 9700 | loss train: 322.3023986816406\n",
      "Epoch: 9800 | loss train: 274.8331604003906\n",
      "Epoch: 9900 | loss train: 270.5047302246094\n",
      "Epoch: 10000 | loss train: 328.2737121582031\n",
      "Epoch: 10100 | loss train: 268.5220642089844\n",
      "Epoch: 10200 | loss train: 352.4568176269531\n",
      "Epoch: 10300 | loss train: 271.11566162109375\n",
      "Epoch: 10400 | loss train: 250.72508239746094\n",
      "Epoch: 10500 | loss train: 283.159912109375\n",
      "Epoch: 10600 | loss train: 248.77099609375\n",
      "Epoch: 10700 | loss train: 239.6612091064453\n",
      "Epoch: 10800 | loss train: 236.82846069335938\n",
      "Epoch: 10900 | loss train: 239.2310333251953\n",
      "Epoch: 11000 | loss train: 225.35025024414062\n",
      "Epoch: 11100 | loss train: 223.14724731445312\n",
      "Epoch: 11200 | loss train: 227.9473419189453\n",
      "Epoch: 11300 | loss train: 218.5064239501953\n",
      "Epoch: 11400 | loss train: 226.82479858398438\n",
      "Epoch: 11500 | loss train: 241.8811492919922\n",
      "Epoch: 11600 | loss train: 207.64634704589844\n",
      "Epoch: 11700 | loss train: 203.77037048339844\n",
      "Epoch: 11800 | loss train: 330.1418151855469\n",
      "Epoch: 11900 | loss train: 199.1873779296875\n",
      "Epoch: 12000 | loss train: 252.6990966796875\n",
      "Epoch: 12100 | loss train: 192.9706573486328\n",
      "Epoch: 12200 | loss train: 194.934814453125\n",
      "Epoch: 12300 | loss train: 188.70001220703125\n",
      "Epoch: 12400 | loss train: 207.4261016845703\n",
      "Epoch: 12500 | loss train: 183.8184051513672\n",
      "Epoch: 12600 | loss train: 248.63136291503906\n",
      "Epoch: 12700 | loss train: 198.78114318847656\n",
      "Epoch: 12800 | loss train: 182.86285400390625\n",
      "Epoch: 12900 | loss train: 228.58383178710938\n",
      "Epoch: 13000 | loss train: 172.3712615966797\n",
      "Epoch: 13100 | loss train: 170.24002075195312\n",
      "Epoch: 13200 | loss train: 169.1763916015625\n",
      "Epoch: 13300 | loss train: 190.2908172607422\n",
      "Epoch: 13400 | loss train: 172.50711059570312\n",
      "Epoch: 13500 | loss train: 164.49195861816406\n",
      "Epoch: 13600 | loss train: 192.3068389892578\n",
      "Epoch: 13700 | loss train: 249.25912475585938\n",
      "Epoch: 13800 | loss train: 156.53744506835938\n",
      "Epoch: 13900 | loss train: 181.09986877441406\n",
      "Epoch: 14000 | loss train: 166.06103515625\n",
      "Epoch: 14100 | loss train: 151.46600341796875\n",
      "Epoch: 14200 | loss train: 193.12933349609375\n",
      "Epoch: 14300 | loss train: 155.62890625\n",
      "Epoch: 14400 | loss train: 147.37966918945312\n",
      "Epoch: 14500 | loss train: 146.13800048828125\n",
      "Epoch: 14600 | loss train: 194.85768127441406\n",
      "Epoch: 14700 | loss train: 176.73501586914062\n",
      "Epoch: 14800 | loss train: 139.12709045410156\n",
      "Epoch: 14900 | loss train: 240.7209014892578\n",
      "Epoch: 15000 | loss train: 137.1835174560547\n",
      "Epoch: 15100 | loss train: 137.2496337890625\n",
      "Epoch: 15200 | loss train: 150.5348663330078\n",
      "Epoch: 15300 | loss train: 251.930908203125\n",
      "Epoch: 15400 | loss train: 153.9880828857422\n",
      "Epoch: 15500 | loss train: 199.40773010253906\n",
      "Epoch: 15600 | loss train: 201.31527709960938\n",
      "Epoch: 15700 | loss train: 197.94680786132812\n",
      "Epoch: 15800 | loss train: 133.8160858154297\n",
      "Epoch: 15900 | loss train: 125.78797149658203\n",
      "Epoch: 16000 | loss train: 126.30181884765625\n",
      "Epoch: 16100 | loss train: 134.9329376220703\n",
      "Epoch: 16200 | loss train: 158.17306518554688\n",
      "Epoch: 16300 | loss train: 255.7561492919922\n",
      "Epoch: 16400 | loss train: 131.50698852539062\n",
      "Epoch: 16500 | loss train: 140.1937713623047\n",
      "Epoch: 16600 | loss train: 122.76863098144531\n",
      "Epoch: 16700 | loss train: 114.70510864257812\n",
      "Epoch: 16800 | loss train: 122.66590881347656\n",
      "Epoch: 16900 | loss train: 112.00067901611328\n",
      "Epoch: 17000 | loss train: 116.18692779541016\n",
      "Epoch: 17100 | loss train: 221.04934692382812\n",
      "Epoch: 17200 | loss train: 108.20980072021484\n",
      "Epoch: 17300 | loss train: 238.44430541992188\n",
      "Epoch: 17400 | loss train: 129.45252990722656\n",
      "Epoch: 17500 | loss train: 105.18000030517578\n",
      "Epoch: 17600 | loss train: 345.38812255859375\n",
      "Epoch: 17700 | loss train: 110.31641387939453\n",
      "Epoch: 17800 | loss train: 216.25534057617188\n",
      "Epoch: 17900 | loss train: 113.80301666259766\n",
      "Epoch: 18000 | loss train: 170.73544311523438\n",
      "Epoch: 18100 | loss train: 408.5737609863281\n",
      "Epoch: 18200 | loss train: 99.51841735839844\n",
      "Epoch: 18300 | loss train: 236.36111450195312\n",
      "Epoch: 18400 | loss train: 97.18614196777344\n",
      "Epoch: 18500 | loss train: 103.48301696777344\n",
      "Epoch: 18600 | loss train: 166.92457580566406\n",
      "Epoch: 18700 | loss train: 94.07627868652344\n",
      "Epoch: 18800 | loss train: 256.1029357910156\n",
      "Epoch: 18900 | loss train: 92.04393005371094\n",
      "Epoch: 19000 | loss train: 281.21966552734375\n",
      "Epoch: 19100 | loss train: 95.03145599365234\n",
      "Epoch: 19200 | loss train: 113.79263305664062\n",
      "Epoch: 19300 | loss train: 90.30503845214844\n",
      "Epoch: 19400 | loss train: 121.99800109863281\n",
      "Epoch: 19500 | loss train: 106.98373413085938\n",
      "Epoch: 19600 | loss train: 152.15000915527344\n",
      "Epoch: 19700 | loss train: 119.380615234375\n",
      "Epoch: 19800 | loss train: 183.82843017578125\n",
      "Epoch: 19900 | loss train: 86.55843353271484\n",
      "Epoch: 20000 | loss train: 90.9535903930664\n",
      "Epoch: 20100 | loss train: 90.33829498291016\n",
      "Epoch: 20200 | loss train: 84.05985260009766\n",
      "Epoch: 20300 | loss train: 81.90774536132812\n",
      "Epoch: 20400 | loss train: 293.2707824707031\n",
      "Epoch: 20500 | loss train: 79.83912658691406\n",
      "Epoch: 20600 | loss train: 301.5397644042969\n",
      "Epoch: 20700 | loss train: 78.43927001953125\n",
      "Epoch: 20800 | loss train: 92.31578826904297\n",
      "Epoch: 20900 | loss train: 92.22278594970703\n",
      "Epoch: 21000 | loss train: 80.63863372802734\n",
      "Epoch: 21100 | loss train: 308.50537109375\n",
      "Epoch: 21200 | loss train: 82.62639617919922\n",
      "Epoch: 21300 | loss train: 77.1295166015625\n",
      "Epoch: 21400 | loss train: 123.03642272949219\n",
      "Epoch: 21500 | loss train: 80.32373046875\n",
      "Epoch: 21600 | loss train: 239.6167449951172\n",
      "Epoch: 21700 | loss train: 96.60496520996094\n",
      "Epoch: 21800 | loss train: 129.99838256835938\n",
      "Epoch: 21900 | loss train: 72.33995819091797\n",
      "Epoch: 22000 | loss train: 72.3718490600586\n",
      "Epoch: 22100 | loss train: 309.7023620605469\n",
      "Epoch: 22200 | loss train: 70.92344665527344\n",
      "Epoch: 22300 | loss train: 138.95928955078125\n",
      "Epoch: 22400 | loss train: 77.32173156738281\n",
      "Epoch: 22500 | loss train: 73.97382354736328\n",
      "Epoch: 22600 | loss train: 67.30924987792969\n",
      "Epoch: 22700 | loss train: 91.18201446533203\n",
      "Epoch: 22800 | loss train: 70.00141143798828\n",
      "Epoch: 22900 | loss train: 143.5520782470703\n",
      "Epoch: 23000 | loss train: 72.72314453125\n",
      "Epoch: 23100 | loss train: 107.81655883789062\n",
      "Epoch: 23200 | loss train: 128.31988525390625\n",
      "Epoch: 23300 | loss train: 62.816978454589844\n",
      "Epoch: 23400 | loss train: 174.0159149169922\n",
      "Epoch: 23500 | loss train: 219.2887420654297\n",
      "Epoch: 23600 | loss train: 65.20940399169922\n",
      "Epoch: 23700 | loss train: 66.6474380493164\n",
      "Epoch: 23800 | loss train: 71.26966857910156\n",
      "Epoch: 23900 | loss train: 587.0338745117188\n",
      "Epoch: 24000 | loss train: 60.427490234375\n",
      "Epoch: 24100 | loss train: 223.9066925048828\n",
      "Epoch: 24200 | loss train: 65.7151870727539\n",
      "Epoch: 24300 | loss train: 58.122745513916016\n",
      "Epoch: 24400 | loss train: 94.34356689453125\n",
      "Epoch: 24500 | loss train: 58.49140167236328\n",
      "Epoch: 24600 | loss train: 234.06195068359375\n",
      "Epoch: 24700 | loss train: 87.6932601928711\n",
      "Epoch: 24800 | loss train: 56.396488189697266\n",
      "Epoch: 24900 | loss train: 326.548828125\n",
      "Epoch: 25000 | loss train: 62.86001968383789\n",
      "Epoch: 25100 | loss train: 127.34648895263672\n",
      "Epoch: 25200 | loss train: 168.83653259277344\n",
      "Epoch: 25300 | loss train: 56.70881271362305\n",
      "Epoch: 25400 | loss train: 228.9329071044922\n",
      "Epoch: 25500 | loss train: 65.88777160644531\n",
      "Epoch: 25600 | loss train: 64.39053344726562\n",
      "Epoch: 25700 | loss train: 83.41150665283203\n",
      "Epoch: 25800 | loss train: 52.78132247924805\n",
      "Epoch: 25900 | loss train: 371.1889953613281\n",
      "Epoch: 26000 | loss train: 61.80913543701172\n",
      "Epoch: 26100 | loss train: 55.28126907348633\n",
      "Epoch: 26200 | loss train: 132.48388671875\n",
      "Epoch: 26300 | loss train: 126.08271026611328\n",
      "Epoch: 26400 | loss train: 52.27997970581055\n",
      "Epoch: 26500 | loss train: 61.60504913330078\n",
      "Epoch: 26600 | loss train: 192.63693237304688\n",
      "Epoch: 26700 | loss train: 49.2355842590332\n",
      "Epoch: 26800 | loss train: 75.34845733642578\n",
      "Epoch: 26900 | loss train: 91.58869171142578\n",
      "Epoch: 27000 | loss train: 54.880523681640625\n",
      "Epoch: 27100 | loss train: 317.5264892578125\n",
      "Epoch: 27200 | loss train: 46.74503707885742\n",
      "Epoch: 27300 | loss train: 104.54020690917969\n",
      "Epoch: 27400 | loss train: 47.33284378051758\n",
      "Epoch: 27500 | loss train: 54.246788024902344\n",
      "Epoch: 27600 | loss train: 182.69931030273438\n",
      "Epoch: 27700 | loss train: 47.7255859375\n",
      "Epoch: 27800 | loss train: 48.03372573852539\n",
      "Epoch: 27900 | loss train: 48.242000579833984\n",
      "Epoch: 28000 | loss train: 52.50522994995117\n",
      "Epoch: 28100 | loss train: 314.1231994628906\n",
      "Epoch: 28200 | loss train: 44.16117477416992\n",
      "Epoch: 28300 | loss train: 50.99420166015625\n",
      "Epoch: 28400 | loss train: 60.53790283203125\n",
      "Epoch: 28500 | loss train: 82.91242980957031\n",
      "Epoch: 28600 | loss train: 52.22691345214844\n",
      "Epoch: 28700 | loss train: 42.30107879638672\n",
      "Epoch: 28800 | loss train: 55.303897857666016\n",
      "Epoch: 28900 | loss train: 44.18134689331055\n",
      "Epoch: 29000 | loss train: 41.32061004638672\n",
      "Epoch: 29100 | loss train: 154.67935180664062\n",
      "Epoch: 29200 | loss train: 41.44053268432617\n",
      "Epoch: 29300 | loss train: 45.7699089050293\n",
      "Epoch: 29400 | loss train: 42.36165237426758\n",
      "Epoch: 29500 | loss train: 40.063690185546875\n",
      "Epoch: 29600 | loss train: 240.30894470214844\n",
      "Epoch: 29700 | loss train: 41.0322151184082\n",
      "Epoch: 29800 | loss train: 38.824180603027344\n",
      "Epoch: 29900 | loss train: 688.4005737304688\n",
      "Epoch: 30000 | loss train: 38.95038604736328\n",
      "Epoch: 30100 | loss train: 37.51951217651367\n",
      "Epoch: 30200 | loss train: 115.52520751953125\n",
      "Epoch: 30300 | loss train: 38.267478942871094\n",
      "Epoch: 30400 | loss train: 38.47211837768555\n",
      "Epoch: 30500 | loss train: 40.954410552978516\n",
      "Epoch: 30600 | loss train: 38.46085739135742\n",
      "Epoch: 30700 | loss train: 38.7462043762207\n",
      "Epoch: 30800 | loss train: 41.77167892456055\n",
      "Epoch: 30900 | loss train: 42.974178314208984\n",
      "Epoch: 31000 | loss train: 35.185523986816406\n",
      "Epoch: 31100 | loss train: 35.452293395996094\n",
      "Epoch: 31200 | loss train: 187.9332733154297\n",
      "Epoch: 31300 | loss train: 34.31629943847656\n",
      "Epoch: 31400 | loss train: 33.31431579589844\n",
      "Epoch: 31500 | loss train: 630.372802734375\n",
      "Epoch: 31600 | loss train: 33.458984375\n",
      "Epoch: 31700 | loss train: 33.250083923339844\n",
      "Epoch: 31800 | loss train: 625.2034912109375\n",
      "Epoch: 31900 | loss train: 71.58757019042969\n",
      "Epoch: 32000 | loss train: 63.62421417236328\n",
      "Epoch: 32100 | loss train: 59.06131362915039\n",
      "Epoch: 32200 | loss train: 54.3839225769043\n",
      "Epoch: 32300 | loss train: 52.16231155395508\n",
      "Epoch: 32400 | loss train: 146.06910705566406\n",
      "Epoch: 32500 | loss train: 139.49502563476562\n",
      "Epoch: 32600 | loss train: 194.76808166503906\n",
      "Epoch: 32700 | loss train: 42.95354461669922\n",
      "Epoch: 32800 | loss train: 115.25762939453125\n",
      "Epoch: 32900 | loss train: 44.54035949707031\n",
      "Epoch: 33000 | loss train: 39.3238639831543\n",
      "Epoch: 33100 | loss train: 153.33090209960938\n",
      "Epoch: 33200 | loss train: 38.731834411621094\n",
      "Epoch: 33300 | loss train: 36.33650588989258\n",
      "Epoch: 33400 | loss train: 166.10025024414062\n",
      "Epoch: 33500 | loss train: 36.13020324707031\n",
      "Epoch: 33600 | loss train: 39.20863723754883\n",
      "Epoch: 33700 | loss train: 35.21352005004883\n",
      "Epoch: 33800 | loss train: 35.0367317199707\n",
      "Epoch: 33900 | loss train: 255.39122009277344\n",
      "Epoch: 34000 | loss train: 32.826107025146484\n",
      "Epoch: 34100 | loss train: 31.709569931030273\n",
      "Epoch: 34200 | loss train: 245.25009155273438\n",
      "Epoch: 34300 | loss train: 31.681848526000977\n",
      "Epoch: 34400 | loss train: 118.57344055175781\n",
      "Epoch: 34500 | loss train: 31.324886322021484\n",
      "Epoch: 34600 | loss train: 40.233089447021484\n",
      "Epoch: 34700 | loss train: 36.25156021118164\n",
      "Epoch: 34800 | loss train: 29.41729164123535\n",
      "Epoch: 34900 | loss train: 37.09050750732422\n",
      "Epoch: 35000 | loss train: 35.77302551269531\n",
      "Epoch: 35100 | loss train: 28.07042694091797\n",
      "Epoch: 35200 | loss train: 51.713890075683594\n",
      "Epoch: 35300 | loss train: 30.31892204284668\n",
      "Epoch: 35400 | loss train: 27.73494529724121\n",
      "Epoch: 35500 | loss train: 27.99537467956543\n",
      "Epoch: 35600 | loss train: 443.1847839355469\n",
      "Epoch: 35700 | loss train: 28.582794189453125\n",
      "Epoch: 35800 | loss train: 27.988422393798828\n",
      "Epoch: 35900 | loss train: 44.73221969604492\n",
      "Epoch: 36000 | loss train: 34.610679626464844\n",
      "Epoch: 36100 | loss train: 26.01946258544922\n",
      "Epoch: 36200 | loss train: 25.026094436645508\n",
      "Epoch: 36300 | loss train: 199.22805786132812\n",
      "Epoch: 36400 | loss train: 25.718961715698242\n",
      "Epoch: 36500 | loss train: 24.317689895629883\n",
      "Epoch: 36600 | loss train: 249.50823974609375\n",
      "Epoch: 36700 | loss train: 24.916309356689453\n",
      "Epoch: 36800 | loss train: 24.061113357543945\n",
      "Epoch: 36900 | loss train: 310.9215087890625\n",
      "Epoch: 37000 | loss train: 24.848358154296875\n",
      "Epoch: 37100 | loss train: 23.598499298095703\n",
      "Epoch: 37200 | loss train: 518.6725463867188\n",
      "Epoch: 37300 | loss train: 25.583223342895508\n",
      "Epoch: 37400 | loss train: 23.52385902404785\n",
      "Epoch: 37500 | loss train: 22.945688247680664\n",
      "Epoch: 37600 | loss train: 462.83843994140625\n",
      "Epoch: 37700 | loss train: 27.275997161865234\n",
      "Epoch: 37800 | loss train: 22.799991607666016\n",
      "Epoch: 37900 | loss train: 24.187084197998047\n",
      "Epoch: 38000 | loss train: 220.28982543945312\n",
      "Epoch: 38100 | loss train: 23.916540145874023\n",
      "Epoch: 38200 | loss train: 22.422311782836914\n",
      "Epoch: 38300 | loss train: 21.756282806396484\n",
      "Epoch: 38400 | loss train: 21.515520095825195\n",
      "Epoch: 38500 | loss train: 390.5892639160156\n",
      "Epoch: 38600 | loss train: 23.752870559692383\n",
      "Epoch: 38700 | loss train: 21.720260620117188\n",
      "Epoch: 38800 | loss train: 20.931570053100586\n",
      "Epoch: 38900 | loss train: 20.846187591552734\n",
      "Epoch: 39000 | loss train: 52.75362014770508\n",
      "Epoch: 39100 | loss train: 24.912342071533203\n",
      "Epoch: 39200 | loss train: 20.800403594970703\n",
      "Epoch: 39300 | loss train: 20.569488525390625\n",
      "Epoch: 39400 | loss train: 19.988271713256836\n",
      "Epoch: 39500 | loss train: 21.8949031829834\n",
      "Epoch: 39600 | loss train: 37.0586051940918\n",
      "Epoch: 39700 | loss train: 23.728717803955078\n",
      "Epoch: 39800 | loss train: 22.591773986816406\n",
      "Epoch: 39900 | loss train: 22.361509323120117\n",
      "loss_NN=tensor(3080.1311)\n",
      "pred_NN=tensor([[16748.6719],\n",
      "        [16216.3818],\n",
      "        [15290.9561],\n",
      "        [12372.1836]])\n",
      "loss_RF=3323.4434397875284\n",
      "pred_RF=array([15564.14704102, 14895.74091797, 14671.13124023, 14378.97755859])\n",
      "loss_DT=2213.9649286991116\n",
      "pred_DT=array([15524.62988281, 15165.60253906, 12938.05175781, 12880.        ])\n"
     ]
    }
   ],
   "source": [
    "target_value = \"K\"\n",
    "train_field = [\"T\", \"J\"]\n",
    "sample = generate_sample(data_df_13_09_2022, target_value, train_field)\n",
    "X_train, Y_train = create_X_train_Y_train(sample)\n",
    "sample_target = generate_sample(data_df_13_09_2022, target_value, \"BC\")\n",
    "X_target, Y_target = create_X_train_Y_train(sample_target)\n",
    "super_param={\"lr\": 0.0001, \"weight_decay\": 1e-5, \"n_epochs\": 40000, \"stop_value\": 0.5}\n",
    "re_run = \"N\"\n",
    "loss_NN, pred_NN = predict_using_neutral_network(X_train, Y_train, X_target, Y_target, f\"{'_'.join(list(train_field))}_model_predict_{target_value}_2.pt\", super_param, re_run)\n",
    "print(f\"{loss_NN=}\")\n",
    "print(f\"{pred_NN=}\")\n",
    "loss_RF, pred_RF = predict_using_random_forest(X_train, Y_train, X_target, Y_target, super_param)\n",
    "print(f\"{loss_RF=}\")\n",
    "print(f\"{pred_RF=}\")\n",
    "loss_DT, pred_DT = predict_using_decision_tree(X_train, Y_train, X_target, Y_target, super_param)\n",
    "print(f\"{loss_DT=}\")\n",
    "print(f\"{pred_DT=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from common.read_spectral_common import (\n",
    "    data_df_13_09_2022, \n",
    "    calculate_mutual_info_for_all, \n",
    "    generate_sample, create_X_train_Y_train, \n",
    "    mutual_info_regression, \n",
    "    get_max_bands, \n",
    "    get_bands_ix_from_mutual_info, \n",
    "    get_average_bands, \n",
    "    get_max_bands, \n",
    "    get_min_bands,\n",
    "    predict_using_neutral_network, \n",
    "    predict_using_random_forest, \n",
    "    predict_using_decision_tree,\n",
    "    get_full_path,\n",
    "    load_sklearn_model_to_file_by_cloudpickle,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import cloudpickle\n",
    "import common\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_value = \"P\"\n",
    "train_field = \"T\"\n",
    "function_get = get_max_bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(29, 0.40083381562669373),\n",
       " (27, 0.23341104691909154),\n",
       " (9, 0.21625734506316263),\n",
       " (92, 0.1923746727515736),\n",
       " (14, 0.16710453698817895),\n",
       " (25, 0.16582254665974006),\n",
       " (107, 0.14230151466555396),\n",
       " (87, 0.14155293403733005),\n",
       " (59, 0.13599192896192047),\n",
       " (91, 0.13298349673340537),\n",
       " (93, 0.131278457677666),\n",
       " (111, 0.131174691384496),\n",
       " (39, 0.09026556349259618),\n",
       " (36, 0.09024680046851863),\n",
       " (68, 0.08545210070713471),\n",
       " (112, 0.07868734542378597),\n",
       " (115, 0.07642502842525545),\n",
       " (86, 0.07632789644910343),\n",
       " (69, 0.07624256907503657),\n",
       " (53, 0.07530437930269285),\n",
       " (16, 0.07263162461068307),\n",
       " (38, 0.06966971297006053),\n",
       " (10, 0.0671481306624293),\n",
       " (110, 0.06663090085402024),\n",
       " (76, 0.06547028390629128),\n",
       " (4, 0.0623468273708605),\n",
       " (8, 0.06120796084188296),\n",
       " (79, 0.06073088737201138),\n",
       " (32, 0.06031793731686674),\n",
       " (81, 0.056432020139036254),\n",
       " (58, 0.052839689351315666),\n",
       " (82, 0.05274346937187868),\n",
       " (47, 0.04888824889377563),\n",
       " (35, 0.048328357012977996),\n",
       " (88, 0.048193546511275276),\n",
       " (56, 0.0442497402137656),\n",
       " (37, 0.04266528087434418),\n",
       " (78, 0.04024993930302312),\n",
       " (1, 0.0353216837942687),\n",
       " (54, 0.034308879168083894),\n",
       " (42, 0.032391030307820756),\n",
       " (94, 0.02921757513415857),\n",
       " (77, 0.028522947364463924),\n",
       " (6, 0.026506469108937836),\n",
       " (75, 0.019953325244458142),\n",
       " (105, 0.018241241150533938),\n",
       " (55, 0.010778284763635515),\n",
       " (49, 0.005602163535880056),\n",
       " (34, 0.005176938169426659),\n",
       " (71, 0.0026041817807476164),\n",
       " (61, 0.0021295191444048456),\n",
       " (121, 0.0008169934640518406),\n",
       " (0, 0.0),\n",
       " (2, 0.0),\n",
       " (3, 0.0),\n",
       " (5, 0.0),\n",
       " (7, 0.0),\n",
       " (11, 0.0),\n",
       " (12, 0.0),\n",
       " (13, 0.0),\n",
       " (15, 0.0),\n",
       " (17, 0.0),\n",
       " (18, 0.0),\n",
       " (19, 0.0),\n",
       " (20, 0.0),\n",
       " (21, 0.0),\n",
       " (22, 0.0),\n",
       " (23, 0.0),\n",
       " (24, 0.0),\n",
       " (26, 0.0),\n",
       " (28, 0.0),\n",
       " (30, 0.0),\n",
       " (31, 0.0),\n",
       " (33, 0.0),\n",
       " (40, 0.0),\n",
       " (41, 0.0),\n",
       " (43, 0.0),\n",
       " (44, 0.0),\n",
       " (45, 0.0),\n",
       " (46, 0.0),\n",
       " (48, 0.0),\n",
       " (50, 0.0),\n",
       " (51, 0.0),\n",
       " (52, 0.0),\n",
       " (57, 0.0),\n",
       " (60, 0.0),\n",
       " (62, 0.0),\n",
       " (63, 0.0),\n",
       " (64, 0.0),\n",
       " (65, 0.0),\n",
       " (66, 0.0),\n",
       " (67, 0.0),\n",
       " (70, 0.0),\n",
       " (72, 0.0),\n",
       " (73, 0.0),\n",
       " (74, 0.0),\n",
       " (80, 0.0),\n",
       " (83, 0.0),\n",
       " (84, 0.0),\n",
       " (85, 0.0),\n",
       " (89, 0.0),\n",
       " (90, 0.0),\n",
       " (95, 0.0),\n",
       " (96, 0.0),\n",
       " (97, 0.0),\n",
       " (98, 0.0),\n",
       " (99, 0.0),\n",
       " (100, 0.0),\n",
       " (101, 0.0),\n",
       " (102, 0.0),\n",
       " (103, 0.0),\n",
       " (104, 0.0),\n",
       " (106, 0.0),\n",
       " (108, 0.0),\n",
       " (109, 0.0),\n",
       " (113, 0.0),\n",
       " (114, 0.0),\n",
       " (116, 0.0),\n",
       " (117, 0.0),\n",
       " (118, 0.0),\n",
       " (119, 0.0),\n",
       " (120, 0.0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_mutual_info_for_all(data_df_13_09_2022, target_value, train_field, function_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_ix = get_bands_ix_from_mutual_info(data_df_13_09_2022, -1, target_value, train_field, function_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bands_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bands_ix = filter(lambda data: data < 100, bands_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_ix = list(bands_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bands_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bands_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_now = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "NN_file_path = get_full_path(f\"../../model_saved/NN_save/NN_object/{'_'.join(train_field)}_predict_{'_'.join(target_value)}_{date_now}_using_{function_get.__name__}.pkl\")\n",
    "RF_file_path = get_full_path(f\"../../model_saved/RF_save/{'_'.join(train_field)}_predict_{'_'.join(target_value)}_{date_now}_using_{function_get.__name__}.pkl\")\n",
    "DT_file_path = get_full_path(f\"../../model_saved/DT_save/{'_'.join(train_field)}_predict_{'_'.join(target_value)}_{date_now}_using_{function_get.__name__}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | loss train: 14027.6005859375\n",
      "Epoch: 100 | loss train: 13697.357421875\n",
      "Epoch: 200 | loss train: 2434.593505859375\n",
      "Epoch: 300 | loss train: 2424.35498046875\n",
      "Epoch: 400 | loss train: 2418.376953125\n",
      "Epoch: 500 | loss train: 2407.356689453125\n",
      "Epoch: 600 | loss train: 2392.379638671875\n",
      "Epoch: 700 | loss train: 2375.621826171875\n",
      "Epoch: 800 | loss train: 2357.351806640625\n",
      "Epoch: 900 | loss train: 2336.76904296875\n",
      "Epoch: 1000 | loss train: 2311.5068359375\n",
      "Epoch: 1100 | loss train: 2261.446044921875\n",
      "Epoch: 1200 | loss train: 2189.191162109375\n",
      "Epoch: 1300 | loss train: 2158.31103515625\n",
      "Epoch: 1400 | loss train: 2141.019775390625\n",
      "Epoch: 1500 | loss train: 2115.516357421875\n",
      "Epoch: 1600 | loss train: 2074.738525390625\n",
      "Epoch: 1700 | loss train: 2018.156982421875\n",
      "Epoch: 1800 | loss train: 1941.8135986328125\n",
      "Epoch: 1900 | loss train: 1865.195068359375\n",
      "Epoch: 2000 | loss train: 1801.3253173828125\n",
      "Epoch: 2100 | loss train: 1753.5562744140625\n",
      "Epoch: 2200 | loss train: 1713.948486328125\n",
      "Epoch: 2300 | loss train: 1678.0943603515625\n",
      "Epoch: 2400 | loss train: 1643.5955810546875\n",
      "Epoch: 2500 | loss train: 1606.6162109375\n",
      "Epoch: 2600 | loss train: 1571.304443359375\n",
      "Epoch: 2700 | loss train: 1536.128662109375\n",
      "Epoch: 2800 | loss train: 1499.39208984375\n",
      "Epoch: 2900 | loss train: 1477.0426025390625\n",
      "Epoch: 3000 | loss train: 1425.5870361328125\n",
      "Epoch: 3100 | loss train: 1414.5574951171875\n",
      "Epoch: 3200 | loss train: 1353.085693359375\n",
      "Epoch: 3300 | loss train: 1328.9112548828125\n",
      "Epoch: 3400 | loss train: 1288.3089599609375\n",
      "Epoch: 3500 | loss train: 1269.6046142578125\n",
      "Epoch: 3600 | loss train: 1232.89111328125\n",
      "Epoch: 3700 | loss train: 1218.9302978515625\n",
      "Epoch: 3800 | loss train: 1166.7266845703125\n",
      "Epoch: 3900 | loss train: 1152.94482421875\n",
      "Epoch: 4000 | loss train: 1120.9136962890625\n",
      "Epoch: 4100 | loss train: 1127.2828369140625\n",
      "Epoch: 4200 | loss train: 1043.24072265625\n",
      "Epoch: 4300 | loss train: 1009.4195556640625\n",
      "Epoch: 4400 | loss train: 992.9708251953125\n",
      "Epoch: 4500 | loss train: 950.4752807617188\n",
      "Epoch: 4600 | loss train: 965.0073852539062\n",
      "Epoch: 4700 | loss train: 888.3626708984375\n",
      "Epoch: 4800 | loss train: 872.768310546875\n",
      "Epoch: 4900 | loss train: 860.4056396484375\n",
      "Epoch: 5000 | loss train: 829.3970947265625\n",
      "Epoch: 5100 | loss train: 823.7907104492188\n",
      "Epoch: 5200 | loss train: 852.5330810546875\n",
      "Epoch: 5300 | loss train: 777.977294921875\n",
      "Epoch: 5400 | loss train: 775.0319213867188\n",
      "Epoch: 5500 | loss train: 728.0673217773438\n",
      "Epoch: 5600 | loss train: 704.8003540039062\n",
      "Epoch: 5700 | loss train: 682.5341796875\n",
      "Epoch: 5800 | loss train: 664.8969116210938\n",
      "Epoch: 5900 | loss train: 650.6595458984375\n",
      "Epoch: 6000 | loss train: 655.060791015625\n",
      "Epoch: 6100 | loss train: 646.16943359375\n",
      "Epoch: 6200 | loss train: 627.9525146484375\n",
      "Epoch: 6300 | loss train: 599.9566040039062\n",
      "Epoch: 6400 | loss train: 571.1476440429688\n",
      "Epoch: 6500 | loss train: 598.9476928710938\n",
      "Epoch: 6600 | loss train: 548.4989624023438\n",
      "Epoch: 6700 | loss train: 535.161865234375\n",
      "Epoch: 6800 | loss train: 538.3760375976562\n",
      "Epoch: 6900 | loss train: 547.779052734375\n",
      "Epoch: 7000 | loss train: 511.5691223144531\n",
      "Epoch: 7100 | loss train: 494.06207275390625\n",
      "Epoch: 7200 | loss train: 518.2250366210938\n",
      "Epoch: 7300 | loss train: 536.0286254882812\n",
      "Epoch: 7400 | loss train: 470.8836669921875\n",
      "Epoch: 7500 | loss train: 465.7982177734375\n",
      "Epoch: 7600 | loss train: 494.59466552734375\n",
      "Epoch: 7700 | loss train: 467.94537353515625\n",
      "Epoch: 7800 | loss train: 446.244384765625\n",
      "Epoch: 7900 | loss train: 437.4957580566406\n",
      "Epoch: 8000 | loss train: 432.8564758300781\n",
      "Epoch: 8100 | loss train: 490.1025695800781\n",
      "Epoch: 8200 | loss train: 474.9632263183594\n",
      "Epoch: 8300 | loss train: 426.28131103515625\n",
      "Epoch: 8400 | loss train: 421.84722900390625\n",
      "Epoch: 8500 | loss train: 398.9111328125\n",
      "Epoch: 8600 | loss train: 392.06768798828125\n",
      "Epoch: 8700 | loss train: 393.22332763671875\n",
      "Epoch: 8800 | loss train: 477.17071533203125\n",
      "Epoch: 8900 | loss train: 393.3826904296875\n",
      "Epoch: 9000 | loss train: 389.70166015625\n",
      "Epoch: 9100 | loss train: 378.3934020996094\n",
      "Epoch: 9200 | loss train: 350.4317932128906\n",
      "Epoch: 9300 | loss train: 347.73907470703125\n",
      "Epoch: 9400 | loss train: 341.79046630859375\n",
      "Epoch: 9500 | loss train: 353.44268798828125\n",
      "Epoch: 9600 | loss train: 354.9065856933594\n",
      "Epoch: 9700 | loss train: 347.6854553222656\n",
      "Epoch: 9800 | loss train: 330.8180236816406\n",
      "Epoch: 9900 | loss train: 332.6757507324219\n",
      "Epoch: 10000 | loss train: 418.33026123046875\n",
      "Epoch: 10100 | loss train: 384.2940673828125\n",
      "Epoch: 10200 | loss train: 332.0028991699219\n",
      "Epoch: 10300 | loss train: 439.8553161621094\n",
      "Epoch: 10400 | loss train: 313.14239501953125\n",
      "Epoch: 10500 | loss train: 474.28533935546875\n",
      "Epoch: 10600 | loss train: 374.156982421875\n",
      "Epoch: 10700 | loss train: 315.2913818359375\n",
      "Epoch: 10800 | loss train: 310.8456115722656\n",
      "Epoch: 10900 | loss train: 327.30377197265625\n",
      "Epoch: 11000 | loss train: 336.25482177734375\n",
      "Epoch: 11100 | loss train: 548.8165893554688\n",
      "Epoch: 11200 | loss train: 424.21771240234375\n",
      "Epoch: 11300 | loss train: 370.5925598144531\n",
      "Epoch: 11400 | loss train: 343.1783447265625\n",
      "Epoch: 11500 | loss train: 319.942626953125\n",
      "Epoch: 11600 | loss train: 304.9488525390625\n",
      "Epoch: 11700 | loss train: 374.25140380859375\n",
      "Epoch: 11800 | loss train: 324.9900817871094\n",
      "Epoch: 11900 | loss train: 279.72808837890625\n",
      "Epoch: 12000 | loss train: 287.2965087890625\n",
      "Epoch: 12100 | loss train: 515.7676391601562\n",
      "Epoch: 12200 | loss train: 289.63037109375\n",
      "Epoch: 12300 | loss train: 334.0184631347656\n",
      "Epoch: 12400 | loss train: 296.00909423828125\n",
      "Epoch: 12500 | loss train: 274.6245422363281\n",
      "Epoch: 12600 | loss train: 280.41302490234375\n",
      "Epoch: 12700 | loss train: 247.42881774902344\n",
      "Epoch: 12800 | loss train: 362.7929992675781\n",
      "Epoch: 12900 | loss train: 264.58563232421875\n",
      "Epoch: 13000 | loss train: 247.6680145263672\n",
      "Epoch: 13100 | loss train: 234.96287536621094\n",
      "Epoch: 13200 | loss train: 241.2927703857422\n",
      "Epoch: 13300 | loss train: 375.3778991699219\n",
      "Epoch: 13400 | loss train: 226.23301696777344\n",
      "Epoch: 13500 | loss train: 350.3398742675781\n",
      "Epoch: 13600 | loss train: 247.26751708984375\n",
      "Epoch: 13700 | loss train: 223.54556274414062\n",
      "Epoch: 13800 | loss train: 401.7550048828125\n",
      "Epoch: 13900 | loss train: 252.35177612304688\n",
      "Epoch: 14000 | loss train: 229.77003479003906\n",
      "Epoch: 14100 | loss train: 286.230224609375\n",
      "Epoch: 14200 | loss train: 257.8472900390625\n",
      "Epoch: 14300 | loss train: 340.0033264160156\n",
      "Epoch: 14400 | loss train: 263.7259216308594\n",
      "Epoch: 14500 | loss train: 244.83688354492188\n",
      "Epoch: 14600 | loss train: 364.2523498535156\n",
      "Epoch: 14700 | loss train: 208.8082733154297\n",
      "Epoch: 14800 | loss train: 244.4921112060547\n",
      "Epoch: 14900 | loss train: 211.54086303710938\n",
      "Epoch: 15000 | loss train: 213.39588928222656\n",
      "Epoch: 15100 | loss train: 242.8409881591797\n",
      "Epoch: 15200 | loss train: 195.03402709960938\n",
      "Epoch: 15300 | loss train: 314.7364501953125\n",
      "Epoch: 15400 | loss train: 196.37086486816406\n",
      "Epoch: 15500 | loss train: 345.0401916503906\n",
      "Epoch: 15600 | loss train: 406.4983215332031\n",
      "Epoch: 15700 | loss train: 284.64544677734375\n",
      "Epoch: 15800 | loss train: 208.91134643554688\n",
      "Epoch: 15900 | loss train: 254.0494842529297\n",
      "Epoch: 16000 | loss train: 242.74151611328125\n",
      "Epoch: 16100 | loss train: 299.53436279296875\n",
      "Epoch: 16200 | loss train: 391.03863525390625\n",
      "Epoch: 16300 | loss train: 218.37632751464844\n",
      "Epoch: 16400 | loss train: 180.56723022460938\n",
      "Epoch: 16500 | loss train: 295.82421875\n",
      "Epoch: 16600 | loss train: 206.11514282226562\n",
      "Epoch: 16700 | loss train: 171.24020385742188\n",
      "Epoch: 16800 | loss train: 262.1322021484375\n",
      "Epoch: 16900 | loss train: 246.7624053955078\n",
      "Epoch: 17000 | loss train: 172.81802368164062\n",
      "Epoch: 17100 | loss train: 166.05807495117188\n",
      "Epoch: 17200 | loss train: 170.2178497314453\n",
      "Epoch: 17300 | loss train: 273.3483581542969\n",
      "Epoch: 17400 | loss train: 546.941650390625\n",
      "Epoch: 17500 | loss train: 241.87948608398438\n",
      "Epoch: 17600 | loss train: 232.721435546875\n",
      "Epoch: 17700 | loss train: 181.38247680664062\n",
      "Epoch: 17800 | loss train: 176.42286682128906\n",
      "Epoch: 17900 | loss train: 157.8074188232422\n",
      "Epoch: 18000 | loss train: 342.79833984375\n",
      "Epoch: 18100 | loss train: 298.0112609863281\n",
      "Epoch: 18200 | loss train: 351.59326171875\n",
      "Epoch: 18300 | loss train: 341.2138977050781\n",
      "Epoch: 18400 | loss train: 155.65989685058594\n",
      "Epoch: 18500 | loss train: 289.487060546875\n",
      "Epoch: 18600 | loss train: 156.5381622314453\n",
      "Epoch: 18700 | loss train: 157.96896362304688\n",
      "Epoch: 18800 | loss train: 151.0535430908203\n",
      "Epoch: 18900 | loss train: 152.00128173828125\n",
      "Epoch: 19000 | loss train: 241.3666534423828\n",
      "Epoch: 19100 | loss train: 272.81951904296875\n",
      "Epoch: 19200 | loss train: 439.6702575683594\n",
      "Epoch: 19300 | loss train: 248.47508239746094\n",
      "Epoch: 19400 | loss train: 143.68336486816406\n",
      "Epoch: 19500 | loss train: 139.33082580566406\n",
      "Epoch: 19600 | loss train: 791.86962890625\n",
      "Epoch: 19700 | loss train: 432.188232421875\n",
      "Epoch: 19800 | loss train: 328.1986083984375\n",
      "Epoch: 19900 | loss train: 272.0168762207031\n",
      "Epoch: 20000 | loss train: 236.56365966796875\n",
      "Epoch: 20100 | loss train: 211.10726928710938\n",
      "Epoch: 20200 | loss train: 196.31065368652344\n",
      "Epoch: 20300 | loss train: 196.22671508789062\n",
      "Epoch: 20400 | loss train: 659.4503784179688\n",
      "Epoch: 20500 | loss train: 182.2462158203125\n",
      "Epoch: 20600 | loss train: 183.3223114013672\n",
      "Epoch: 20700 | loss train: 197.11997985839844\n",
      "Epoch: 20800 | loss train: 169.23095703125\n",
      "Epoch: 20900 | loss train: 426.7534484863281\n",
      "Epoch: 21000 | loss train: 164.97901916503906\n",
      "Epoch: 21100 | loss train: 985.5209350585938\n",
      "Epoch: 21200 | loss train: 185.15086364746094\n",
      "Epoch: 21300 | loss train: 174.45787048339844\n",
      "Epoch: 21400 | loss train: 165.2884979248047\n",
      "Epoch: 21500 | loss train: 161.50790405273438\n",
      "Epoch: 21600 | loss train: 162.7489013671875\n",
      "Epoch: 21700 | loss train: 164.6044158935547\n",
      "Epoch: 21800 | loss train: 371.0384826660156\n",
      "Epoch: 21900 | loss train: 153.03378295898438\n",
      "Epoch: 22000 | loss train: 156.75608825683594\n",
      "Epoch: 22100 | loss train: 320.22906494140625\n",
      "Epoch: 22200 | loss train: 526.292724609375\n",
      "Epoch: 22300 | loss train: 147.24485778808594\n",
      "Epoch: 22400 | loss train: 184.02047729492188\n",
      "Epoch: 22500 | loss train: 145.41954040527344\n",
      "Epoch: 22600 | loss train: 148.6823272705078\n",
      "Epoch: 22700 | loss train: 607.3720703125\n",
      "Epoch: 22800 | loss train: 141.79287719726562\n",
      "Epoch: 22900 | loss train: 140.1674346923828\n",
      "Epoch: 23000 | loss train: 199.52606201171875\n",
      "Epoch: 23100 | loss train: 183.4193878173828\n",
      "Epoch: 23200 | loss train: 137.0963134765625\n",
      "Epoch: 23300 | loss train: 141.184814453125\n",
      "Epoch: 23400 | loss train: 137.67514038085938\n",
      "Epoch: 23500 | loss train: 138.4947967529297\n",
      "Epoch: 23600 | loss train: 306.04705810546875\n",
      "Epoch: 23700 | loss train: 347.85986328125\n",
      "Epoch: 23800 | loss train: 213.6103057861328\n",
      "Epoch: 23900 | loss train: 192.06459045410156\n",
      "Epoch: 24000 | loss train: 177.68246459960938\n",
      "Epoch: 24100 | loss train: 168.66351318359375\n",
      "Epoch: 24200 | loss train: 164.11756896972656\n",
      "Epoch: 24300 | loss train: 161.39190673828125\n",
      "Epoch: 24400 | loss train: 159.49951171875\n",
      "Epoch: 24500 | loss train: 159.23477172851562\n",
      "Epoch: 24600 | loss train: 296.4604187011719\n",
      "Epoch: 24700 | loss train: 349.7481689453125\n",
      "Epoch: 24800 | loss train: 201.54415893554688\n",
      "Epoch: 24900 | loss train: 173.2703399658203\n",
      "Epoch: 25000 | loss train: 163.58743286132812\n",
      "Epoch: 25100 | loss train: 161.09141540527344\n",
      "Epoch: 25200 | loss train: 158.9936981201172\n",
      "Epoch: 25300 | loss train: 158.53036499023438\n",
      "Epoch: 25400 | loss train: 156.4142608642578\n",
      "Epoch: 25500 | loss train: 196.6800079345703\n",
      "Epoch: 25600 | loss train: 204.44898986816406\n",
      "Epoch: 25700 | loss train: 224.38375854492188\n",
      "Epoch: 25800 | loss train: 228.8798370361328\n",
      "Epoch: 25900 | loss train: 209.25306701660156\n",
      "Epoch: 26000 | loss train: 206.5839385986328\n",
      "Epoch: 26100 | loss train: 197.09353637695312\n",
      "Epoch: 26200 | loss train: 215.05551147460938\n",
      "Epoch: 26300 | loss train: 141.00709533691406\n",
      "Epoch: 26400 | loss train: 312.6940612792969\n",
      "Epoch: 26500 | loss train: 137.6807098388672\n",
      "Epoch: 26600 | loss train: 146.72781372070312\n",
      "Epoch: 26700 | loss train: 751.3541259765625\n",
      "Epoch: 26800 | loss train: 151.01788330078125\n",
      "Epoch: 26900 | loss train: 145.0776824951172\n",
      "Epoch: 27000 | loss train: 141.73580932617188\n",
      "Epoch: 27100 | loss train: 139.5286407470703\n",
      "Epoch: 27200 | loss train: 137.78184509277344\n",
      "Epoch: 27300 | loss train: 136.22801208496094\n",
      "Epoch: 27400 | loss train: 136.5407257080078\n",
      "Epoch: 27500 | loss train: 164.57374572753906\n",
      "Epoch: 27600 | loss train: 207.26742553710938\n",
      "Epoch: 27700 | loss train: 208.07664489746094\n",
      "Epoch: 27800 | loss train: 217.026611328125\n",
      "Epoch: 27900 | loss train: 837.362548828125\n",
      "Epoch: 28000 | loss train: 229.4495849609375\n",
      "Epoch: 28100 | loss train: 200.26458740234375\n",
      "Epoch: 28200 | loss train: 190.13365173339844\n",
      "Epoch: 28300 | loss train: 184.72471618652344\n",
      "Epoch: 28400 | loss train: 180.71754455566406\n",
      "Epoch: 28500 | loss train: 177.57308959960938\n",
      "Epoch: 28600 | loss train: 202.8843231201172\n",
      "Epoch: 28700 | loss train: 193.43862915039062\n",
      "Epoch: 28800 | loss train: 162.79931640625\n",
      "Epoch: 28900 | loss train: 159.4495849609375\n",
      "Epoch: 29000 | loss train: 582.051025390625\n",
      "Epoch: 29100 | loss train: 187.30126953125\n",
      "Epoch: 29200 | loss train: 172.1195068359375\n",
      "Epoch: 29300 | loss train: 167.9237823486328\n",
      "Epoch: 29400 | loss train: 165.14170837402344\n",
      "Epoch: 29500 | loss train: 162.88072204589844\n",
      "Epoch: 29600 | loss train: 161.7392578125\n",
      "Epoch: 29700 | loss train: 160.45260620117188\n",
      "Epoch: 29800 | loss train: 158.1158905029297\n",
      "Epoch: 29900 | loss train: 159.2696533203125\n",
      "Epoch: 30000 | loss train: 232.76673889160156\n",
      "Epoch: 30100 | loss train: 225.06884765625\n",
      "Epoch: 30200 | loss train: 226.3714141845703\n",
      "Epoch: 30300 | loss train: 152.4242401123047\n",
      "Epoch: 30400 | loss train: 149.69158935546875\n",
      "Epoch: 30500 | loss train: 194.28895568847656\n",
      "Epoch: 30600 | loss train: 149.80532836914062\n",
      "Epoch: 30700 | loss train: 146.09202575683594\n",
      "Epoch: 30800 | loss train: 152.9833221435547\n",
      "Epoch: 30900 | loss train: 246.1824951171875\n",
      "Epoch: 31000 | loss train: 146.2205352783203\n",
      "Epoch: 31100 | loss train: 143.0492706298828\n",
      "Epoch: 31200 | loss train: 140.992431640625\n",
      "Epoch: 31300 | loss train: 137.57029724121094\n",
      "Epoch: 31400 | loss train: 140.0354461669922\n",
      "Epoch: 31500 | loss train: 130.6803741455078\n",
      "Epoch: 31600 | loss train: 159.8690643310547\n",
      "Epoch: 31700 | loss train: 176.7620391845703\n",
      "Epoch: 31800 | loss train: 163.88681030273438\n",
      "Epoch: 31900 | loss train: 1337.14794921875\n",
      "Epoch: 32000 | loss train: 1200.1806640625\n",
      "Epoch: 32100 | loss train: 1059.315185546875\n",
      "Epoch: 32200 | loss train: 949.1469116210938\n",
      "Epoch: 32300 | loss train: 846.77783203125\n",
      "Epoch: 32400 | loss train: 763.271484375\n",
      "Epoch: 32500 | loss train: 714.8218383789062\n",
      "Epoch: 32600 | loss train: 671.68115234375\n",
      "Epoch: 32700 | loss train: 648.5791625976562\n",
      "Epoch: 32800 | loss train: 613.6528930664062\n",
      "Epoch: 32900 | loss train: 596.4191284179688\n",
      "Epoch: 33000 | loss train: 573.5452880859375\n",
      "Epoch: 33100 | loss train: 554.3675537109375\n",
      "Epoch: 33200 | loss train: 735.5914306640625\n",
      "Epoch: 33300 | loss train: 522.599609375\n",
      "Epoch: 33400 | loss train: 514.50244140625\n",
      "Epoch: 33500 | loss train: 495.503662109375\n",
      "Epoch: 33600 | loss train: 503.2545471191406\n",
      "Epoch: 33700 | loss train: 483.9828186035156\n",
      "Epoch: 33800 | loss train: 458.215576171875\n",
      "Epoch: 33900 | loss train: 445.97100830078125\n",
      "Epoch: 34000 | loss train: 435.88909912109375\n",
      "Epoch: 34100 | loss train: 426.49603271484375\n",
      "Epoch: 34200 | loss train: 419.07696533203125\n",
      "Epoch: 34300 | loss train: 407.54901123046875\n",
      "Epoch: 34400 | loss train: 459.59747314453125\n",
      "Epoch: 34500 | loss train: 391.55938720703125\n",
      "Epoch: 34600 | loss train: 386.0906677246094\n",
      "Epoch: 34700 | loss train: 379.6729736328125\n",
      "Epoch: 34800 | loss train: 377.3878173828125\n",
      "Epoch: 34900 | loss train: 571.3388671875\n",
      "Epoch: 35000 | loss train: 390.76434326171875\n",
      "Epoch: 35100 | loss train: 398.02801513671875\n",
      "Epoch: 35200 | loss train: 352.8597106933594\n",
      "Epoch: 35300 | loss train: 480.03570556640625\n",
      "Epoch: 35400 | loss train: 380.9057312011719\n",
      "Epoch: 35500 | loss train: 331.7564392089844\n",
      "Epoch: 35600 | loss train: 336.31805419921875\n",
      "Epoch: 35700 | loss train: 354.43377685546875\n",
      "Epoch: 35800 | loss train: 334.779541015625\n",
      "Epoch: 35900 | loss train: 325.0740661621094\n",
      "Epoch: 36000 | loss train: 320.1939392089844\n",
      "Epoch: 36100 | loss train: 347.47216796875\n",
      "Epoch: 36200 | loss train: 368.6976318359375\n",
      "Epoch: 36300 | loss train: 308.3799743652344\n",
      "Epoch: 36400 | loss train: 484.5558166503906\n",
      "Epoch: 36500 | loss train: 337.8963317871094\n",
      "Epoch: 36600 | loss train: 478.87060546875\n",
      "Epoch: 36700 | loss train: 357.1673583984375\n",
      "Epoch: 36800 | loss train: 280.9242858886719\n",
      "Epoch: 36900 | loss train: 339.595947265625\n",
      "Epoch: 37000 | loss train: 374.4451904296875\n",
      "Epoch: 37100 | loss train: 420.5064392089844\n",
      "Epoch: 37200 | loss train: 432.749267578125\n",
      "Epoch: 37300 | loss train: 436.9151916503906\n",
      "Epoch: 37400 | loss train: 323.5501403808594\n",
      "Epoch: 37500 | loss train: 263.7412109375\n",
      "Epoch: 37600 | loss train: 293.10797119140625\n",
      "Epoch: 37700 | loss train: 242.12725830078125\n",
      "Epoch: 37800 | loss train: 239.05970764160156\n",
      "Epoch: 37900 | loss train: 265.03204345703125\n",
      "Epoch: 38000 | loss train: 281.04901123046875\n",
      "Epoch: 38100 | loss train: 322.8297119140625\n",
      "Epoch: 38200 | loss train: 273.812255859375\n",
      "Epoch: 38300 | loss train: 229.52073669433594\n",
      "Epoch: 38400 | loss train: 408.6420593261719\n",
      "Epoch: 38500 | loss train: 271.6037902832031\n",
      "Epoch: 38600 | loss train: 218.1331787109375\n",
      "Epoch: 38700 | loss train: 223.76486206054688\n",
      "Epoch: 38800 | loss train: 220.43710327148438\n",
      "Epoch: 38900 | loss train: 230.58749389648438\n",
      "Epoch: 39000 | loss train: 274.687744140625\n",
      "Epoch: 39100 | loss train: 262.14508056640625\n",
      "Epoch: 39200 | loss train: 234.1796112060547\n",
      "Epoch: 39300 | loss train: 264.77423095703125\n",
      "Epoch: 39400 | loss train: 229.12796020507812\n",
      "Epoch: 39500 | loss train: 194.75433349609375\n",
      "Epoch: 39600 | loss train: 348.93621826171875\n",
      "Epoch: 39700 | loss train: 198.87782287597656\n",
      "Epoch: 39800 | loss train: 252.7899169921875\n",
      "Epoch: 39900 | loss train: 263.587890625\n",
      "loss_NN=tensor(4755.7300)\n",
      "pred_NN=tensor([[ 8231.2930],\n",
      "        [12612.8027],\n",
      "        [15672.2334],\n",
      "        [13282.1973]])\n",
      "loss_RF=3506.901809800899\n",
      "pred_RF=array([13650.52188477, 13940.47621094, 13296.97640625, 15601.44124023])\n",
      "loss_DT=3099.8191084691034\n",
      "pred_DT=array([11303.47851562, 12474.95019531, 10043.84179688, 13668.33691406])\n"
     ]
    }
   ],
   "source": [
    "sample = generate_sample(data_df_13_09_2022, bands_ix, target_value, train_field)\n",
    "X_train, Y_train = create_X_train_Y_train(sample, bands_ix)\n",
    "sample_target = generate_sample(data_df_13_09_2022, bands_ix, target_value, \"BC\")\n",
    "X_target, Y_target = create_X_train_Y_train(sample_target, bands_ix)\n",
    "super_param={\"lr\": 0.0001, \"weight_decay\": 1e-5, \"n_epochs\": 40000, \"stop_value\": 130}\n",
    "re_run = \"Y\"\n",
    "loss_NN, pred_NN, NN_model = predict_using_neutral_network(\n",
    "    X_train, \n",
    "    Y_train, \n",
    "    X_target, \n",
    "    Y_target, \n",
    "    bands_ix,\n",
    "    NN_file_path, \n",
    "    super_param, \n",
    "    re_run)\n",
    "print(f\"{loss_NN=}\")\n",
    "print(f\"{pred_NN=}\")\n",
    "loss_RF, pred_RF, RF_model = predict_using_random_forest(X_train, Y_train, X_target, Y_target, bands_ix, super_param)\n",
    "print(f\"{loss_RF=}\")\n",
    "print(f\"{pred_RF=}\")\n",
    "loss_DT, pred_DT, DT_model = predict_using_decision_tree(X_train, Y_train, X_target, Y_target, bands_ix, super_param)\n",
    "print(f\"{loss_DT=}\")\n",
    "print(f\"{pred_DT=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save NN object to: D:\\code\\model_saved\\NN_save\\NN_object\\T_predict_K_2024-03-07-22-56-00_using_get_max_bands.pkl\n",
      "Save RF object to: D:\\code\\model_saved\\RF_save\\T_predict_K_2024-03-07-22-56-00_using_get_max_bands.pkl\n",
      "Save DT object to: D:\\code\\model_saved\\DT_save\\T_predict_K_2024-03-07-22-56-00_using_get_max_bands.pkl\n"
     ]
    }
   ],
   "source": [
    "cloudpickle.register_pickle_by_value(common)\n",
    "\n",
    "with open(NN_file_path, \"wb\") as file:\n",
    "    cloudpickle.dump(NN_model, file)\n",
    "    print(f\"Save NN object to: {NN_file_path}\")\n",
    "with open(RF_file_path, \"wb\") as file:\n",
    "    cloudpickle.dump(RF_model, file)\n",
    "    print(f\"Save RF object to: {RF_file_path}\")\n",
    "with open(DT_file_path, \"wb\") as file:\n",
    "    cloudpickle.dump(DT_model, file)\n",
    "    print(f\"Save DT object to: {DT_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "avarage_target_value = np.average(Y_target)\n",
    "loss_NN = float(loss_NN)\n",
    "df_report = pd.DataFrame(\n",
    "    {\n",
    "        \"date_time\": [datetime.now()],\n",
    "        \"train_field\": [str(train_field)],\n",
    "        \"target_value\": [str(target_value)],\n",
    "        \"bands_ix\": [str(bands_ix)],\n",
    "        \"total_band\": [len(bands_ix)],\n",
    "        \"function_get\": [function_get.__name__],\n",
    "        \"loss_NN\": [loss_NN],\n",
    "        \"% loss_NN\": [loss_NN / avarage_target_value * 100],\n",
    "        \"loss_RF\": [loss_RF],\n",
    "        \"% loss_RF\": [loss_RF / avarage_target_value * 100],\n",
    "        \"loss_DT\": [loss_DT],\n",
    "        \"% loss_DT\": [loss_DT / avarage_target_value * 100],\n",
    "        \"average_target_value\": [avarage_target_value],\n",
    "        \"File save NN\": [NN_file_path],\n",
    "        \"File save RF\": [RF_file_path],\n",
    "        \"File save DT\": [DT_file_path],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_time</th>\n",
       "      <th>train_field</th>\n",
       "      <th>target_value</th>\n",
       "      <th>bands_ix</th>\n",
       "      <th>total_band</th>\n",
       "      <th>function_get</th>\n",
       "      <th>loss_NN</th>\n",
       "      <th>% loss_NN</th>\n",
       "      <th>loss_RF</th>\n",
       "      <th>% loss_RF</th>\n",
       "      <th>loss_DT</th>\n",
       "      <th>% loss_DT</th>\n",
       "      <th>average_target_value</th>\n",
       "      <th>File save NN</th>\n",
       "      <th>File save RF</th>\n",
       "      <th>File save DT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-03-07 23:02:15.481179</td>\n",
       "      <td>T</td>\n",
       "      <td>K</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>122</td>\n",
       "      <td>get_max_bands</td>\n",
       "      <td>4755.72998</td>\n",
       "      <td>37.707614</td>\n",
       "      <td>3506.90181</td>\n",
       "      <td>27.805805</td>\n",
       "      <td>3099.819108</td>\n",
       "      <td>24.578095</td>\n",
       "      <td>12612.121094</td>\n",
       "      <td>D:\\code\\model_saved\\NN_save\\NN_object\\T_predic...</td>\n",
       "      <td>D:\\code\\model_saved\\RF_save\\T_predict_K_2024-0...</td>\n",
       "      <td>D:\\code\\model_saved\\DT_save\\T_predict_K_2024-0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   date_time train_field target_value  \\\n",
       "0 2024-03-07 23:02:15.481179           T            K   \n",
       "\n",
       "                                            bands_ix  total_band  \\\n",
       "0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...         122   \n",
       "\n",
       "    function_get     loss_NN  % loss_NN     loss_RF  % loss_RF      loss_DT  \\\n",
       "0  get_max_bands  4755.72998  37.707614  3506.90181  27.805805  3099.819108   \n",
       "\n",
       "   % loss_DT  average_target_value  \\\n",
       "0  24.578095          12612.121094   \n",
       "\n",
       "                                        File save NN  \\\n",
       "0  D:\\code\\model_saved\\NN_save\\NN_object\\T_predic...   \n",
       "\n",
       "                                        File save RF  \\\n",
       "0  D:\\code\\model_saved\\RF_save\\T_predict_K_2024-0...   \n",
       "\n",
       "                                        File save DT  \n",
       "0  D:\\code\\model_saved\\DT_save\\T_predict_K_2024-0...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_df_to_excel(df: pd.DataFrame, file_path, sheet_name=\"Sheet1\"):\n",
    "    if not os.path.exists(file_path):\n",
    "        df.to_excel(\n",
    "            file_path,\n",
    "            sheet_name=sheet_name,\n",
    "            index=False,\n",
    "            header=True\n",
    "        )\n",
    "        return\n",
    "    \n",
    "    wb = load_workbook(file_path)\n",
    "    ws = wb[sheet_name]\n",
    "    for r in dataframe_to_rows(df, index=False, header=False):\n",
    "        ws.append(r)\n",
    "    wb.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: DATA IS WRITED TO FILE D:\\code\\report\\agriculture_report.xlsx\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    report_file_path = get_full_path(\"../../report/agriculture_report.xlsx\")\n",
    "    sheet_name = \"Sheet1\"\n",
    "\n",
    "    append_df_to_excel(df_report, report_file_path, sheet_name)\n",
    "    print(f\"SUCCESS: DATA IS WRITED TO FILE {report_file_path}\")\n",
    "except PermissionError as err:\n",
    "    print(f\"ERROR: YOU ARE OPENNING FILE {report_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

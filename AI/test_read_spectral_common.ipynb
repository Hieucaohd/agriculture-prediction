{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from common.read_spectral_common import (\n",
    "    data_df_13_09_2022, \n",
    "    calculate_mutual_info_for_all, \n",
    "    generate_sample, create_X_train_Y_train, \n",
    "    mutual_info_regression, \n",
    "    get_max_bands, \n",
    "    get_bands_ix_from_mutual_info, \n",
    "    get_average_bands, \n",
    "    get_max_bands, \n",
    "    get_min_bands,\n",
    "    predict_using_neutral_network, \n",
    "    predict_using_random_forest, \n",
    "    predict_using_decision_tree,\n",
    "    get_full_path,\n",
    "    load_sklearn_model_to_file_by_cloudpickle,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import cloudpickle\n",
    "import common\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_value = \"N\"\n",
    "train_field = [\"T\", \"J\"]\n",
    "function_get = get_max_bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(109, 0.266475167231325),\n",
       " (45, 0.196122954304089),\n",
       " (102, 0.15157344875394374),\n",
       " (21, 0.12335508977432275),\n",
       " (41, 0.11416500875691415),\n",
       " (93, 0.11012437169631761),\n",
       " (69, 0.10751346860020483),\n",
       " (24, 0.09428707544226489),\n",
       " (7, 0.09332524049104407),\n",
       " (63, 0.09158496679653805),\n",
       " (114, 0.09080279753805254),\n",
       " (96, 0.08926160096411273),\n",
       " (42, 0.08649622610630026),\n",
       " (106, 0.08215284543165557),\n",
       " (2, 0.07718416278583629),\n",
       " (12, 0.07123400334383456),\n",
       " (48, 0.06622563941400728),\n",
       " (37, 0.05957749549469726),\n",
       " (87, 0.058034064567277444),\n",
       " (53, 0.05525325130245262),\n",
       " (51, 0.05333506237671726),\n",
       " (23, 0.04664410309205991),\n",
       " (79, 0.042472031146980616),\n",
       " (98, 0.04115870796455834),\n",
       " (70, 0.04053388407511482),\n",
       " (35, 0.03791249713776956),\n",
       " (72, 0.03536779679315005),\n",
       " (57, 0.0348409780245591),\n",
       " (16, 0.03396583913210005),\n",
       " (18, 0.029266313625623486),\n",
       " (119, 0.027582159624412794),\n",
       " (4, 0.023798618055023812),\n",
       " (9, 0.023696899803736127),\n",
       " (28, 0.02034555660044024),\n",
       " (59, 0.019915797186019102),\n",
       " (97, 0.019414841428535112),\n",
       " (121, 0.017988809176209708),\n",
       " (82, 0.016336854189142436),\n",
       " (77, 0.015289794074952567),\n",
       " (10, 0.014001875585842427),\n",
       " (116, 0.01388888888888895),\n",
       " (120, 0.01388888888888895),\n",
       " (19, 0.013029716164933092),\n",
       " (117, 0.012141832476338488),\n",
       " (1, 0.012002583967759328),\n",
       " (112, 0.011714780772912103),\n",
       " (84, 0.011555552010832137),\n",
       " (90, 0.01139433997535999),\n",
       " (100, 0.011252133506498119),\n",
       " (111, 0.011001562155064981),\n",
       " (68, 0.01088553222141364),\n",
       " (99, 0.01075052045187519),\n",
       " (5, 0.009016040519413515),\n",
       " (73, 0.00649986792341295),\n",
       " (94, 0.003531673780794886),\n",
       " (104, 0.003003177424849568),\n",
       " (8, 0.00226954697513726),\n",
       " (105, 0.0022545922090912995),\n",
       " (25, 0.001642359549235195),\n",
       " (103, 0.0015654473924247014),\n",
       " (0, 0.0),\n",
       " (3, 0.0),\n",
       " (6, 0.0),\n",
       " (11, 0.0),\n",
       " (13, 0.0),\n",
       " (14, 0.0),\n",
       " (15, 0.0),\n",
       " (17, 0.0),\n",
       " (20, 0.0),\n",
       " (22, 0.0),\n",
       " (26, 0.0),\n",
       " (27, 0.0),\n",
       " (29, 0.0),\n",
       " (30, 0.0),\n",
       " (31, 0.0),\n",
       " (32, 0.0),\n",
       " (33, 0.0),\n",
       " (34, 0.0),\n",
       " (36, 0.0),\n",
       " (38, 0.0),\n",
       " (39, 0.0),\n",
       " (40, 0.0),\n",
       " (43, 0.0),\n",
       " (44, 0.0),\n",
       " (46, 0.0),\n",
       " (47, 0.0),\n",
       " (49, 0.0),\n",
       " (50, 0.0),\n",
       " (52, 0.0),\n",
       " (54, 0.0),\n",
       " (55, 0.0),\n",
       " (56, 0.0),\n",
       " (58, 0.0),\n",
       " (60, 0.0),\n",
       " (61, 0.0),\n",
       " (62, 0.0),\n",
       " (64, 0.0),\n",
       " (65, 0.0),\n",
       " (66, 0.0),\n",
       " (67, 0.0),\n",
       " (71, 0.0),\n",
       " (74, 0.0),\n",
       " (75, 0.0),\n",
       " (76, 0.0),\n",
       " (78, 0.0),\n",
       " (80, 0.0),\n",
       " (81, 0.0),\n",
       " (83, 0.0),\n",
       " (85, 0.0),\n",
       " (86, 0.0),\n",
       " (88, 0.0),\n",
       " (89, 0.0),\n",
       " (91, 0.0),\n",
       " (92, 0.0),\n",
       " (95, 0.0),\n",
       " (101, 0.0),\n",
       " (107, 0.0),\n",
       " (108, 0.0),\n",
       " (110, 0.0),\n",
       " (113, 0.0),\n",
       " (115, 0.0),\n",
       " (118, 0.0)]"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_mutual_info_for_all(data_df_13_09_2022, target_value, train_field, function_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_ix = get_bands_ix_from_mutual_info(data_df_13_09_2022, -1, target_value, train_field, function_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bands_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bands_ix = filter(lambda data: data < 100, bands_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_ix = list(bands_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121]"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bands_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bands_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | loss train: 5765.775390625\n",
      "Epoch: 100 | loss train: 5546.0078125\n",
      "Epoch: 200 | loss train: 2139.216552734375\n",
      "Epoch: 300 | loss train: 2137.769287109375\n",
      "Epoch: 400 | loss train: 2136.955810546875\n",
      "Epoch: 500 | loss train: 2136.01513671875\n",
      "Epoch: 600 | loss train: 2134.75830078125\n",
      "Epoch: 700 | loss train: 2133.018310546875\n",
      "Epoch: 800 | loss train: 2129.172119140625\n",
      "Epoch: 900 | loss train: 2122.7646484375\n",
      "Epoch: 1000 | loss train: 2109.422119140625\n",
      "Epoch: 1100 | loss train: 2084.91015625\n",
      "Epoch: 1200 | loss train: 2054.74755859375\n",
      "Epoch: 1300 | loss train: 2005.3902587890625\n",
      "Epoch: 1400 | loss train: 1949.9307861328125\n",
      "Epoch: 1500 | loss train: 1920.4351806640625\n",
      "Epoch: 1600 | loss train: 1894.9906005859375\n",
      "Epoch: 1700 | loss train: 1872.1358642578125\n",
      "Epoch: 1800 | loss train: 1852.30029296875\n",
      "Epoch: 1900 | loss train: 1831.8896484375\n",
      "Epoch: 2000 | loss train: 1814.079345703125\n",
      "Epoch: 2100 | loss train: 1796.51708984375\n",
      "Epoch: 2200 | loss train: 1771.2845458984375\n",
      "Epoch: 2300 | loss train: 1762.208251953125\n",
      "Epoch: 2400 | loss train: 1727.53369140625\n",
      "Epoch: 2500 | loss train: 1765.043701171875\n",
      "Epoch: 2600 | loss train: 1695.8311767578125\n",
      "Epoch: 2700 | loss train: 1673.862548828125\n",
      "Epoch: 2800 | loss train: 1651.8170166015625\n",
      "Epoch: 2900 | loss train: 1634.4127197265625\n",
      "Epoch: 3000 | loss train: 1617.835205078125\n",
      "Epoch: 3100 | loss train: 1603.0675048828125\n",
      "Epoch: 3200 | loss train: 1590.5467529296875\n",
      "Epoch: 3300 | loss train: 1573.005126953125\n",
      "Epoch: 3400 | loss train: 1558.541259765625\n",
      "Epoch: 3500 | loss train: 1550.2220458984375\n",
      "Epoch: 3600 | loss train: 1533.6033935546875\n",
      "Epoch: 3700 | loss train: 1520.327880859375\n",
      "Epoch: 3800 | loss train: 1512.23681640625\n",
      "Epoch: 3900 | loss train: 1496.9453125\n",
      "Epoch: 4000 | loss train: 1533.08447265625\n",
      "Epoch: 4100 | loss train: 1475.849365234375\n",
      "Epoch: 4200 | loss train: 1469.35009765625\n",
      "Epoch: 4300 | loss train: 1455.0694580078125\n",
      "Epoch: 4400 | loss train: 1451.5562744140625\n",
      "Epoch: 4500 | loss train: 1438.7030029296875\n",
      "Epoch: 4600 | loss train: 1427.0711669921875\n",
      "Epoch: 4700 | loss train: 1416.22216796875\n",
      "Epoch: 4800 | loss train: 1403.2913818359375\n",
      "Epoch: 4900 | loss train: 1403.989990234375\n",
      "Epoch: 5000 | loss train: 1386.496337890625\n",
      "Epoch: 5100 | loss train: 1385.7891845703125\n",
      "Epoch: 5200 | loss train: 1383.3157958984375\n",
      "Epoch: 5300 | loss train: 1390.8038330078125\n",
      "Epoch: 5400 | loss train: 1353.53857421875\n",
      "Epoch: 5500 | loss train: 1380.6953125\n",
      "Epoch: 5600 | loss train: 1370.003173828125\n",
      "Epoch: 5700 | loss train: 1337.049560546875\n",
      "Epoch: 5800 | loss train: 1367.55712890625\n",
      "Epoch: 5900 | loss train: 1344.8046875\n",
      "Epoch: 6000 | loss train: 1353.7767333984375\n",
      "Epoch: 6100 | loss train: 1350.37939453125\n",
      "Epoch: 6200 | loss train: 1309.617919921875\n",
      "Epoch: 6300 | loss train: 1295.7554931640625\n",
      "Epoch: 6400 | loss train: 1290.8013916015625\n",
      "Epoch: 6500 | loss train: 1326.220703125\n",
      "Epoch: 6600 | loss train: 1338.0562744140625\n",
      "Epoch: 6700 | loss train: 1267.409423828125\n",
      "Epoch: 6800 | loss train: 1276.8548583984375\n",
      "Epoch: 6900 | loss train: 1259.16162109375\n",
      "Epoch: 7000 | loss train: 1261.32861328125\n",
      "Epoch: 7100 | loss train: 1285.984375\n",
      "Epoch: 7200 | loss train: 1250.7413330078125\n",
      "Epoch: 7300 | loss train: 1276.3148193359375\n",
      "Epoch: 7400 | loss train: 1264.1287841796875\n",
      "Epoch: 7500 | loss train: 1253.8260498046875\n",
      "Epoch: 7600 | loss train: 1259.748291015625\n",
      "Epoch: 7700 | loss train: 1224.837646484375\n",
      "Epoch: 7800 | loss train: 1249.3572998046875\n",
      "Epoch: 7900 | loss train: 1206.7791748046875\n",
      "Epoch: 8000 | loss train: 1206.8038330078125\n",
      "Epoch: 8100 | loss train: 1197.4512939453125\n",
      "Epoch: 8200 | loss train: 1227.287841796875\n",
      "Epoch: 8300 | loss train: 1221.3544921875\n",
      "Epoch: 8400 | loss train: 1183.6851806640625\n",
      "Epoch: 8500 | loss train: 1213.9473876953125\n",
      "Epoch: 8600 | loss train: 1178.4405517578125\n",
      "Epoch: 8700 | loss train: 1180.292236328125\n",
      "Epoch: 8800 | loss train: 1185.8487548828125\n",
      "Epoch: 8900 | loss train: 1182.1055908203125\n",
      "Epoch: 9000 | loss train: 1172.0604248046875\n",
      "Epoch: 9100 | loss train: 1173.710693359375\n",
      "Epoch: 9200 | loss train: 1189.2161865234375\n",
      "Epoch: 9300 | loss train: 1158.505126953125\n",
      "Epoch: 9400 | loss train: 1147.571044921875\n",
      "Epoch: 9500 | loss train: 1184.2220458984375\n",
      "Epoch: 9600 | loss train: 1142.2613525390625\n",
      "Epoch: 9700 | loss train: 1180.13916015625\n",
      "Epoch: 9800 | loss train: 1130.2279052734375\n",
      "Epoch: 9900 | loss train: 1125.5838623046875\n",
      "Epoch: 10000 | loss train: 1186.9493408203125\n",
      "Epoch: 10100 | loss train: 1124.2210693359375\n",
      "Epoch: 10200 | loss train: 1123.482666015625\n",
      "Epoch: 10300 | loss train: 1121.720947265625\n",
      "Epoch: 10400 | loss train: 1108.828857421875\n",
      "Epoch: 10500 | loss train: 1136.6297607421875\n",
      "Epoch: 10600 | loss train: 1100.3873291015625\n",
      "Epoch: 10700 | loss train: 1161.017333984375\n",
      "Epoch: 10800 | loss train: 1145.730712890625\n",
      "Epoch: 10900 | loss train: 1145.9140625\n",
      "Epoch: 11000 | loss train: 1104.21630859375\n",
      "Epoch: 11100 | loss train: 1109.93896484375\n",
      "Epoch: 11200 | loss train: 1104.599365234375\n",
      "Epoch: 11300 | loss train: 1079.9700927734375\n",
      "Epoch: 11400 | loss train: 1077.9212646484375\n",
      "Epoch: 11500 | loss train: 1095.4505615234375\n",
      "Epoch: 11600 | loss train: 1117.5269775390625\n",
      "Epoch: 11700 | loss train: 1100.162109375\n",
      "Epoch: 11800 | loss train: 1068.8870849609375\n",
      "Epoch: 11900 | loss train: 1071.301513671875\n",
      "Epoch: 12000 | loss train: 1118.7657470703125\n",
      "Epoch: 12100 | loss train: 1051.1163330078125\n",
      "Epoch: 12200 | loss train: 1039.9837646484375\n",
      "Epoch: 12300 | loss train: 1035.2938232421875\n",
      "Epoch: 12400 | loss train: 1097.4891357421875\n",
      "Epoch: 12500 | loss train: 1046.716796875\n",
      "Epoch: 12600 | loss train: 1084.335693359375\n",
      "Epoch: 12700 | loss train: 1058.0345458984375\n",
      "Epoch: 12800 | loss train: 1057.3062744140625\n",
      "Epoch: 12900 | loss train: 1019.769775390625\n",
      "Epoch: 13000 | loss train: 1056.40625\n",
      "Epoch: 13100 | loss train: 1082.0472412109375\n",
      "Epoch: 13200 | loss train: 995.6517944335938\n",
      "Epoch: 13300 | loss train: 1021.9697265625\n",
      "Epoch: 13400 | loss train: 1010.0535888671875\n",
      "Epoch: 13500 | loss train: 1054.0684814453125\n",
      "Epoch: 13600 | loss train: 1014.0791625976562\n",
      "Epoch: 13700 | loss train: 1011.7559204101562\n",
      "Epoch: 13800 | loss train: 1009.3829345703125\n",
      "Epoch: 13900 | loss train: 990.3167724609375\n",
      "Epoch: 14000 | loss train: 963.351806640625\n",
      "Epoch: 14100 | loss train: 1002.9265747070312\n",
      "Epoch: 14200 | loss train: 961.4550170898438\n",
      "Epoch: 14300 | loss train: 955.8395385742188\n",
      "Epoch: 14400 | loss train: 964.4439697265625\n",
      "Epoch: 14500 | loss train: 1007.10888671875\n",
      "Epoch: 14600 | loss train: 1002.3675537109375\n",
      "Epoch: 14700 | loss train: 1066.651123046875\n",
      "Epoch: 14800 | loss train: 978.5722045898438\n",
      "Epoch: 14900 | loss train: 929.085693359375\n",
      "Epoch: 15000 | loss train: 941.86181640625\n",
      "Epoch: 15100 | loss train: 930.2399291992188\n",
      "Epoch: 15200 | loss train: 923.7855834960938\n",
      "Epoch: 15300 | loss train: 939.162109375\n",
      "Epoch: 15400 | loss train: 908.5258178710938\n",
      "Epoch: 15500 | loss train: 1009.7369995117188\n",
      "Epoch: 15600 | loss train: 942.2753295898438\n",
      "Epoch: 15700 | loss train: 953.7228393554688\n",
      "Epoch: 15800 | loss train: 919.2930297851562\n",
      "Epoch: 15900 | loss train: 985.4566040039062\n",
      "Epoch: 16000 | loss train: 890.1129760742188\n",
      "Epoch: 16100 | loss train: 976.2437133789062\n",
      "Epoch: 16200 | loss train: 866.263916015625\n",
      "Epoch: 16300 | loss train: 975.8567504882812\n",
      "Epoch: 16400 | loss train: 922.73291015625\n",
      "Epoch: 16500 | loss train: 946.2512817382812\n",
      "Epoch: 16600 | loss train: 937.06005859375\n",
      "Epoch: 16700 | loss train: 956.415771484375\n",
      "Epoch: 16800 | loss train: 906.6683349609375\n",
      "Epoch: 16900 | loss train: 835.2772216796875\n",
      "Epoch: 17000 | loss train: 829.7701416015625\n",
      "Epoch: 17100 | loss train: 934.279541015625\n",
      "Epoch: 17200 | loss train: 821.1936645507812\n",
      "Epoch: 17300 | loss train: 911.1710205078125\n",
      "Epoch: 17400 | loss train: 948.6475830078125\n",
      "Epoch: 17500 | loss train: 872.0801391601562\n",
      "Epoch: 17600 | loss train: 953.5671997070312\n",
      "Epoch: 17700 | loss train: 877.639892578125\n",
      "Epoch: 17800 | loss train: 801.4896850585938\n",
      "Epoch: 17900 | loss train: 829.150634765625\n",
      "Epoch: 18000 | loss train: 901.5021362304688\n",
      "Epoch: 18100 | loss train: 794.730712890625\n",
      "Epoch: 18200 | loss train: 781.5721435546875\n",
      "Epoch: 18300 | loss train: 777.2581787109375\n",
      "Epoch: 18400 | loss train: 784.264404296875\n",
      "Epoch: 18500 | loss train: 808.9503173828125\n",
      "Epoch: 18600 | loss train: 822.4075317382812\n",
      "Epoch: 18700 | loss train: 822.57763671875\n",
      "Epoch: 18800 | loss train: 764.6640625\n",
      "Epoch: 18900 | loss train: 766.9002685546875\n",
      "Epoch: 19000 | loss train: 757.0166625976562\n",
      "Epoch: 19100 | loss train: 779.235595703125\n",
      "Epoch: 19200 | loss train: 802.7030639648438\n",
      "Epoch: 19300 | loss train: 752.88671875\n",
      "Epoch: 19400 | loss train: 837.6063842773438\n",
      "Epoch: 19500 | loss train: 746.2312622070312\n",
      "Epoch: 19600 | loss train: 826.7586059570312\n",
      "Epoch: 19700 | loss train: 723.2357788085938\n",
      "Epoch: 19800 | loss train: 781.0149536132812\n",
      "Epoch: 19900 | loss train: 874.6439208984375\n",
      "Epoch: 20000 | loss train: 715.435791015625\n",
      "Epoch: 20100 | loss train: 850.947509765625\n",
      "Epoch: 20200 | loss train: 729.3241577148438\n",
      "Epoch: 20300 | loss train: 764.32958984375\n",
      "Epoch: 20400 | loss train: 862.6863403320312\n",
      "Epoch: 20500 | loss train: 867.8991088867188\n",
      "Epoch: 20600 | loss train: 816.2581787109375\n",
      "Epoch: 20700 | loss train: 765.072021484375\n",
      "Epoch: 20800 | loss train: 681.4706420898438\n",
      "Epoch: 20900 | loss train: 705.2614135742188\n",
      "Epoch: 21000 | loss train: 696.4238891601562\n",
      "Epoch: 21100 | loss train: 684.2987670898438\n",
      "Epoch: 21200 | loss train: 766.9759521484375\n",
      "Epoch: 21300 | loss train: 701.9974365234375\n",
      "Epoch: 21400 | loss train: 657.8381958007812\n",
      "Epoch: 21500 | loss train: 658.341796875\n",
      "Epoch: 21600 | loss train: 646.4036254882812\n",
      "Epoch: 21700 | loss train: 689.9611206054688\n",
      "Epoch: 21800 | loss train: 641.5881958007812\n",
      "Epoch: 21900 | loss train: 953.1177368164062\n",
      "Epoch: 22000 | loss train: 792.8473510742188\n",
      "Epoch: 22100 | loss train: 631.105224609375\n",
      "Epoch: 22200 | loss train: 762.1126098632812\n",
      "Epoch: 22300 | loss train: 789.87255859375\n",
      "Epoch: 22400 | loss train: 791.2444458007812\n",
      "Epoch: 22500 | loss train: 620.456298828125\n",
      "Epoch: 22600 | loss train: 613.3130493164062\n",
      "Epoch: 22700 | loss train: 691.8225708007812\n",
      "Epoch: 22800 | loss train: 898.2472534179688\n",
      "Epoch: 22900 | loss train: 705.228515625\n",
      "Epoch: 23000 | loss train: 712.863037109375\n",
      "Epoch: 23100 | loss train: 669.5237426757812\n",
      "Epoch: 23200 | loss train: 591.3496704101562\n",
      "Epoch: 23300 | loss train: 709.62060546875\n",
      "Epoch: 23400 | loss train: 573.395263671875\n",
      "Epoch: 23500 | loss train: 689.31982421875\n",
      "Epoch: 23600 | loss train: 1176.105712890625\n",
      "Epoch: 23700 | loss train: 797.0518188476562\n",
      "Epoch: 23800 | loss train: 605.9816284179688\n",
      "Epoch: 23900 | loss train: 664.115234375\n",
      "Epoch: 24000 | loss train: 583.1805419921875\n",
      "Epoch: 24100 | loss train: 583.2169189453125\n",
      "Epoch: 24200 | loss train: 978.8046264648438\n",
      "Epoch: 24300 | loss train: 550.0316162109375\n",
      "Epoch: 24400 | loss train: 630.4188842773438\n",
      "Epoch: 24500 | loss train: 532.9382934570312\n",
      "Epoch: 24600 | loss train: 667.167724609375\n",
      "Epoch: 24700 | loss train: 1017.2384033203125\n",
      "Epoch: 24800 | loss train: 518.1366577148438\n",
      "Epoch: 24900 | loss train: 598.4371948242188\n",
      "Epoch: 25000 | loss train: 518.5233764648438\n",
      "Epoch: 25100 | loss train: 613.4241943359375\n",
      "Epoch: 25200 | loss train: 506.3259582519531\n",
      "Epoch: 25300 | loss train: 673.1761474609375\n",
      "Epoch: 25400 | loss train: 530.2048950195312\n",
      "Epoch: 25500 | loss train: 505.1855773925781\n",
      "Epoch: 25600 | loss train: 498.801513671875\n",
      "Epoch: 25700 | loss train: 496.9952697753906\n",
      "Epoch: 25800 | loss train: 707.615478515625\n",
      "Epoch: 25900 | loss train: 491.0505676269531\n",
      "Epoch: 26000 | loss train: 627.7042846679688\n",
      "Epoch: 26100 | loss train: 1331.8226318359375\n",
      "Epoch: 26200 | loss train: 561.7789916992188\n",
      "Epoch: 26300 | loss train: 581.0921630859375\n",
      "Epoch: 26400 | loss train: 579.96435546875\n",
      "Epoch: 26500 | loss train: 936.2476806640625\n",
      "Epoch: 26600 | loss train: 464.6056213378906\n",
      "Epoch: 26700 | loss train: 485.7742614746094\n",
      "Epoch: 26800 | loss train: 485.4219055175781\n",
      "Epoch: 26900 | loss train: 692.3687133789062\n",
      "Epoch: 27000 | loss train: 563.7408447265625\n",
      "Epoch: 27100 | loss train: 620.0890502929688\n",
      "Epoch: 27200 | loss train: 487.88330078125\n",
      "Epoch: 27300 | loss train: 559.8009033203125\n",
      "Epoch: 27400 | loss train: 440.2043151855469\n",
      "Epoch: 27500 | loss train: 608.3980102539062\n",
      "Epoch: 27600 | loss train: 543.3609008789062\n",
      "Epoch: 27700 | loss train: 499.5308532714844\n",
      "Epoch: 27800 | loss train: 466.19525146484375\n",
      "Epoch: 27900 | loss train: 471.9163818359375\n",
      "Epoch: 28000 | loss train: 441.48797607421875\n",
      "Epoch: 28100 | loss train: 875.5540771484375\n",
      "Epoch: 28200 | loss train: 522.5113525390625\n",
      "Epoch: 28300 | loss train: 431.1258544921875\n",
      "Epoch: 28400 | loss train: 535.1148681640625\n",
      "Epoch: 28500 | loss train: 453.775146484375\n",
      "Epoch: 28600 | loss train: 405.8431396484375\n",
      "Epoch: 28700 | loss train: 407.505126953125\n",
      "Epoch: 28800 | loss train: 584.392333984375\n",
      "Epoch: 28900 | loss train: 416.104736328125\n",
      "Epoch: 29000 | loss train: 526.7139892578125\n",
      "Epoch: 29100 | loss train: 395.6940002441406\n",
      "Epoch: 29200 | loss train: 387.2702941894531\n",
      "Epoch: 29300 | loss train: 474.950927734375\n",
      "Epoch: 29400 | loss train: 408.3425598144531\n",
      "Epoch: 29500 | loss train: 420.92022705078125\n",
      "Epoch: 29600 | loss train: 459.18634033203125\n",
      "Epoch: 29700 | loss train: 453.0549011230469\n",
      "Epoch: 29800 | loss train: 382.0904541015625\n",
      "Epoch: 29900 | loss train: 372.9809875488281\n",
      "Epoch: 30000 | loss train: 888.387939453125\n",
      "Epoch: 30100 | loss train: 548.1071166992188\n",
      "Epoch: 30200 | loss train: 410.6452941894531\n",
      "Epoch: 30300 | loss train: 369.43853759765625\n",
      "Epoch: 30400 | loss train: 360.4151611328125\n",
      "Epoch: 30500 | loss train: 601.1487426757812\n",
      "Epoch: 30600 | loss train: 442.03118896484375\n",
      "Epoch: 30700 | loss train: 417.5691223144531\n",
      "Epoch: 30800 | loss train: 622.1822509765625\n",
      "Epoch: 30900 | loss train: 353.7128601074219\n",
      "Epoch: 31000 | loss train: 359.8957824707031\n",
      "Epoch: 31100 | loss train: 349.5184326171875\n",
      "Epoch: 31200 | loss train: 359.9420471191406\n",
      "Epoch: 31300 | loss train: 1289.97509765625\n",
      "Epoch: 31400 | loss train: 1074.236083984375\n",
      "Epoch: 31500 | loss train: 351.8752746582031\n",
      "Epoch: 31600 | loss train: 409.3056640625\n",
      "Epoch: 31700 | loss train: 360.8422546386719\n",
      "Epoch: 31800 | loss train: 312.8448486328125\n",
      "Epoch: 31900 | loss train: 454.1371154785156\n",
      "Epoch: 32000 | loss train: 603.6408081054688\n",
      "Epoch: 32100 | loss train: 498.21734619140625\n",
      "Epoch: 32200 | loss train: 329.47540283203125\n",
      "Epoch: 32300 | loss train: 324.9157409667969\n",
      "Epoch: 32400 | loss train: 343.7015686035156\n",
      "Epoch: 32500 | loss train: 423.0750732421875\n",
      "Epoch: 32600 | loss train: 549.6398315429688\n",
      "Epoch: 32700 | loss train: 934.1400146484375\n",
      "Epoch: 32800 | loss train: 396.8843078613281\n",
      "Epoch: 32900 | loss train: 418.56549072265625\n",
      "Epoch: 33000 | loss train: 365.5975036621094\n",
      "Epoch: 33100 | loss train: 360.7028503417969\n",
      "Epoch: 33200 | loss train: 507.29034423828125\n",
      "Epoch: 33300 | loss train: 295.9552307128906\n",
      "Epoch: 33400 | loss train: 282.8103942871094\n",
      "Epoch: 33500 | loss train: 249.2517852783203\n",
      "Epoch: 33600 | loss train: 363.9583740234375\n",
      "Epoch: 33700 | loss train: 800.3748168945312\n",
      "Epoch: 33800 | loss train: 434.0497131347656\n",
      "Epoch: 33900 | loss train: 253.84478759765625\n",
      "Epoch: 34000 | loss train: 1116.0521240234375\n",
      "Epoch: 34100 | loss train: 228.90621948242188\n",
      "Epoch: 34200 | loss train: 412.3646240234375\n",
      "Epoch: 34300 | loss train: 303.6781311035156\n",
      "Epoch: 34400 | loss train: 890.6094360351562\n",
      "Epoch: 34500 | loss train: 223.6634063720703\n",
      "Epoch: 34600 | loss train: 323.394287109375\n",
      "Epoch: 34700 | loss train: 248.57211303710938\n",
      "Epoch: 34800 | loss train: 426.9541015625\n",
      "Epoch: 34900 | loss train: 601.6793823242188\n",
      "Epoch: 35000 | loss train: 216.04452514648438\n",
      "Epoch: 35100 | loss train: 229.15957641601562\n",
      "Epoch: 35200 | loss train: 294.8327941894531\n",
      "Epoch: 35300 | loss train: 207.18711853027344\n",
      "Epoch: 35400 | loss train: 395.7608947753906\n",
      "Epoch: 35500 | loss train: 1253.0172119140625\n",
      "Epoch: 35600 | loss train: 206.33139038085938\n",
      "Epoch: 35700 | loss train: 223.73019409179688\n",
      "Epoch: 35800 | loss train: 385.7616882324219\n",
      "Epoch: 35900 | loss train: 318.29290771484375\n",
      "Epoch: 36000 | loss train: 214.1728973388672\n",
      "Epoch: 36100 | loss train: 318.4496154785156\n",
      "Epoch: 36200 | loss train: 302.967529296875\n",
      "Epoch: 36300 | loss train: 238.9073486328125\n",
      "Epoch: 36400 | loss train: 348.8128967285156\n",
      "Epoch: 36500 | loss train: 408.70849609375\n",
      "Epoch: 36600 | loss train: 1188.802734375\n",
      "Epoch: 36700 | loss train: 204.36114501953125\n",
      "Epoch: 36800 | loss train: 195.80032348632812\n",
      "Epoch: 36900 | loss train: 814.9216918945312\n",
      "Epoch: 37000 | loss train: 194.3251190185547\n",
      "Epoch: 37100 | loss train: 218.6351318359375\n",
      "Epoch: 37200 | loss train: 774.5342407226562\n",
      "Epoch: 37300 | loss train: 186.819091796875\n",
      "Epoch: 37400 | loss train: 193.49684143066406\n",
      "Epoch: 37500 | loss train: 753.897705078125\n",
      "Epoch: 37600 | loss train: 201.26039123535156\n",
      "Epoch: 37700 | loss train: 186.6158447265625\n",
      "Epoch: 37800 | loss train: 869.5597534179688\n",
      "Epoch: 37900 | loss train: 189.17506408691406\n",
      "Epoch: 38000 | loss train: 197.8969268798828\n",
      "Epoch: 38100 | loss train: 443.2090148925781\n",
      "Epoch: 38200 | loss train: 264.5487365722656\n",
      "Epoch: 38300 | loss train: 174.23065185546875\n",
      "Epoch: 38400 | loss train: 354.3122253417969\n",
      "Epoch: 38500 | loss train: 209.0050506591797\n",
      "Epoch: 38600 | loss train: 246.4407958984375\n",
      "Epoch: 38700 | loss train: 619.9849243164062\n",
      "Epoch: 38800 | loss train: 327.09039306640625\n",
      "Epoch: 38900 | loss train: 163.3460235595703\n",
      "Epoch: 39000 | loss train: 176.71298217773438\n",
      "Epoch: 39100 | loss train: 210.67005920410156\n",
      "Epoch: 39200 | loss train: 178.27496337890625\n",
      "Epoch: 39300 | loss train: 164.4770050048828\n",
      "Epoch: 39400 | loss train: 542.3067626953125\n",
      "Epoch: 39500 | loss train: 163.7131805419922\n",
      "Epoch: 39600 | loss train: 374.12725830078125\n",
      "Epoch: 39700 | loss train: 202.5863037109375\n",
      "Epoch: 39800 | loss train: 399.1888122558594\n",
      "Epoch: 39900 | loss train: 156.1723175048828\n",
      "loss_NN=tensor(1021.2430)\n",
      "pred_NN=tensor([[2599.3020],\n",
      "        [1902.0200],\n",
      "        [4007.8677],\n",
      "        [3527.3086]])\n",
      "loss_RF=2126.999543772726\n",
      "pred_RF=array([4685.94958008, 4768.10935669, 4788.89066284, 5052.86958862])\n",
      "loss_DT=2742.3098798748456\n",
      "pred_DT=array([4121.73583984, 1875.54492188, 2551.82226562, 7411.33251953])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sample = generate_sample(data_df_13_09_2022, bands_ix, target_value, train_field)\n",
    "X_train, Y_train = create_X_train_Y_train(sample, bands_ix)\n",
    "sample_target = generate_sample(data_df_13_09_2022, bands_ix, target_value, \"BC\")\n",
    "X_target, Y_target = create_X_train_Y_train(sample_target, bands_ix)\n",
    "super_param={\"lr\": 0.0001, \"weight_decay\": 1e-5, \"n_epochs\": 40000, \"stop_value\": 130}\n",
    "re_run = \"Y\"\n",
    "loss_NN, pred_NN, NN_model = predict_using_neutral_network(\n",
    "    X_train, \n",
    "    Y_train, \n",
    "    X_target, \n",
    "    Y_target, \n",
    "    bands_ix,\n",
    "    get_full_path(f\"../../model_saved/NN_save/NN_object/{'_'.join(train_field)}_predict_{'_'.join(target_value)}_{str(datetime.now().date())}_using_{function_get.__name__}.pkl\"), \n",
    "    super_param, \n",
    "    re_run)\n",
    "print(f\"{loss_NN=}\")\n",
    "print(f\"{pred_NN=}\")\n",
    "loss_RF, pred_RF, RF_model = predict_using_random_forest(X_train, Y_train, X_target, Y_target, bands_ix, super_param)\n",
    "print(f\"{loss_RF=}\")\n",
    "print(f\"{pred_RF=}\")\n",
    "loss_DT, pred_DT, DT_model = predict_using_decision_tree(X_train, Y_train, X_target, Y_target, bands_ix, super_param)\n",
    "print(f\"{loss_DT=}\")\n",
    "print(f\"{pred_DT=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save NN object to: D:\\code\\model_saved\\NN_save\\NN_object\\T_J_predict_N_2024-03-02-22-54-37_using_get_max_bands.pkl\n",
      "Save RF object to: D:\\code\\model_saved\\RF_save\\T_J_predict_N_2024-03-02-22-54-37_using_get_max_bands.pkl\n",
      "Save DT object to: D:\\code\\model_saved\\DT_save\\T_J_predict_N_2024-03-02-22-54-37_using_get_max_bands.pkl\n"
     ]
    }
   ],
   "source": [
    "cloudpickle.register_pickle_by_value(common)\n",
    "date_now = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "NN_file_path = get_full_path(f\"../../model_saved/NN_save/NN_object/{'_'.join(train_field)}_predict_{'_'.join(target_value)}_{date_now}_using_{function_get.__name__}.pkl\")\n",
    "RF_file_path = get_full_path(f\"../../model_saved/RF_save/{'_'.join(train_field)}_predict_{'_'.join(target_value)}_{date_now}_using_{function_get.__name__}.pkl\")\n",
    "DT_file_path = get_full_path(f\"../../model_saved/DT_save/{'_'.join(train_field)}_predict_{'_'.join(target_value)}_{date_now}_using_{function_get.__name__}.pkl\")\n",
    "    \n",
    "\n",
    "with open(NN_file_path, \"wb\") as file:\n",
    "    cloudpickle.dump(NN_model, file)\n",
    "    print(f\"Save NN object to: {NN_file_path}\")\n",
    "with open(RF_file_path, \"wb\") as file:\n",
    "    cloudpickle.dump(RF_model, file)\n",
    "    print(f\"Save RF object to: {RF_file_path}\")\n",
    "with open(DT_file_path, \"wb\") as file:\n",
    "    cloudpickle.dump(DT_model, file)\n",
    "    print(f\"Save DT object to: {DT_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "avarage_target_value = np.average(Y_target)\n",
    "loss_NN = float(loss_NN)\n",
    "df_report = pd.DataFrame(\n",
    "    {\n",
    "        \"date_time\": [datetime.now()],\n",
    "        \"train_field\": [str(train_field)],\n",
    "        \"target_value\": [str(target_value)],\n",
    "        \"bands_ix\": [str(bands_ix)],\n",
    "        \"total_band\": [len(bands_ix)],\n",
    "        \"function_get\": [function_get.__name__],\n",
    "        \"loss_NN\": [loss_NN],\n",
    "        \"% loss_NN\": [loss_NN / avarage_target_value * 100],\n",
    "        \"loss_RF\": [loss_RF],\n",
    "        \"% loss_RF\": [loss_RF / avarage_target_value * 100],\n",
    "        \"loss_DT\": [loss_DT],\n",
    "        \"% loss_DT\": [loss_DT / avarage_target_value * 100],\n",
    "        \"average_target_value\": [avarage_target_value],\n",
    "        \"File save NN\": [NN_file_path],\n",
    "        \"File save RF\": [RF_file_path],\n",
    "        \"File save DT\": [DT_file_path],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2743.865"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avarage_target_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_time</th>\n",
       "      <th>train_field</th>\n",
       "      <th>target_value</th>\n",
       "      <th>bands_ix</th>\n",
       "      <th>total_band</th>\n",
       "      <th>function_get</th>\n",
       "      <th>loss_NN</th>\n",
       "      <th>% loss_NN</th>\n",
       "      <th>loss_RF</th>\n",
       "      <th>% loss_RF</th>\n",
       "      <th>loss_DT</th>\n",
       "      <th>% loss_DT</th>\n",
       "      <th>average_target_value</th>\n",
       "      <th>File save NN</th>\n",
       "      <th>File save RF</th>\n",
       "      <th>File save DT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-03-02 22:54:37.207856</td>\n",
       "      <td>['T', 'J']</td>\n",
       "      <td>N</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>122</td>\n",
       "      <td>get_max_bands</td>\n",
       "      <td>1021.242981</td>\n",
       "      <td>37.219141</td>\n",
       "      <td>2126.999544</td>\n",
       "      <td>77.518375</td>\n",
       "      <td>2742.30988</td>\n",
       "      <td>99.943324</td>\n",
       "      <td>2743.86499</td>\n",
       "      <td>D:\\code\\model_saved\\NN_save\\NN_object\\T_J_pred...</td>\n",
       "      <td>D:\\code\\model_saved\\RF_save\\T_J_predict_N_2024...</td>\n",
       "      <td>D:\\code\\model_saved\\DT_save\\T_J_predict_N_2024...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   date_time train_field target_value  \\\n",
       "0 2024-03-02 22:54:37.207856  ['T', 'J']            N   \n",
       "\n",
       "                                            bands_ix  total_band  \\\n",
       "0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...         122   \n",
       "\n",
       "    function_get      loss_NN  % loss_NN      loss_RF  % loss_RF     loss_DT  \\\n",
       "0  get_max_bands  1021.242981  37.219141  2126.999544  77.518375  2742.30988   \n",
       "\n",
       "   % loss_DT  average_target_value  \\\n",
       "0  99.943324            2743.86499   \n",
       "\n",
       "                                        File save NN  \\\n",
       "0  D:\\code\\model_saved\\NN_save\\NN_object\\T_J_pred...   \n",
       "\n",
       "                                        File save RF  \\\n",
       "0  D:\\code\\model_saved\\RF_save\\T_J_predict_N_2024...   \n",
       "\n",
       "                                        File save DT  \n",
       "0  D:\\code\\model_saved\\DT_save\\T_J_predict_N_2024...  "
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_df_to_excel(df: pd.DataFrame, file_path, sheet_name=\"Sheet1\"):\n",
    "    if not os.path.exists(file_path):\n",
    "        df.to_excel(\n",
    "            file_path,\n",
    "            sheet_name=sheet_name,\n",
    "            index=False,\n",
    "            header=True\n",
    "        )\n",
    "        return\n",
    "    \n",
    "    wb = load_workbook(file_path)\n",
    "    ws = wb[sheet_name]\n",
    "    for r in dataframe_to_rows(df, index=False, header=False):\n",
    "        ws.append(r)\n",
    "    wb.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: DATA IS WRITED TO FILE D:\\code\\report\\agriculture_report.xlsx\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    report_file_path = get_full_path(\"../../report/agriculture_report.xlsx\")\n",
    "    sheet_name = \"Sheet1\"\n",
    "\n",
    "    append_df_to_excel(df_report, report_file_path, sheet_name)\n",
    "    print(f\"SUCCESS: DATA IS WRITED TO FILE {report_file_path}\")\n",
    "except PermissionError as err:\n",
    "    print(f\"ERROR: YOU ARE OPENNING FILE {report_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

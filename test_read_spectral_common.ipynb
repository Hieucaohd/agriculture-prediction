{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Unable to locate file \"hyper_20220913_3cm.hdr\". If the file exists, use its full path or place its directory in the SPECTRAL_DATA environment variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mread_spectral_common\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     data_df_13_09_2022, \n\u001b[0;32m      3\u001b[0m     calculate_matual_info_for_all, \n\u001b[0;32m      4\u001b[0m     generate_sample, create_X_train_Y_train, \n\u001b[0;32m      5\u001b[0m     mutual_info_regression, \n\u001b[0;32m      6\u001b[0m     get_max_bands, \n\u001b[0;32m      7\u001b[0m     get_bands_ix_from_mutual_info, \n\u001b[0;32m      8\u001b[0m     get_average_bands, \n\u001b[0;32m      9\u001b[0m     get_max_bands, \n\u001b[0;32m     10\u001b[0m     predict_using_neutral_network, \n\u001b[0;32m     11\u001b[0m     predict_using_random_forest, \n\u001b[0;32m     12\u001b[0m     predict_using_decision_tree\n\u001b[0;32m     13\u001b[0m )\n",
      "File \u001b[1;32md:\\code\\agriculture-prediction\\read_spectral_common.py:34\u001b[0m\n\u001b[0;32m     21\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# BAND_START_IX = 4\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# BAND_END_IX = 101\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# mở file .hdr và file .img\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43menvi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhyper_20220913_3cm.hdr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhyper_20220913_3cm.img\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_bands\u001b[39m(row_num, col_num):\n",
      "File \u001b[1;32md:\\code\\agriculture-prediction\\venv\\lib\\site-packages\\spectral\\io\\envi.py:289\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(file, image)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen\u001b[39m(file, image\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    258\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;124;03m    Opens an image or spectral library with an associated ENVI HDR header file.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;124;03m    Capitalized versions of the file extensions are also searched.\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m     header_path \u001b[38;5;241m=\u001b[39m \u001b[43mfind_file_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    290\u001b[0m     h \u001b[38;5;241m=\u001b[39m read_envi_header(header_path)\n\u001b[0;32m    291\u001b[0m     check_compatibility(h)\n",
      "File \u001b[1;32md:\\code\\agriculture-prediction\\venv\\lib\\site-packages\\spectral\\io\\spyfile.py:120\u001b[0m, in \u001b[0;36mfind_file_path\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pathname:\n\u001b[0;32m    117\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnable to locate file \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. If the file exists, \u001b[39m\u001b[38;5;124m'\u001b[39m \\\n\u001b[0;32m    118\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse its full path or place its directory in the \u001b[39m\u001b[38;5;124m'\u001b[39m \\\n\u001b[0;32m    119\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSPECTRAL_DATA environment variable.\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;241m%\u001b[39m filename\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(msg)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pathname\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Unable to locate file \"hyper_20220913_3cm.hdr\". If the file exists, use its full path or place its directory in the SPECTRAL_DATA environment variable."
     ]
    }
   ],
   "source": [
    "from read_spectral_common import (\n",
    "    data_df_13_09_2022, \n",
    "    calculate_matual_info_for_all, \n",
    "    generate_sample, create_X_train_Y_train, \n",
    "    mutual_info_regression, \n",
    "    get_max_bands, \n",
    "    get_bands_ix_from_mutual_info, \n",
    "    get_average_bands, \n",
    "    get_max_bands, \n",
    "    predict_using_neutral_network, \n",
    "    predict_using_random_forest, \n",
    "    predict_using_decision_tree\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(102, 0.38958809060987676),\n",
       " (45, 0.25365831624099044),\n",
       " (109, 0.22994459565549397),\n",
       " (114, 0.2294360654447467),\n",
       " (96, 0.21808856274405342),\n",
       " (37, 0.187844876692854),\n",
       " (68, 0.18617920559721446),\n",
       " (33, 0.18406235536255489),\n",
       " (28, 0.16962294367727093),\n",
       " (63, 0.15653445205098349),\n",
       " (98, 0.150444828936104),\n",
       " (106, 0.13814011649313862),\n",
       " (92, 0.13764876172933782),\n",
       " (79, 0.13046753237554132),\n",
       " (9, 0.1207026074186528),\n",
       " (84, 0.11472519765487865),\n",
       " (21, 0.10914336561190874),\n",
       " (88, 0.1086625956925178),\n",
       " (82, 0.10660642313066049),\n",
       " (19, 0.10628314474670608),\n",
       " (90, 0.10410564968110902),\n",
       " (50, 0.10240754860477308),\n",
       " (104, 0.10132894461511999),\n",
       " (48, 0.10094177681156902),\n",
       " (100, 0.09798367826523524),\n",
       " (24, 0.09242404369092894),\n",
       " (70, 0.08984510098070331),\n",
       " (55, 0.08405842103974503),\n",
       " (26, 0.0834967698330984),\n",
       " (80, 0.08188890424367035),\n",
       " (93, 0.07852335126188903),\n",
       " (41, 0.07384532430643365),\n",
       " (59, 0.07118155746198607),\n",
       " (103, 0.0687250059439024),\n",
       " (69, 0.06839141384432423),\n",
       " (42, 0.06665787722988714),\n",
       " (43, 0.059284599995723664),\n",
       " (108, 0.05697422268576613),\n",
       " (58, 0.05508709213722529),\n",
       " (18, 0.05295924234192073),\n",
       " (61, 0.05248026475698664),\n",
       " (5, 0.051744929156034125),\n",
       " (2, 0.05135620806064356),\n",
       " (16, 0.0426747039544888),\n",
       " (95, 0.04151146795315519),\n",
       " (113, 0.04036988404817965),\n",
       " (47, 0.038122754580566465),\n",
       " (54, 0.036362010075789986),\n",
       " (7, 0.033199981116341704),\n",
       " (105, 0.03293908667545953),\n",
       " (40, 0.03224058814305941),\n",
       " (77, 0.025076853095973295),\n",
       " (22, 0.024319443837111265),\n",
       " (0, 0.02269911510429168),\n",
       " (87, 0.019212632734418644),\n",
       " (4, 0.01884447912674614),\n",
       " (15, 0.017249233192974334),\n",
       " (20, 0.014583393056159633),\n",
       " (51, 0.013524804598310958),\n",
       " (71, 0.013410749943023603),\n",
       " (107, 0.013130104605322046),\n",
       " (110, 0.012092423914649153),\n",
       " (14, 0.011977863562094182),\n",
       " (117, 0.009259259259258412),\n",
       " (112, 0.008975985870674386),\n",
       " (101, 0.007884653539858899),\n",
       " (36, 0.0067207567213016794),\n",
       " (115, 0.004328489050112516),\n",
       " (35, 6.57846725005129e-05),\n",
       " (1, 0.0),\n",
       " (3, 0.0),\n",
       " (6, 0.0),\n",
       " (8, 0.0),\n",
       " (10, 0.0),\n",
       " (11, 0.0),\n",
       " (12, 0.0),\n",
       " (13, 0.0),\n",
       " (17, 0.0),\n",
       " (23, 0.0),\n",
       " (25, 0.0),\n",
       " (27, 0.0),\n",
       " (29, 0.0),\n",
       " (30, 0.0),\n",
       " (31, 0.0),\n",
       " (32, 0.0),\n",
       " (34, 0.0),\n",
       " (38, 0.0),\n",
       " (39, 0.0),\n",
       " (44, 0.0),\n",
       " (46, 0.0),\n",
       " (49, 0.0),\n",
       " (52, 0.0),\n",
       " (53, 0.0),\n",
       " (56, 0.0),\n",
       " (57, 0.0),\n",
       " (60, 0.0),\n",
       " (62, 0.0),\n",
       " (64, 0.0),\n",
       " (65, 0.0),\n",
       " (66, 0.0),\n",
       " (67, 0.0),\n",
       " (72, 0.0),\n",
       " (73, 0.0),\n",
       " (74, 0.0),\n",
       " (75, 0.0),\n",
       " (76, 0.0),\n",
       " (78, 0.0),\n",
       " (81, 0.0),\n",
       " (83, 0.0),\n",
       " (85, 0.0),\n",
       " (86, 0.0),\n",
       " (89, 0.0),\n",
       " (91, 0.0),\n",
       " (94, 0.0),\n",
       " (97, 0.0),\n",
       " (99, 0.0),\n",
       " (111, 0.0),\n",
       " (116, 0.0),\n",
       " (118, 0.0),\n",
       " (119, 0.0),\n",
       " (120, 0.0),\n",
       " (121, 0.0)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_matual_info_for_all(data_df_13_09_2022, \"N\", \"J\", get_average_bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_ix = get_bands_ix_from_mutual_info(data_df_13_09_2022, 0.004, \"N\", \"J\", get_max_bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_ix = filter(lambda data: data < 100, bands_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands_ix = list(bands_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 9,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 24,\n",
       " 26,\n",
       " 28,\n",
       " 33,\n",
       " 37,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 45,\n",
       " 47,\n",
       " 48,\n",
       " 50,\n",
       " 51,\n",
       " 54,\n",
       " 55,\n",
       " 58,\n",
       " 59,\n",
       " 61,\n",
       " 63,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 77,\n",
       " 79,\n",
       " 80,\n",
       " 82,\n",
       " 84,\n",
       " 87,\n",
       " 88,\n",
       " 90,\n",
       " 92,\n",
       " 93,\n",
       " 95,\n",
       " 96,\n",
       " 98]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bands_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bands_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | loss train: 6206.951171875\n",
      "Epoch: 100 | loss train: 6159.8232421875\n",
      "Epoch: 200 | loss train: 2315.97900390625\n",
      "Epoch: 300 | loss train: 2294.082275390625\n",
      "Epoch: 400 | loss train: 2289.582275390625\n",
      "Epoch: 500 | loss train: 2283.5205078125\n",
      "Epoch: 600 | loss train: 2274.64013671875\n",
      "Epoch: 700 | loss train: 2262.973388671875\n",
      "Epoch: 800 | loss train: 2247.54833984375\n",
      "Epoch: 900 | loss train: 2220.887451171875\n",
      "Epoch: 1000 | loss train: 2145.51318359375\n",
      "Epoch: 1100 | loss train: 1951.9764404296875\n",
      "Epoch: 1200 | loss train: 1898.412353515625\n",
      "Epoch: 1300 | loss train: 1868.0113525390625\n",
      "Epoch: 1400 | loss train: 1834.4022216796875\n",
      "Epoch: 1500 | loss train: 1791.0186767578125\n",
      "Epoch: 1600 | loss train: 1734.3675537109375\n",
      "Epoch: 1700 | loss train: 1639.46435546875\n",
      "Epoch: 1800 | loss train: 1552.49658203125\n",
      "Epoch: 1900 | loss train: 1501.8951416015625\n",
      "Epoch: 2000 | loss train: 1457.031005859375\n",
      "Epoch: 2100 | loss train: 1429.1871337890625\n",
      "Epoch: 2200 | loss train: 1406.3536376953125\n",
      "Epoch: 2300 | loss train: 1384.5928955078125\n",
      "Epoch: 2400 | loss train: 1363.500732421875\n",
      "Epoch: 2500 | loss train: 1342.6673583984375\n",
      "Epoch: 2600 | loss train: 1331.583251953125\n",
      "Epoch: 2700 | loss train: 1315.1859130859375\n",
      "Epoch: 2800 | loss train: 1301.4224853515625\n",
      "Epoch: 2900 | loss train: 1287.54833984375\n",
      "Epoch: 3000 | loss train: 1282.7889404296875\n",
      "Epoch: 3100 | loss train: 1262.7783203125\n",
      "Epoch: 3200 | loss train: 1248.344482421875\n",
      "Epoch: 3300 | loss train: 1230.862548828125\n",
      "Epoch: 3400 | loss train: 1217.971923828125\n",
      "Epoch: 3500 | loss train: 1191.847412109375\n",
      "Epoch: 3600 | loss train: 1160.906494140625\n",
      "Epoch: 3700 | loss train: 1129.7755126953125\n",
      "Epoch: 3800 | loss train: 1106.541259765625\n",
      "Epoch: 3900 | loss train: 1087.407470703125\n",
      "Epoch: 4000 | loss train: 1074.08447265625\n",
      "Epoch: 4100 | loss train: 1058.3681640625\n",
      "Epoch: 4200 | loss train: 1045.1385498046875\n",
      "Epoch: 4300 | loss train: 1040.20458984375\n",
      "Epoch: 4400 | loss train: 1021.4375610351562\n",
      "Epoch: 4500 | loss train: 1013.7323608398438\n",
      "Epoch: 4600 | loss train: 998.4855346679688\n",
      "Epoch: 4700 | loss train: 989.90380859375\n",
      "Epoch: 4800 | loss train: 979.6353759765625\n",
      "Epoch: 4900 | loss train: 992.73828125\n",
      "Epoch: 5000 | loss train: 962.5109252929688\n",
      "Epoch: 5100 | loss train: 950.4471435546875\n",
      "Epoch: 5200 | loss train: 942.9776000976562\n",
      "Epoch: 5300 | loss train: 942.3228149414062\n",
      "Epoch: 5400 | loss train: 942.3483276367188\n",
      "Epoch: 5500 | loss train: 929.6328735351562\n",
      "Epoch: 5600 | loss train: 916.9359130859375\n",
      "Epoch: 5700 | loss train: 918.3489990234375\n",
      "Epoch: 5800 | loss train: 921.8184204101562\n",
      "Epoch: 5900 | loss train: 897.3447265625\n",
      "Epoch: 6000 | loss train: 897.1900024414062\n",
      "Epoch: 6100 | loss train: 886.2108154296875\n",
      "Epoch: 6200 | loss train: 880.5968627929688\n",
      "Epoch: 6300 | loss train: 912.1837768554688\n",
      "Epoch: 6400 | loss train: 869.9342041015625\n",
      "Epoch: 6500 | loss train: 870.2837524414062\n",
      "Epoch: 6600 | loss train: 865.82177734375\n",
      "Epoch: 6700 | loss train: 895.0874633789062\n",
      "Epoch: 6800 | loss train: 856.2152099609375\n",
      "Epoch: 6900 | loss train: 890.633056640625\n",
      "Epoch: 7000 | loss train: 846.3416748046875\n",
      "Epoch: 7100 | loss train: 838.767333984375\n",
      "Epoch: 7200 | loss train: 833.6229858398438\n",
      "Epoch: 7300 | loss train: 827.6755981445312\n",
      "Epoch: 7400 | loss train: 829.9671020507812\n",
      "Epoch: 7500 | loss train: 818.9913940429688\n",
      "Epoch: 7600 | loss train: 813.19189453125\n",
      "Epoch: 7700 | loss train: 895.9714965820312\n",
      "Epoch: 7800 | loss train: 804.5160522460938\n",
      "Epoch: 7900 | loss train: 796.811279296875\n",
      "Epoch: 8000 | loss train: 791.1607666015625\n",
      "Epoch: 8100 | loss train: 785.75830078125\n",
      "Epoch: 8200 | loss train: 793.2189331054688\n",
      "Epoch: 8300 | loss train: 781.3030395507812\n",
      "Epoch: 8400 | loss train: 767.4938354492188\n",
      "Epoch: 8500 | loss train: 760.4202880859375\n",
      "Epoch: 8600 | loss train: 779.1939697265625\n",
      "Epoch: 8700 | loss train: 725.5855102539062\n",
      "Epoch: 8800 | loss train: 714.6013793945312\n",
      "Epoch: 8900 | loss train: 687.9360961914062\n",
      "Epoch: 9000 | loss train: 670.0567626953125\n",
      "Epoch: 9100 | loss train: 655.7056884765625\n",
      "Epoch: 9200 | loss train: 649.2758178710938\n",
      "Epoch: 9300 | loss train: 628.2904663085938\n",
      "Epoch: 9400 | loss train: 606.731201171875\n",
      "Epoch: 9500 | loss train: 594.6544189453125\n",
      "Epoch: 9600 | loss train: 580.6006469726562\n",
      "Epoch: 9700 | loss train: 586.1001586914062\n",
      "Epoch: 9800 | loss train: 541.884765625\n",
      "Epoch: 9900 | loss train: 530.08642578125\n",
      "Epoch: 10000 | loss train: 555.6605834960938\n",
      "Epoch: 10100 | loss train: 536.8407592773438\n",
      "Epoch: 10200 | loss train: 510.6599426269531\n",
      "Epoch: 10300 | loss train: 480.40557861328125\n",
      "Epoch: 10400 | loss train: 470.4489440917969\n",
      "Epoch: 10500 | loss train: 462.53192138671875\n",
      "Epoch: 10600 | loss train: 458.417724609375\n",
      "Epoch: 10700 | loss train: 451.8571472167969\n",
      "Epoch: 10800 | loss train: 449.5657043457031\n",
      "Epoch: 10900 | loss train: 419.7928161621094\n",
      "Epoch: 11000 | loss train: 443.0621337890625\n",
      "Epoch: 11100 | loss train: 418.4019470214844\n",
      "Epoch: 11200 | loss train: 431.31463623046875\n",
      "Epoch: 11300 | loss train: 398.6548767089844\n",
      "Epoch: 11400 | loss train: 379.0202331542969\n",
      "Epoch: 11500 | loss train: 401.23223876953125\n",
      "Epoch: 11600 | loss train: 400.9252624511719\n",
      "Epoch: 11700 | loss train: 494.9368896484375\n",
      "Epoch: 11800 | loss train: 363.266357421875\n",
      "Epoch: 11900 | loss train: 356.5556335449219\n",
      "Epoch: 12000 | loss train: 396.09771728515625\n",
      "Epoch: 12100 | loss train: 358.9634704589844\n",
      "Epoch: 12200 | loss train: 347.7807922363281\n",
      "Epoch: 12300 | loss train: 333.0533142089844\n",
      "Epoch: 12400 | loss train: 482.1134033203125\n",
      "Epoch: 12500 | loss train: 330.3031921386719\n",
      "Epoch: 12600 | loss train: 320.41033935546875\n",
      "Epoch: 12700 | loss train: 359.87335205078125\n",
      "Epoch: 12800 | loss train: 364.1358337402344\n",
      "Epoch: 12900 | loss train: 341.82373046875\n",
      "Epoch: 13000 | loss train: 306.0970458984375\n",
      "Epoch: 13100 | loss train: 301.2647705078125\n",
      "Epoch: 13200 | loss train: 298.4499816894531\n",
      "Epoch: 13300 | loss train: 312.4485778808594\n",
      "Epoch: 13400 | loss train: 301.29254150390625\n",
      "Epoch: 13500 | loss train: 292.9371032714844\n",
      "Epoch: 13600 | loss train: 316.79241943359375\n",
      "Epoch: 13700 | loss train: 340.6488342285156\n",
      "Epoch: 13800 | loss train: 291.5852966308594\n",
      "Epoch: 13900 | loss train: 286.6996154785156\n",
      "Epoch: 14000 | loss train: 289.0947265625\n",
      "Epoch: 14100 | loss train: 287.4960632324219\n",
      "Epoch: 14200 | loss train: 277.23101806640625\n",
      "Epoch: 14300 | loss train: 314.16943359375\n",
      "Epoch: 14400 | loss train: 286.84527587890625\n",
      "Epoch: 14500 | loss train: 276.0509948730469\n",
      "Epoch: 14600 | loss train: 277.2322998046875\n",
      "Epoch: 14700 | loss train: 271.4616394042969\n",
      "Epoch: 14800 | loss train: 308.6674499511719\n",
      "Epoch: 14900 | loss train: 318.64874267578125\n",
      "Epoch: 15000 | loss train: 266.7807922363281\n",
      "Epoch: 15100 | loss train: 272.6757507324219\n",
      "Epoch: 15200 | loss train: 262.0895080566406\n",
      "Epoch: 15300 | loss train: 263.1512451171875\n",
      "Epoch: 15400 | loss train: 349.60430908203125\n",
      "Epoch: 15500 | loss train: 255.51202392578125\n",
      "Epoch: 15600 | loss train: 321.0926513671875\n",
      "Epoch: 15700 | loss train: 292.8439636230469\n",
      "Epoch: 15800 | loss train: 251.68630981445312\n",
      "Epoch: 15900 | loss train: 249.6522216796875\n",
      "Epoch: 16000 | loss train: 276.7194519042969\n",
      "Epoch: 16100 | loss train: 279.0016174316406\n",
      "Epoch: 16200 | loss train: 247.70228576660156\n",
      "Epoch: 16300 | loss train: 252.49288940429688\n",
      "Epoch: 16400 | loss train: 250.07919311523438\n",
      "Epoch: 16500 | loss train: 252.2778778076172\n",
      "Epoch: 16600 | loss train: 301.3839111328125\n",
      "Epoch: 16700 | loss train: 416.5064392089844\n",
      "Epoch: 16800 | loss train: 463.9201965332031\n",
      "Epoch: 16900 | loss train: 258.32244873046875\n",
      "Epoch: 17000 | loss train: 314.35858154296875\n",
      "Epoch: 17100 | loss train: 263.5207214355469\n",
      "Epoch: 17200 | loss train: 249.92584228515625\n",
      "Epoch: 17300 | loss train: 253.906494140625\n",
      "Epoch: 17400 | loss train: 313.9456787109375\n",
      "Epoch: 17500 | loss train: 267.2407531738281\n",
      "Epoch: 17600 | loss train: 229.7913055419922\n",
      "Epoch: 17700 | loss train: 241.99551391601562\n",
      "Epoch: 17800 | loss train: 244.99322509765625\n",
      "Epoch: 17900 | loss train: 226.57945251464844\n",
      "Epoch: 18000 | loss train: 224.70706176757812\n",
      "Epoch: 18100 | loss train: 243.43289184570312\n",
      "Epoch: 18200 | loss train: 260.2464599609375\n",
      "Epoch: 18300 | loss train: 251.61288452148438\n",
      "Epoch: 18400 | loss train: 222.31820678710938\n",
      "Epoch: 18500 | loss train: 222.23220825195312\n",
      "Epoch: 18600 | loss train: 226.4393768310547\n",
      "Epoch: 18700 | loss train: 327.6733093261719\n",
      "Epoch: 18800 | loss train: 245.43966674804688\n",
      "Epoch: 18900 | loss train: 241.1373291015625\n",
      "Epoch: 19000 | loss train: 228.19293212890625\n",
      "Epoch: 19100 | loss train: 230.2587127685547\n",
      "Epoch: 19200 | loss train: 258.05657958984375\n",
      "Epoch: 19300 | loss train: 213.83761596679688\n",
      "Epoch: 19400 | loss train: 208.70062255859375\n",
      "Epoch: 19500 | loss train: 209.80152893066406\n",
      "Epoch: 19600 | loss train: 243.29864501953125\n",
      "Epoch: 19700 | loss train: 206.79603576660156\n",
      "Epoch: 19800 | loss train: 238.77110290527344\n",
      "Epoch: 19900 | loss train: 215.6176300048828\n",
      "Epoch: 20000 | loss train: 233.12074279785156\n",
      "Epoch: 20100 | loss train: 228.1874542236328\n",
      "Epoch: 20200 | loss train: 214.72178649902344\n",
      "Epoch: 20300 | loss train: 235.62734985351562\n",
      "Epoch: 20400 | loss train: 202.02488708496094\n",
      "Epoch: 20500 | loss train: 254.5289764404297\n",
      "Epoch: 20600 | loss train: 239.30319213867188\n",
      "Epoch: 20700 | loss train: 241.5098876953125\n",
      "Epoch: 20800 | loss train: 247.9916229248047\n",
      "Epoch: 20900 | loss train: 200.841552734375\n",
      "Epoch: 21000 | loss train: 227.90724182128906\n",
      "Epoch: 21100 | loss train: 237.99671936035156\n",
      "Epoch: 21200 | loss train: 230.02523803710938\n",
      "Epoch: 21300 | loss train: 214.71475219726562\n",
      "Epoch: 21400 | loss train: 274.4735412597656\n",
      "Epoch: 21500 | loss train: 192.21243286132812\n",
      "Epoch: 21600 | loss train: 198.61932373046875\n",
      "Epoch: 21700 | loss train: 179.75344848632812\n",
      "Epoch: 21800 | loss train: 181.5695037841797\n",
      "Epoch: 21900 | loss train: 183.69049072265625\n",
      "Epoch: 22000 | loss train: 180.77664184570312\n",
      "Epoch: 22100 | loss train: 177.45611572265625\n",
      "Epoch: 22200 | loss train: 176.21620178222656\n",
      "Epoch: 22300 | loss train: 213.6715087890625\n",
      "Epoch: 22400 | loss train: 246.20083618164062\n",
      "Epoch: 22500 | loss train: 172.25567626953125\n",
      "Epoch: 22600 | loss train: 357.9270324707031\n",
      "Epoch: 22700 | loss train: 195.5941925048828\n",
      "Epoch: 22800 | loss train: 163.57485961914062\n",
      "Epoch: 22900 | loss train: 175.67279052734375\n",
      "Epoch: 23000 | loss train: 179.00531005859375\n",
      "Epoch: 23100 | loss train: 300.59173583984375\n",
      "Epoch: 23200 | loss train: 161.39089965820312\n",
      "Epoch: 23300 | loss train: 170.32522583007812\n",
      "Epoch: 23400 | loss train: 336.7256164550781\n",
      "Epoch: 23500 | loss train: 149.96664428710938\n",
      "Epoch: 23600 | loss train: 149.11842346191406\n",
      "Epoch: 23700 | loss train: 156.18923950195312\n",
      "Epoch: 23800 | loss train: 208.24136352539062\n",
      "Epoch: 23900 | loss train: 329.0934143066406\n",
      "Epoch: 24000 | loss train: 196.65475463867188\n",
      "Epoch: 24100 | loss train: 162.08248901367188\n",
      "Epoch: 24200 | loss train: 136.39418029785156\n",
      "Epoch: 24300 | loss train: 394.09930419921875\n",
      "Epoch: 24400 | loss train: 134.37606811523438\n",
      "Epoch: 24500 | loss train: 131.85008239746094\n",
      "Epoch: 24600 | loss train: 185.4799041748047\n",
      "Epoch: 24700 | loss train: 141.53683471679688\n",
      "Epoch: 24800 | loss train: 406.7734680175781\n",
      "Epoch: 24900 | loss train: 144.81198120117188\n",
      "Epoch: 25000 | loss train: 120.84417724609375\n",
      "Epoch: 25100 | loss train: 120.24273681640625\n",
      "Epoch: 25200 | loss train: 341.5196533203125\n",
      "Epoch: 25300 | loss train: 116.21398162841797\n",
      "Epoch: 25400 | loss train: 263.0637512207031\n",
      "Epoch: 25500 | loss train: 123.94554901123047\n",
      "Epoch: 25600 | loss train: 107.08478546142578\n",
      "Epoch: 25700 | loss train: 118.50636291503906\n",
      "Epoch: 25800 | loss train: 118.39727020263672\n",
      "Epoch: 25900 | loss train: 110.31637573242188\n",
      "Epoch: 26000 | loss train: 112.03401947021484\n",
      "Epoch: 26100 | loss train: 154.91650390625\n",
      "Epoch: 26200 | loss train: 319.36126708984375\n",
      "Epoch: 26300 | loss train: 99.06600189208984\n",
      "Epoch: 26400 | loss train: 117.29594421386719\n",
      "Epoch: 26500 | loss train: 152.35198974609375\n",
      "Epoch: 26600 | loss train: 88.40782165527344\n",
      "Epoch: 26700 | loss train: 112.88507080078125\n",
      "Epoch: 26800 | loss train: 87.35894012451172\n",
      "Epoch: 26900 | loss train: 103.8494873046875\n",
      "Epoch: 27000 | loss train: 266.63494873046875\n",
      "Epoch: 27100 | loss train: 109.76209259033203\n",
      "Epoch: 27200 | loss train: 113.74898529052734\n",
      "Epoch: 27300 | loss train: 116.1687240600586\n",
      "Epoch: 27400 | loss train: 124.01786804199219\n",
      "Epoch: 27500 | loss train: 85.23209381103516\n",
      "Epoch: 27600 | loss train: 105.2860107421875\n",
      "Epoch: 27700 | loss train: 142.81825256347656\n",
      "Epoch: 27800 | loss train: 125.9039306640625\n",
      "Epoch: 27900 | loss train: 236.01548767089844\n",
      "Epoch: 28000 | loss train: 63.1541862487793\n",
      "Epoch: 28100 | loss train: 167.00094604492188\n",
      "Epoch: 28200 | loss train: 60.707618713378906\n",
      "Epoch: 28300 | loss train: 62.71947479248047\n",
      "Epoch: 28400 | loss train: 162.09326171875\n",
      "Epoch: 28500 | loss train: 86.27281951904297\n",
      "Epoch: 28600 | loss train: 58.75910186767578\n",
      "Epoch: 28700 | loss train: 55.26292419433594\n",
      "Epoch: 28800 | loss train: 246.71548461914062\n",
      "Epoch: 28900 | loss train: 78.29810333251953\n",
      "Epoch: 29000 | loss train: 61.591041564941406\n",
      "Epoch: 29100 | loss train: 215.9580078125\n",
      "Epoch: 29200 | loss train: 90.48042297363281\n",
      "Epoch: 29300 | loss train: 199.3176727294922\n",
      "Epoch: 29400 | loss train: 83.83645629882812\n",
      "Epoch: 29500 | loss train: 180.03903198242188\n",
      "Epoch: 29600 | loss train: 46.725650787353516\n",
      "Epoch: 29700 | loss train: 70.33743286132812\n",
      "Epoch: 29800 | loss train: 188.54222106933594\n",
      "Epoch: 29900 | loss train: 41.86042404174805\n",
      "Epoch: 30000 | loss train: 46.14847946166992\n",
      "Epoch: 30100 | loss train: 80.63716888427734\n",
      "Epoch: 30200 | loss train: 140.70010375976562\n",
      "Epoch: 30300 | loss train: 78.47943115234375\n",
      "Epoch: 30400 | loss train: 53.48778533935547\n",
      "Epoch: 30500 | loss train: 189.28793334960938\n",
      "Epoch: 30600 | loss train: 108.87895202636719\n",
      "Epoch: 30700 | loss train: 36.62932205200195\n",
      "Epoch: 30800 | loss train: 38.07389450073242\n",
      "Epoch: 30900 | loss train: 38.00379180908203\n",
      "Epoch: 31000 | loss train: 265.91998291015625\n",
      "Epoch: 31100 | loss train: 28.030561447143555\n",
      "Epoch: 31200 | loss train: 77.72831726074219\n",
      "Epoch: 31300 | loss train: 55.89167404174805\n",
      "Epoch: 31400 | loss train: 81.65029907226562\n",
      "Epoch: 31500 | loss train: 63.12739181518555\n",
      "Epoch: 31600 | loss train: 87.3211441040039\n",
      "Epoch: 31700 | loss train: 62.10321807861328\n",
      "Epoch: 31800 | loss train: 87.9461441040039\n",
      "Epoch: 31900 | loss train: 111.09945678710938\n",
      "Epoch: 32000 | loss train: 51.89775848388672\n",
      "Epoch: 32100 | loss train: 61.75455856323242\n",
      "Epoch: 32200 | loss train: 140.27711486816406\n",
      "Epoch: 32300 | loss train: 284.7867126464844\n",
      "Epoch: 32400 | loss train: 46.74988555908203\n",
      "Epoch: 32500 | loss train: 637.0362548828125\n",
      "Epoch: 32600 | loss train: 54.90666580200195\n",
      "Epoch: 32700 | loss train: 420.1766662597656\n",
      "Epoch: 32800 | loss train: 320.8730773925781\n",
      "Epoch: 32900 | loss train: 54.353946685791016\n",
      "Epoch: 33000 | loss train: 313.7226257324219\n",
      "Epoch: 33100 | loss train: 34.014320373535156\n",
      "Epoch: 33200 | loss train: 198.18505859375\n",
      "Epoch: 33300 | loss train: 60.22085952758789\n",
      "Epoch: 33400 | loss train: 397.78668212890625\n",
      "Epoch: 33500 | loss train: 75.72005462646484\n",
      "Epoch: 33600 | loss train: 71.27896118164062\n",
      "Epoch: 33700 | loss train: 52.93413162231445\n",
      "Epoch: 33800 | loss train: 28.652652740478516\n",
      "Epoch: 33900 | loss train: 28.61305046081543\n",
      "Epoch: 34000 | loss train: 26.814773559570312\n",
      "Epoch: 34100 | loss train: 52.50566864013672\n",
      "Epoch: 34200 | loss train: 30.980545043945312\n",
      "Epoch: 34300 | loss train: 215.81907653808594\n",
      "Epoch: 34400 | loss train: 39.62298583984375\n",
      "Epoch: 34500 | loss train: 280.1954345703125\n",
      "Epoch: 34600 | loss train: 25.80916976928711\n",
      "Epoch: 34700 | loss train: 112.3248062133789\n",
      "Epoch: 34800 | loss train: 22.81022071838379\n",
      "Epoch: 34900 | loss train: 50.934879302978516\n",
      "Epoch: 35000 | loss train: 192.89955139160156\n",
      "Epoch: 35100 | loss train: 45.90966033935547\n",
      "Epoch: 35200 | loss train: 186.9783935546875\n",
      "Epoch: 35300 | loss train: 36.68581771850586\n",
      "Epoch: 35400 | loss train: 141.20053100585938\n",
      "Epoch: 35500 | loss train: 23.849674224853516\n",
      "Epoch: 35600 | loss train: 55.45560073852539\n",
      "Epoch: 35700 | loss train: 22.691041946411133\n",
      "Epoch: 35800 | loss train: 39.595848083496094\n",
      "Epoch: 35900 | loss train: 65.17276000976562\n",
      "Epoch: 36000 | loss train: 46.795169830322266\n",
      "Epoch: 36100 | loss train: 24.67649269104004\n",
      "Epoch: 36200 | loss train: 138.70816040039062\n",
      "Epoch: 36300 | loss train: 48.7159423828125\n",
      "Epoch: 36400 | loss train: 181.7959442138672\n",
      "Epoch: 36500 | loss train: 67.59056091308594\n",
      "Epoch: 36600 | loss train: 60.913509368896484\n",
      "Epoch: 36700 | loss train: 58.80204391479492\n",
      "Epoch: 36800 | loss train: 116.09481048583984\n",
      "Epoch: 36900 | loss train: 60.24430465698242\n",
      "Epoch: 37000 | loss train: 153.31106567382812\n",
      "Epoch: 37100 | loss train: 21.469371795654297\n",
      "Epoch: 37200 | loss train: 30.351131439208984\n",
      "Epoch: 37300 | loss train: 288.6593017578125\n",
      "Epoch: 37400 | loss train: 50.83296585083008\n",
      "Epoch: 37500 | loss train: 52.78733444213867\n",
      "Epoch: 37600 | loss train: 58.213565826416016\n",
      "Epoch: 37700 | loss train: 483.4022216796875\n",
      "Epoch: 37800 | loss train: 55.231807708740234\n",
      "Epoch: 37900 | loss train: 170.7792510986328\n",
      "Epoch: 38000 | loss train: 33.32455062866211\n",
      "Epoch: 38100 | loss train: 135.69517517089844\n",
      "Epoch: 38200 | loss train: 196.48825073242188\n",
      "Epoch: 38300 | loss train: 31.470304489135742\n",
      "Epoch: 38400 | loss train: 249.0823516845703\n",
      "Epoch: 38500 | loss train: 21.744190216064453\n",
      "Epoch: 38600 | loss train: 49.96044158935547\n",
      "Epoch: 38700 | loss train: 102.93924713134766\n",
      "Epoch: 38800 | loss train: 71.47527313232422\n",
      "Epoch: 38900 | loss train: 387.2357177734375\n",
      "Epoch: 39000 | loss train: 32.549560546875\n",
      "Epoch: 39100 | loss train: 44.884952545166016\n",
      "Epoch: 39200 | loss train: 37.489410400390625\n",
      "Epoch: 39300 | loss train: 65.62164306640625\n",
      "Epoch: 39400 | loss train: 45.9246940612793\n",
      "Epoch: 39500 | loss train: 20.683483123779297\n",
      "Epoch: 39600 | loss train: 97.29949951171875\n",
      "Epoch: 39700 | loss train: 42.977638244628906\n",
      "Epoch: 39800 | loss train: 22.101274490356445\n",
      "Epoch: 39900 | loss train: 17.022729873657227\n",
      "loss_NN=tensor(5690.3950)\n",
      "pred_NN=tensor([[7964.7300],\n",
      "        [9417.4307],\n",
      "        [8685.2686],\n",
      "        [7627.0024]])\n",
      "loss_RF=2678.8063519065995\n",
      "pred_RF=array([4742.46834473, 5462.0944873 , 5772.13293457, 5617.40926514])\n",
      "loss_DT=2700.4680358581704\n",
      "pred_DT=array([6610.33154297, 4605.5078125 , 2721.14624023, 5365.20800781])\n"
     ]
    }
   ],
   "source": [
    "target_value = \"N\"\n",
    "train_field = \"J\"\n",
    "sample = generate_sample(data_df_13_09_2022, bands_ix, target_value, train_field)\n",
    "X_train, Y_train = create_X_train_Y_train(sample, bands_ix)\n",
    "sample_target = generate_sample(data_df_13_09_2022, bands_ix, target_value, \"BC\")\n",
    "X_target, Y_target = create_X_train_Y_train(sample_target, bands_ix)\n",
    "super_param={\"lr\": 0.0001, \"weight_decay\": 1e-5, \"n_epochs\": 40000, \"stop_value\": 0.5}\n",
    "re_run = \"Y\"\n",
    "loss_NN, pred_NN = predict_using_neutral_network(X_train, Y_train, X_target, Y_target, \"test_nn_using_mutual_info_2.pt\", super_param, re_run)\n",
    "print(f\"{loss_NN=}\")\n",
    "print(f\"{pred_NN=}\")\n",
    "loss_RF, pred_RF = predict_using_random_forest(X_train, Y_train, X_target, Y_target, super_param)\n",
    "print(f\"{loss_RF=}\")\n",
    "print(f\"{pred_RF=}\")\n",
    "loss_DT, pred_DT = predict_using_decision_tree(X_train, Y_train, X_target, Y_target, super_param)\n",
    "print(f\"{loss_DT=}\")\n",
    "print(f\"{pred_DT=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
